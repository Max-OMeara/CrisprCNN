{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4172d63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2921b503c90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== Import libraries ====\n",
    "import math\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b1f4a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "CUDA version: 13.0\n",
      "GPU device: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "# ==== Check GPU/CUDA availability ====\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"Training will use CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "995c5deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train=29452509  val=3681564  test=3681564\n",
      "cells=420  phenotypes=34  chrs=279  strands=3\n",
      "Target stats: mu=-0.0878, sigma=0.8148\n"
     ]
    }
   ],
   "source": [
    "DATA_CSV = \"data/GenomeCRISPR_+_strands.csv\"\n",
    "SEQ_LEN  = 23\n",
    "VAL_FRAC = 0.10\n",
    "TEST_FRAC= 0.10\n",
    "\n",
    "seq_col   = \"sequence\"\n",
    "cell_col  = \"cellline\"\n",
    "phen_col  = \"condition\"\n",
    "chr_col   = \"chr\"\n",
    "strand_col= \"strand\"\n",
    "target_col= \"log2fc\"\n",
    "\n",
    "# Read & keep only what we need\n",
    "df = pd.read_csv(DATA_CSV, low_memory=False)\n",
    "required_cols = [seq_col, cell_col, phen_col, chr_col, target_col]\n",
    "optional_cols = [strand_col]\n",
    "\n",
    "# Check for required columns\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing expected columns: {missing}. Got: {list(df.columns)}\")\n",
    "\n",
    "# Check for optional columns (strand)\n",
    "has_strand = strand_col in df.columns\n",
    "cols_to_keep = required_cols + ([strand_col] if has_strand else [])\n",
    "\n",
    "df = df[cols_to_keep].copy()\n",
    "df = df.dropna(subset=required_cols)\n",
    "\n",
    "# Clean sequences: uppercase A/C/G/T and enforce length 23\n",
    "df[seq_col] = df[seq_col].astype(str).str.upper().str.strip()\n",
    "df = df[df[seq_col].str.len() == SEQ_LEN]\n",
    "df = df[df[seq_col].str.match(r\"^[ACGT]+$\")]\n",
    "\n",
    "# Factorize categoricals (one-liners)\n",
    "cell_codes, cell_uniques = pd.factorize(df[cell_col].astype(str).str.strip(), sort=True)\n",
    "phen_codes, phen_uniques = pd.factorize(df[phen_col].astype(str).str.strip(), sort=True)\n",
    "# cast chr to string so 10/11/X/Y are handled uniformly\n",
    "chr_codes,  chr_uniques  = pd.factorize(df[chr_col].astype(str).str.strip(),  sort=True)\n",
    "\n",
    "# Factorize strand if available, otherwise create dummy\n",
    "if has_strand:\n",
    "    strand_codes, strand_uniques = pd.factorize(df[strand_col].astype(str).str.strip(), sort=True)\n",
    "    n_strand = len(strand_uniques)\n",
    "else:\n",
    "    # Create dummy strand codes (all zeros) if not available\n",
    "    strand_codes = np.zeros(len(df), dtype=np.int64)\n",
    "    strand_uniques = np.array([\"+\"] if \"+\" in str(df.get(strand_col, \"+\").iloc[0] if len(df) > 0 else \"+\") else [\"+\"])\n",
    "    n_strand = 1\n",
    "\n",
    "n_cell, n_ph, n_chr = len(cell_uniques), len(phen_uniques), len(chr_uniques)\n",
    "\n",
    "# One-hot the 23-mer sequences\n",
    "BASE2IDX = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\n",
    "def onehot_batch(seqs, L=SEQ_LEN):\n",
    "    N = len(seqs)\n",
    "    X = np.zeros((N, 4, L), dtype=np.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        for j, ch in enumerate(s):\n",
    "            X[i, BASE2IDX[ch], j] = 1.0\n",
    "    return X\n",
    "\n",
    "X_seq = onehot_batch(df[seq_col].tolist())\n",
    "X_cell = cell_codes.astype(np.int64)\n",
    "X_ph   = phen_codes.astype(np.int64)\n",
    "X_chr  = chr_codes.astype(np.int64)\n",
    "X_strand = strand_codes.astype(np.int64)\n",
    "y      = df[target_col].astype(np.float32).to_numpy()\n",
    "\n",
    "# Simple random split\n",
    "idx_all = np.arange(len(df))\n",
    "idx_train, idx_test = train_test_split(idx_all, test_size=TEST_FRAC, random_state=42)\n",
    "idx_train, idx_val  = train_test_split(idx_train, test_size=VAL_FRAC/(1-TEST_FRAC), random_state=42)\n",
    "\n",
    "def take(a, idx): return a[idx]\n",
    "Xtr_seq, Xva_seq, Xte_seq = take(X_seq, idx_train), take(X_seq, idx_val), take(X_seq, idx_test)\n",
    "Xtr_cel, Xva_cel, Xte_cel = take(X_cell, idx_train), take(X_cell, idx_val), take(X_cell, idx_test)\n",
    "Xtr_ph,  Xva_ph,  Xte_ph  = take(X_ph,  idx_train), take(X_ph,  idx_val), take(X_ph,  idx_test)\n",
    "Xtr_chr, Xva_chr, Xte_chr = take(X_chr, idx_train), take(X_chr, idx_val), take(X_chr, idx_test)\n",
    "Xtr_str, Xva_str, Xte_str = take(X_strand, idx_train), take(X_strand, idx_val), take(X_strand, idx_test)\n",
    "y_tr,    y_va,    y_te    = take(y,     idx_train), take(y,     idx_val), take(y,     idx_test)\n",
    "\n",
    "# Standardize targets using training set statistics\n",
    "mu = y_tr.mean()\n",
    "sigma = y_tr.std()\n",
    "y_tr_norm = (y_tr - mu) / sigma\n",
    "y_va_norm = (y_va - mu) / sigma\n",
    "y_te_norm = (y_te - mu) / sigma\n",
    "\n",
    "# Create full normalized array for easy indexing\n",
    "y_norm = np.zeros_like(y)\n",
    "y_norm[idx_train] = y_tr_norm\n",
    "y_norm[idx_val] = y_va_norm\n",
    "y_norm[idx_test] = y_te_norm\n",
    "\n",
    "print(f\"train={len(idx_train)}  val={len(idx_val)}  test={len(idx_test)}\")\n",
    "print(f\"cells={n_cell}  phenotypes={n_ph}  chrs={n_chr}  strands={n_strand}\")\n",
    "print(f\"Target stats: mu={mu:.4f}, sigma={sigma:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c2f41",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mImprovedCrisprCNN\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, base_channels=\u001b[32m64\u001b[39m, \n\u001b[32m      3\u001b[39m                  n_cell=\u001b[32m420\u001b[39m, n_phen=\u001b[32m34\u001b[39m, n_chr=\u001b[32m301\u001b[39m, n_strand=\u001b[32m2\u001b[39m,\n\u001b[32m      4\u001b[39m                  emb_dim=\u001b[32m32\u001b[39m, dropout=\u001b[32m0.3\u001b[39m):\n\u001b[32m      5\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class ImprovedCrisprCNN(nn.Module):\n",
    "    def __init__(self, base_channels=64, \n",
    "                 n_cell=420, n_phen=34, n_chr=301, n_strand=2,\n",
    "                 emb_dim=32, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Deep Conv2D layers\n",
    "        self.conv2d_1 = nn.Sequential(\n",
    "            nn.Conv2d(1, base_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv2d_2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels * 2, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(base_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv2d_3 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 2, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(base_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv2d_4 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 2, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(base_channels * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Multi-scale feature extraction\n",
    "        self.conv_multi1 = nn.Conv2d(base_channels * 2, base_channels, kernel_size=(1, 1))\n",
    "        self.conv_multi2 = nn.Conv2d(base_channels * 2, base_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "        \n",
    "        # After pooling (max + avg for 2 branches): base_channels * 4\n",
    "        seq_feat_dim = base_channels * 4\n",
    "        \n",
    "        # Larger embeddings for categorical features\n",
    "        self.cell_emb = nn.Embedding(n_cell, emb_dim)\n",
    "        self.phen_emb = nn.Embedding(n_phen, emb_dim)\n",
    "        self.chr_emb = nn.Embedding(n_chr, emb_dim)\n",
    "        self.strand_emb = nn.Embedding(n_strand, emb_dim)\n",
    "        \n",
    "        # Feature interaction layers (simpler, more stable)\n",
    "        # Cell-Phenotype interaction\n",
    "        self.cell_phen_interaction = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, emb_dim),\n",
    "            nn.BatchNorm1d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.3)\n",
    "        )\n",
    "        \n",
    "        # Cell-Chromosome interaction\n",
    "        self.cell_chr_interaction = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, emb_dim),\n",
    "            nn.BatchNorm1d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.3)\n",
    "        )\n",
    "        \n",
    "        # Sequence feature projection to match embedding dimension for better fusion\n",
    "        self.seq_proj = nn.Sequential(\n",
    "            nn.Linear(seq_feat_dim, emb_dim * 2),\n",
    "            nn.BatchNorm1d(emb_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.2)\n",
    "        )\n",
    "        \n",
    "        # Combined features: seq + embeddings + interactions\n",
    "        interaction_features = emb_dim * 2  # 2 interaction terms\n",
    "        # Use projected sequence features instead of raw\n",
    "        total_features = (emb_dim * 2) + (emb_dim * 4) + interaction_features\n",
    "        \n",
    "        # Deep feature fusion layers\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(total_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.8),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.6)\n",
    "        )\n",
    "        \n",
    "        # Final prediction head\n",
    "        self.head = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, seq4x23, cell_idx, phen_idx, chr_idx, strand_idx):\n",
    "        # Reshape for Conv2D: (B, 4, 23) -> (B, 1, 4, 23)\n",
    "        x = seq4x23.unsqueeze(1)  # (B, 1, 4, 23)\n",
    "        \n",
    "        # Deep Conv2D processing\n",
    "        x = self.conv2d_1(x)  # (B, base_channels, 4, 23)\n",
    "        x = self.conv2d_2(x)  # (B, base_channels*2, 4, 23)\n",
    "        x = self.conv2d_3(x)  # (B, base_channels*2, 4, 23)\n",
    "        x = self.conv2d_4(x)  # (B, base_channels*2, 4, 23)\n",
    "        \n",
    "        # Multi-scale feature extraction\n",
    "        x1 = self.conv_multi1(x)  # (B, base_channels, 4, 23)\n",
    "        x2 = self.conv_multi2(x)  # (B, base_channels, 4, 23)\n",
    "        \n",
    "        # Better pooling: both max and avg\n",
    "        x1_avg = F.adaptive_avg_pool2d(x1, 1).squeeze(-1).squeeze(-1)  # (B, base_channels)\n",
    "        x1_max = F.adaptive_max_pool2d(x1, 1).squeeze(-1).squeeze(-1)  # (B, base_channels)\n",
    "        x2_avg = F.adaptive_avg_pool2d(x2, 1).squeeze(-1).squeeze(-1)  # (B, base_channels)\n",
    "        x2_max = F.adaptive_max_pool2d(x2, 1).squeeze(-1).squeeze(-1)  # (B, base_channels)\n",
    "        \n",
    "        # Concatenate pooled features\n",
    "        x_seq = torch.cat([x1_avg, x1_max, x2_avg, x2_max], dim=1)  # (B, base_channels * 4)\n",
    "        \n",
    "        # Project sequence features to better match embedding space\n",
    "        x_seq_proj = self.seq_proj(x_seq)  # (B, emb_dim * 2)\n",
    "        \n",
    "        # Categorical embeddings\n",
    "        x_cell = self.cell_emb(cell_idx)    # (B, emb_dim)\n",
    "        x_phen = self.phen_emb(phen_idx)    # (B, emb_dim)\n",
    "        x_chr = self.chr_emb(chr_idx)       # (B, emb_dim)\n",
    "        x_strand = self.strand_emb(strand_idx)  # (B, emb_dim)\n",
    "        \n",
    "        # Feature interactions between categorical embeddings\n",
    "        cell_phen_inter = self.cell_phen_interaction(torch.cat([x_cell, x_phen], dim=1))  # (B, emb_dim)\n",
    "        cell_chr_inter = self.cell_chr_interaction(torch.cat([x_cell, x_chr], dim=1))  # (B, emb_dim)\n",
    "        \n",
    "        # Combine all features: projected sequence + embeddings + interactions\n",
    "        x = torch.cat([\n",
    "            x_seq_proj,  # Projected sequence features\n",
    "            x_cell, x_phen, x_chr, x_strand,  # Original embeddings\n",
    "            cell_phen_inter, cell_chr_inter  # Interaction features\n",
    "        ], dim=1)  # (B, total_features)\n",
    "        \n",
    "        # Feature fusion\n",
    "        x = self.feature_fusion(x)  # (B, 64)\n",
    "        \n",
    "        # Final prediction\n",
    "        return self.head(x).squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = ImprovedCrisprCNN(\n",
    "    base_channels=64,\n",
    "    n_cell=n_cell, n_phen=n_ph, n_chr=n_chr, n_strand=n_strand,\n",
    "    emb_dim=32,\n",
    "    dropout = 0.3\n",
    ").to(device)\n",
    "print(\"params:\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9aecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 01/3\n",
      "============================================================\n",
      "  Train: [  100/57525] (  0.2%) | Loss: 1.0430 | Speed: 40.8 batches/s | ETA: 23:26\n",
      "  Train: [  200/57525] (  0.3%) | Loss: 1.0344 | Speed: 56.6 batches/s | ETA: 16:52\n",
      "  Train: [  300/57525] (  0.5%) | Loss: 1.0185 | Speed: 65.4 batches/s | ETA: 14:35\n",
      "  Train: [  400/57525] (  0.7%) | Loss: 1.0214 | Speed: 70.8 batches/s | ETA: 13:26\n",
      "  Train: [  500/57525] (  0.9%) | Loss: 1.0215 | Speed: 74.8 batches/s | ETA: 12:42\n",
      "  Train: [  600/57525] (  1.0%) | Loss: 1.0185 | Speed: 77.7 batches/s | ETA: 12:12\n",
      "  Train: [  700/57525] (  1.2%) | Loss: 1.0098 | Speed: 80.0 batches/s | ETA: 11:50\n",
      "  Train: [  800/57525] (  1.4%) | Loss: 1.0015 | Speed: 81.6 batches/s | ETA: 11:35\n",
      "  Train: [  900/57525] (  1.6%) | Loss: 0.9978 | Speed: 83.2 batches/s | ETA: 11:20\n",
      "  Train: [ 1000/57525] (  1.7%) | Loss: 0.9945 | Speed: 84.7 batches/s | ETA: 11:07\n",
      "  Train: [ 1100/57525] (  1.9%) | Loss: 0.9921 | Speed: 86.0 batches/s | ETA: 10:56\n",
      "  Train: [ 1200/57525] (  2.1%) | Loss: 0.9862 | Speed: 86.9 batches/s | ETA: 10:47\n",
      "  Train: [ 1300/57525] (  2.3%) | Loss: 0.9841 | Speed: 87.6 batches/s | ETA: 10:41\n",
      "  Train: [ 1400/57525] (  2.4%) | Loss: 0.9823 | Speed: 87.9 batches/s | ETA: 10:38\n",
      "  Train: [ 1500/57525] (  2.6%) | Loss: 0.9809 | Speed: 88.5 batches/s | ETA: 10:33\n",
      "  Train: [ 1600/57525] (  2.8%) | Loss: 0.9783 | Speed: 89.4 batches/s | ETA: 10:25\n",
      "  Train: [ 1700/57525] (  3.0%) | Loss: 0.9765 | Speed: 90.1 batches/s | ETA: 10:19\n",
      "  Train: [ 1800/57525] (  3.1%) | Loss: 0.9741 | Speed: 90.8 batches/s | ETA: 10:13\n",
      "  Train: [ 1900/57525] (  3.3%) | Loss: 0.9732 | Speed: 91.5 batches/s | ETA: 10:08\n",
      "  Train: [ 2000/57525] (  3.5%) | Loss: 0.9731 | Speed: 92.1 batches/s | ETA: 10:02\n",
      "  Train: [ 2100/57525] (  3.7%) | Loss: 0.9722 | Speed: 92.6 batches/s | ETA: 09:58\n",
      "  Train: [ 2200/57525] (  3.8%) | Loss: 0.9704 | Speed: 93.1 batches/s | ETA: 09:54\n",
      "  Train: [ 2300/57525] (  4.0%) | Loss: 0.9678 | Speed: 93.4 batches/s | ETA: 09:51\n",
      "  Train: [ 2400/57525] (  4.2%) | Loss: 0.9659 | Speed: 93.6 batches/s | ETA: 09:48\n",
      "  Train: [ 2500/57525] (  4.3%) | Loss: 0.9656 | Speed: 94.1 batches/s | ETA: 09:44\n",
      "  Train: [ 2600/57525] (  4.5%) | Loss: 0.9651 | Speed: 94.4 batches/s | ETA: 09:41\n",
      "  Train: [ 2700/57525] (  4.7%) | Loss: 0.9630 | Speed: 94.8 batches/s | ETA: 09:38\n",
      "  Train: [ 2800/57525] (  4.9%) | Loss: 0.9620 | Speed: 95.1 batches/s | ETA: 09:35\n",
      "  Train: [ 2900/57525] (  5.0%) | Loss: 0.9604 | Speed: 95.4 batches/s | ETA: 09:32\n",
      "  Train: [ 3000/57525] (  5.2%) | Loss: 0.9587 | Speed: 95.8 batches/s | ETA: 09:29\n",
      "  Train: [ 3100/57525] (  5.4%) | Loss: 0.9587 | Speed: 95.9 batches/s | ETA: 09:27\n",
      "  Train: [ 3200/57525] (  5.6%) | Loss: 0.9575 | Speed: 96.1 batches/s | ETA: 09:25\n",
      "  Train: [ 3300/57525] (  5.7%) | Loss: 0.9577 | Speed: 96.3 batches/s | ETA: 09:22\n",
      "  Train: [ 3400/57525] (  5.9%) | Loss: 0.9576 | Speed: 96.5 batches/s | ETA: 09:21\n",
      "  Train: [ 3500/57525] (  6.1%) | Loss: 0.9561 | Speed: 96.6 batches/s | ETA: 09:19\n",
      "  Train: [ 3600/57525] (  6.3%) | Loss: 0.9558 | Speed: 96.8 batches/s | ETA: 09:16\n",
      "  Train: [ 3700/57525] (  6.4%) | Loss: 0.9549 | Speed: 97.1 batches/s | ETA: 09:14\n",
      "  Train: [ 3800/57525] (  6.6%) | Loss: 0.9540 | Speed: 97.3 batches/s | ETA: 09:12\n",
      "  Train: [ 3900/57525] (  6.8%) | Loss: 0.9531 | Speed: 97.4 batches/s | ETA: 09:10\n",
      "  Train: [ 4000/57525] (  7.0%) | Loss: 0.9522 | Speed: 97.5 batches/s | ETA: 09:08\n",
      "  Train: [ 4100/57525] (  7.1%) | Loss: 0.9510 | Speed: 97.7 batches/s | ETA: 09:06\n",
      "  Train: [ 4200/57525] (  7.3%) | Loss: 0.9496 | Speed: 97.8 batches/s | ETA: 09:05\n",
      "  Train: [ 4300/57525] (  7.5%) | Loss: 0.9485 | Speed: 98.0 batches/s | ETA: 09:03\n",
      "  Train: [ 4400/57525] (  7.6%) | Loss: 0.9477 | Speed: 98.1 batches/s | ETA: 09:01\n",
      "  Train: [ 4500/57525] (  7.8%) | Loss: 0.9472 | Speed: 98.2 batches/s | ETA: 08:59\n",
      "  Train: [ 4600/57525] (  8.0%) | Loss: 0.9464 | Speed: 98.3 batches/s | ETA: 08:58\n",
      "  Train: [ 4700/57525] (  8.2%) | Loss: 0.9456 | Speed: 98.5 batches/s | ETA: 08:56\n",
      "  Train: [ 4800/57525] (  8.3%) | Loss: 0.9439 | Speed: 98.6 batches/s | ETA: 08:54\n",
      "  Train: [ 4900/57525] (  8.5%) | Loss: 0.9435 | Speed: 98.7 batches/s | ETA: 08:53\n",
      "  Train: [ 5000/57525] (  8.7%) | Loss: 0.9424 | Speed: 98.8 batches/s | ETA: 08:51\n",
      "  Train: [ 5100/57525] (  8.9%) | Loss: 0.9419 | Speed: 99.0 batches/s | ETA: 08:49\n",
      "  Train: [ 5200/57525] (  9.0%) | Loss: 0.9414 | Speed: 99.1 batches/s | ETA: 08:47\n",
      "  Train: [ 5300/57525] (  9.2%) | Loss: 0.9410 | Speed: 99.2 batches/s | ETA: 08:46\n",
      "  Train: [ 5400/57525] (  9.4%) | Loss: 0.9402 | Speed: 99.3 batches/s | ETA: 08:44\n",
      "  Train: [ 5500/57525] (  9.6%) | Loss: 0.9398 | Speed: 99.4 batches/s | ETA: 08:43\n",
      "  Train: [ 5600/57525] (  9.7%) | Loss: 0.9394 | Speed: 99.4 batches/s | ETA: 08:42\n",
      "  Train: [ 5700/57525] (  9.9%) | Loss: 0.9387 | Speed: 99.5 batches/s | ETA: 08:40\n",
      "  Train: [ 5800/57525] ( 10.1%) | Loss: 0.9379 | Speed: 99.6 batches/s | ETA: 08:39\n",
      "  Train: [ 5900/57525] ( 10.3%) | Loss: 0.9372 | Speed: 99.6 batches/s | ETA: 08:38\n",
      "  Train: [ 6000/57525] ( 10.4%) | Loss: 0.9362 | Speed: 99.6 batches/s | ETA: 08:37\n",
      "  Train: [ 6100/57525] ( 10.6%) | Loss: 0.9355 | Speed: 99.6 batches/s | ETA: 08:36\n",
      "  Train: [ 6200/57525] ( 10.8%) | Loss: 0.9346 | Speed: 99.6 batches/s | ETA: 08:35\n",
      "  Train: [ 6300/57525] ( 11.0%) | Loss: 0.9344 | Speed: 99.6 batches/s | ETA: 08:34\n",
      "  Train: [ 6400/57525] ( 11.1%) | Loss: 0.9333 | Speed: 99.7 batches/s | ETA: 08:32\n",
      "  Train: [ 6500/57525] ( 11.3%) | Loss: 0.9332 | Speed: 99.8 batches/s | ETA: 08:31\n",
      "  Train: [ 6600/57525] ( 11.5%) | Loss: 0.9324 | Speed: 99.8 batches/s | ETA: 08:30\n",
      "  Train: [ 6700/57525] ( 11.6%) | Loss: 0.9320 | Speed: 99.9 batches/s | ETA: 08:28\n",
      "  Train: [ 6800/57525] ( 11.8%) | Loss: 0.9312 | Speed: 100.0 batches/s | ETA: 08:27\n",
      "  Train: [ 6900/57525] ( 12.0%) | Loss: 0.9302 | Speed: 100.1 batches/s | ETA: 08:25\n",
      "  Train: [ 7000/57525] ( 12.2%) | Loss: 0.9293 | Speed: 100.1 batches/s | ETA: 08:24\n",
      "  Train: [ 7100/57525] ( 12.3%) | Loss: 0.9284 | Speed: 100.2 batches/s | ETA: 08:23\n",
      "  Train: [ 7200/57525] ( 12.5%) | Loss: 0.9273 | Speed: 100.3 batches/s | ETA: 08:21\n",
      "  Train: [ 7300/57525] ( 12.7%) | Loss: 0.9266 | Speed: 100.4 batches/s | ETA: 08:20\n",
      "  Train: [ 7400/57525] ( 12.9%) | Loss: 0.9261 | Speed: 100.4 batches/s | ETA: 08:19\n",
      "  Train: [ 7500/57525] ( 13.0%) | Loss: 0.9252 | Speed: 100.5 batches/s | ETA: 08:17\n",
      "  Train: [ 7600/57525] ( 13.2%) | Loss: 0.9248 | Speed: 100.6 batches/s | ETA: 08:16\n",
      "  Train: [ 7700/57525] ( 13.4%) | Loss: 0.9241 | Speed: 100.6 batches/s | ETA: 08:15\n",
      "  Train: [ 7800/57525] ( 13.6%) | Loss: 0.9237 | Speed: 100.6 batches/s | ETA: 08:14\n",
      "  Train: [ 7900/57525] ( 13.7%) | Loss: 0.9229 | Speed: 100.7 batches/s | ETA: 08:13\n",
      "  Train: [ 8000/57525] ( 13.9%) | Loss: 0.9224 | Speed: 100.7 batches/s | ETA: 08:11\n",
      "  Train: [ 8100/57525] ( 14.1%) | Loss: 0.9217 | Speed: 100.8 batches/s | ETA: 08:10\n",
      "  Train: [ 8200/57525] ( 14.3%) | Loss: 0.9212 | Speed: 100.8 batches/s | ETA: 08:09\n",
      "  Train: [ 8300/57525] ( 14.4%) | Loss: 0.9206 | Speed: 100.9 batches/s | ETA: 08:08\n",
      "  Train: [ 8400/57525] ( 14.6%) | Loss: 0.9202 | Speed: 100.8 batches/s | ETA: 08:07\n",
      "  Train: [ 8500/57525] ( 14.8%) | Loss: 0.9196 | Speed: 100.9 batches/s | ETA: 08:06\n",
      "  Train: [ 8600/57525] ( 15.0%) | Loss: 0.9190 | Speed: 100.9 batches/s | ETA: 08:04\n",
      "  Train: [ 8700/57525] ( 15.1%) | Loss: 0.9184 | Speed: 101.0 batches/s | ETA: 08:03\n",
      "  Train: [ 8800/57525] ( 15.3%) | Loss: 0.9175 | Speed: 101.0 batches/s | ETA: 08:02\n",
      "  Train: [ 8900/57525] ( 15.5%) | Loss: 0.9172 | Speed: 101.1 batches/s | ETA: 08:00\n",
      "  Train: [ 9000/57525] ( 15.6%) | Loss: 0.9165 | Speed: 101.1 batches/s | ETA: 07:59\n",
      "  Train: [ 9100/57525] ( 15.8%) | Loss: 0.9161 | Speed: 101.2 batches/s | ETA: 07:58\n",
      "  Train: [ 9200/57525] ( 16.0%) | Loss: 0.9152 | Speed: 101.2 batches/s | ETA: 07:57\n",
      "  Train: [ 9300/57525] ( 16.2%) | Loss: 0.9148 | Speed: 101.3 batches/s | ETA: 07:56\n",
      "  Train: [ 9400/57525] ( 16.3%) | Loss: 0.9141 | Speed: 101.3 batches/s | ETA: 07:55\n",
      "  Train: [ 9500/57525] ( 16.5%) | Loss: 0.9135 | Speed: 101.3 batches/s | ETA: 07:54\n",
      "  Train: [ 9600/57525] ( 16.7%) | Loss: 0.9130 | Speed: 101.3 batches/s | ETA: 07:53\n",
      "  Train: [ 9700/57525] ( 16.9%) | Loss: 0.9122 | Speed: 101.3 batches/s | ETA: 07:51\n",
      "  Train: [ 9800/57525] ( 17.0%) | Loss: 0.9116 | Speed: 101.4 batches/s | ETA: 07:50\n",
      "  Train: [ 9900/57525] ( 17.2%) | Loss: 0.9112 | Speed: 101.4 batches/s | ETA: 07:49\n",
      "  Train: [10000/57525] ( 17.4%) | Loss: 0.9106 | Speed: 101.5 batches/s | ETA: 07:48\n",
      "  Train: [10100/57525] ( 17.6%) | Loss: 0.9099 | Speed: 101.5 batches/s | ETA: 07:47\n",
      "  Train: [10200/57525] ( 17.7%) | Loss: 0.9094 | Speed: 101.6 batches/s | ETA: 07:45\n",
      "  Train: [10300/57525] ( 17.9%) | Loss: 0.9085 | Speed: 101.6 batches/s | ETA: 07:44\n",
      "  Train: [10400/57525] ( 18.1%) | Loss: 0.9080 | Speed: 101.7 batches/s | ETA: 07:43\n",
      "  Train: [10500/57525] ( 18.3%) | Loss: 0.9074 | Speed: 101.7 batches/s | ETA: 07:42\n",
      "  Train: [10600/57525] ( 18.4%) | Loss: 0.9066 | Speed: 101.8 batches/s | ETA: 07:41\n",
      "  Train: [10700/57525] ( 18.6%) | Loss: 0.9061 | Speed: 101.8 batches/s | ETA: 07:39\n",
      "  Train: [10800/57525] ( 18.8%) | Loss: 0.9055 | Speed: 101.8 batches/s | ETA: 07:38\n",
      "  Train: [10900/57525] ( 18.9%) | Loss: 0.9052 | Speed: 101.9 batches/s | ETA: 07:37\n",
      "  Train: [11000/57525] ( 19.1%) | Loss: 0.9044 | Speed: 101.9 batches/s | ETA: 07:36\n",
      "  Train: [11100/57525] ( 19.3%) | Loss: 0.9038 | Speed: 102.0 batches/s | ETA: 07:35\n",
      "  Train: [11200/57525] ( 19.5%) | Loss: 0.9033 | Speed: 102.0 batches/s | ETA: 07:34\n",
      "  Train: [11300/57525] ( 19.6%) | Loss: 0.9027 | Speed: 102.0 batches/s | ETA: 07:33\n",
      "  Train: [11400/57525] ( 19.8%) | Loss: 0.9022 | Speed: 102.1 batches/s | ETA: 07:31\n",
      "  Train: [11500/57525] ( 20.0%) | Loss: 0.9017 | Speed: 102.1 batches/s | ETA: 07:30\n",
      "  Train: [11600/57525] ( 20.2%) | Loss: 0.9013 | Speed: 102.1 batches/s | ETA: 07:29\n",
      "  Train: [11700/57525] ( 20.3%) | Loss: 0.9012 | Speed: 102.2 batches/s | ETA: 07:28\n",
      "  Train: [11800/57525] ( 20.5%) | Loss: 0.9008 | Speed: 102.2 batches/s | ETA: 07:27\n",
      "  Train: [11900/57525] ( 20.7%) | Loss: 0.9001 | Speed: 102.2 batches/s | ETA: 07:26\n",
      "  Train: [12000/57525] ( 20.9%) | Loss: 0.8996 | Speed: 102.2 batches/s | ETA: 07:25\n",
      "  Train: [12100/57525] ( 21.0%) | Loss: 0.8991 | Speed: 102.2 batches/s | ETA: 07:24\n",
      "  Train: [12200/57525] ( 21.2%) | Loss: 0.8985 | Speed: 102.2 batches/s | ETA: 07:23\n",
      "  Train: [12300/57525] ( 21.4%) | Loss: 0.8978 | Speed: 102.2 batches/s | ETA: 07:22\n",
      "  Train: [12400/57525] ( 21.6%) | Loss: 0.8974 | Speed: 102.3 batches/s | ETA: 07:21\n",
      "  Train: [12500/57525] ( 21.7%) | Loss: 0.8967 | Speed: 102.3 batches/s | ETA: 07:20\n",
      "  Train: [12600/57525] ( 21.9%) | Loss: 0.8965 | Speed: 102.3 batches/s | ETA: 07:19\n",
      "  Train: [12700/57525] ( 22.1%) | Loss: 0.8959 | Speed: 102.3 batches/s | ETA: 07:18\n",
      "  Train: [12800/57525] ( 22.3%) | Loss: 0.8955 | Speed: 102.3 batches/s | ETA: 07:17\n",
      "  Train: [12900/57525] ( 22.4%) | Loss: 0.8949 | Speed: 102.3 batches/s | ETA: 07:16\n",
      "  Train: [13000/57525] ( 22.6%) | Loss: 0.8944 | Speed: 102.3 batches/s | ETA: 07:15\n",
      "  Train: [13100/57525] ( 22.8%) | Loss: 0.8937 | Speed: 102.4 batches/s | ETA: 07:14\n",
      "  Train: [13200/57525] ( 22.9%) | Loss: 0.8933 | Speed: 102.4 batches/s | ETA: 07:12\n",
      "  Train: [13300/57525] ( 23.1%) | Loss: 0.8928 | Speed: 102.4 batches/s | ETA: 07:11\n",
      "  Train: [13400/57525] ( 23.3%) | Loss: 0.8923 | Speed: 102.4 batches/s | ETA: 07:10\n",
      "  Train: [13500/57525] ( 23.5%) | Loss: 0.8918 | Speed: 102.5 batches/s | ETA: 07:09\n",
      "  Train: [13600/57525] ( 23.6%) | Loss: 0.8911 | Speed: 102.5 batches/s | ETA: 07:08\n",
      "  Train: [13700/57525] ( 23.8%) | Loss: 0.8904 | Speed: 102.5 batches/s | ETA: 07:07\n",
      "  Train: [13800/57525] ( 24.0%) | Loss: 0.8899 | Speed: 102.5 batches/s | ETA: 07:06\n",
      "  Train: [13900/57525] ( 24.2%) | Loss: 0.8896 | Speed: 102.5 batches/s | ETA: 07:05\n",
      "  Train: [14000/57525] ( 24.3%) | Loss: 0.8889 | Speed: 102.5 batches/s | ETA: 07:04\n",
      "  Train: [14100/57525] ( 24.5%) | Loss: 0.8882 | Speed: 102.6 batches/s | ETA: 07:03\n",
      "  Train: [14200/57525] ( 24.7%) | Loss: 0.8879 | Speed: 102.6 batches/s | ETA: 07:02\n",
      "  Train: [14300/57525] ( 24.9%) | Loss: 0.8874 | Speed: 102.5 batches/s | ETA: 07:01\n",
      "  Train: [14400/57525] ( 25.0%) | Loss: 0.8869 | Speed: 102.5 batches/s | ETA: 07:00\n",
      "  Train: [14500/57525] ( 25.2%) | Loss: 0.8866 | Speed: 102.5 batches/s | ETA: 06:59\n",
      "  Train: [14600/57525] ( 25.4%) | Loss: 0.8863 | Speed: 102.4 batches/s | ETA: 06:58\n",
      "  Train: [14700/57525] ( 25.6%) | Loss: 0.8859 | Speed: 102.5 batches/s | ETA: 06:57\n",
      "  Train: [14800/57525] ( 25.7%) | Loss: 0.8853 | Speed: 102.5 batches/s | ETA: 06:56\n",
      "  Train: [14900/57525] ( 25.9%) | Loss: 0.8847 | Speed: 102.5 batches/s | ETA: 06:55\n",
      "  Train: [15000/57525] ( 26.1%) | Loss: 0.8844 | Speed: 102.6 batches/s | ETA: 06:54\n",
      "  Train: [15100/57525] ( 26.2%) | Loss: 0.8838 | Speed: 102.5 batches/s | ETA: 06:53\n",
      "  Train: [15200/57525] ( 26.4%) | Loss: 0.8833 | Speed: 102.6 batches/s | ETA: 06:52\n",
      "  Train: [15300/57525] ( 26.6%) | Loss: 0.8828 | Speed: 102.5 batches/s | ETA: 06:51\n",
      "  Train: [15400/57525] ( 26.8%) | Loss: 0.8822 | Speed: 102.5 batches/s | ETA: 06:50\n",
      "  Train: [15500/57525] ( 26.9%) | Loss: 0.8817 | Speed: 102.6 batches/s | ETA: 06:49\n",
      "  Train: [15600/57525] ( 27.1%) | Loss: 0.8814 | Speed: 102.6 batches/s | ETA: 06:48\n",
      "  Train: [15700/57525] ( 27.3%) | Loss: 0.8810 | Speed: 102.6 batches/s | ETA: 06:47\n",
      "  Train: [15800/57525] ( 27.5%) | Loss: 0.8803 | Speed: 102.6 batches/s | ETA: 06:46\n",
      "  Train: [15900/57525] ( 27.6%) | Loss: 0.8800 | Speed: 102.7 batches/s | ETA: 06:45\n",
      "  Train: [16000/57525] ( 27.8%) | Loss: 0.8795 | Speed: 102.7 batches/s | ETA: 06:44\n",
      "  Train: [16100/57525] ( 28.0%) | Loss: 0.8790 | Speed: 102.7 batches/s | ETA: 06:43\n",
      "  Train: [16200/57525] ( 28.2%) | Loss: 0.8785 | Speed: 102.7 batches/s | ETA: 06:42\n",
      "  Train: [16300/57525] ( 28.3%) | Loss: 0.8780 | Speed: 102.7 batches/s | ETA: 06:41\n",
      "  Train: [16400/57525] ( 28.5%) | Loss: 0.8774 | Speed: 102.7 batches/s | ETA: 06:40\n",
      "  Train: [16500/57525] ( 28.7%) | Loss: 0.8771 | Speed: 102.7 batches/s | ETA: 06:39\n",
      "  Train: [16600/57525] ( 28.9%) | Loss: 0.8765 | Speed: 102.7 batches/s | ETA: 06:38\n",
      "  Train: [16700/57525] ( 29.0%) | Loss: 0.8760 | Speed: 102.7 batches/s | ETA: 06:37\n",
      "  Train: [16800/57525] ( 29.2%) | Loss: 0.8755 | Speed: 102.7 batches/s | ETA: 06:36\n",
      "  Train: [16900/57525] ( 29.4%) | Loss: 0.8750 | Speed: 102.7 batches/s | ETA: 06:35\n",
      "  Train: [17000/57525] ( 29.6%) | Loss: 0.8746 | Speed: 102.7 batches/s | ETA: 06:34\n",
      "  Train: [17100/57525] ( 29.7%) | Loss: 0.8740 | Speed: 102.7 batches/s | ETA: 06:33\n",
      "  Train: [17200/57525] ( 29.9%) | Loss: 0.8736 | Speed: 102.7 batches/s | ETA: 06:32\n",
      "  Train: [17300/57525] ( 30.1%) | Loss: 0.8731 | Speed: 102.7 batches/s | ETA: 06:31\n",
      "  Train: [17400/57525] ( 30.2%) | Loss: 0.8728 | Speed: 102.7 batches/s | ETA: 06:30\n",
      "  Train: [17500/57525] ( 30.4%) | Loss: 0.8721 | Speed: 102.7 batches/s | ETA: 06:29\n",
      "  Train: [17600/57525] ( 30.6%) | Loss: 0.8716 | Speed: 102.7 batches/s | ETA: 06:28\n",
      "  Train: [17700/57525] ( 30.8%) | Loss: 0.8712 | Speed: 102.7 batches/s | ETA: 06:27\n",
      "  Train: [17800/57525] ( 30.9%) | Loss: 0.8708 | Speed: 102.7 batches/s | ETA: 06:26\n",
      "  Train: [17900/57525] ( 31.1%) | Loss: 0.8703 | Speed: 102.7 batches/s | ETA: 06:25\n",
      "  Train: [18000/57525] ( 31.3%) | Loss: 0.8700 | Speed: 102.7 batches/s | ETA: 06:24\n",
      "  Train: [18100/57525] ( 31.5%) | Loss: 0.8696 | Speed: 102.7 batches/s | ETA: 06:23\n",
      "  Train: [18200/57525] ( 31.6%) | Loss: 0.8692 | Speed: 102.7 batches/s | ETA: 06:22\n",
      "  Train: [18300/57525] ( 31.8%) | Loss: 0.8687 | Speed: 102.7 batches/s | ETA: 06:21\n",
      "  Train: [18400/57525] ( 32.0%) | Loss: 0.8684 | Speed: 102.7 batches/s | ETA: 06:21\n",
      "  Train: [18500/57525] ( 32.2%) | Loss: 0.8679 | Speed: 102.7 batches/s | ETA: 06:20\n",
      "  Train: [18600/57525] ( 32.3%) | Loss: 0.8675 | Speed: 102.7 batches/s | ETA: 06:19\n",
      "  Train: [18700/57525] ( 32.5%) | Loss: 0.8670 | Speed: 102.7 batches/s | ETA: 06:18\n",
      "  Train: [18800/57525] ( 32.7%) | Loss: 0.8667 | Speed: 102.7 batches/s | ETA: 06:17\n",
      "  Train: [18900/57525] ( 32.9%) | Loss: 0.8662 | Speed: 102.7 batches/s | ETA: 06:16\n",
      "  Train: [19000/57525] ( 33.0%) | Loss: 0.8659 | Speed: 102.7 batches/s | ETA: 06:15\n",
      "  Train: [19100/57525] ( 33.2%) | Loss: 0.8654 | Speed: 102.7 batches/s | ETA: 06:14\n",
      "  Train: [19200/57525] ( 33.4%) | Loss: 0.8650 | Speed: 102.7 batches/s | ETA: 06:13\n",
      "  Train: [19300/57525] ( 33.6%) | Loss: 0.8646 | Speed: 102.7 batches/s | ETA: 06:12\n",
      "  Train: [19400/57525] ( 33.7%) | Loss: 0.8644 | Speed: 102.7 batches/s | ETA: 06:11\n",
      "  Train: [19500/57525] ( 33.9%) | Loss: 0.8640 | Speed: 102.7 batches/s | ETA: 06:10\n",
      "  Train: [19600/57525] ( 34.1%) | Loss: 0.8636 | Speed: 102.7 batches/s | ETA: 06:09\n",
      "  Train: [19700/57525] ( 34.2%) | Loss: 0.8632 | Speed: 102.7 batches/s | ETA: 06:08\n",
      "  Train: [19800/57525] ( 34.4%) | Loss: 0.8629 | Speed: 102.7 batches/s | ETA: 06:07\n",
      "  Train: [19900/57525] ( 34.6%) | Loss: 0.8624 | Speed: 102.7 batches/s | ETA: 06:06\n",
      "  Train: [20000/57525] ( 34.8%) | Loss: 0.8620 | Speed: 102.7 batches/s | ETA: 06:05\n",
      "  Train: [20100/57525] ( 34.9%) | Loss: 0.8614 | Speed: 102.7 batches/s | ETA: 06:04\n",
      "  Train: [20200/57525] ( 35.1%) | Loss: 0.8610 | Speed: 102.7 batches/s | ETA: 06:03\n",
      "  Train: [20300/57525] ( 35.3%) | Loss: 0.8607 | Speed: 102.7 batches/s | ETA: 06:02\n",
      "  Train: [20400/57525] ( 35.5%) | Loss: 0.8603 | Speed: 102.7 batches/s | ETA: 06:01\n",
      "  Train: [20500/57525] ( 35.6%) | Loss: 0.8599 | Speed: 102.7 batches/s | ETA: 06:00\n",
      "  Train: [20600/57525] ( 35.8%) | Loss: 0.8595 | Speed: 102.7 batches/s | ETA: 05:59\n",
      "  Train: [20700/57525] ( 36.0%) | Loss: 0.8591 | Speed: 102.7 batches/s | ETA: 05:58\n",
      "  Train: [20800/57525] ( 36.2%) | Loss: 0.8587 | Speed: 102.7 batches/s | ETA: 05:57\n",
      "  Train: [20900/57525] ( 36.3%) | Loss: 0.8584 | Speed: 102.8 batches/s | ETA: 05:56\n",
      "  Train: [21000/57525] ( 36.5%) | Loss: 0.8580 | Speed: 102.8 batches/s | ETA: 05:55\n",
      "  Train: [21100/57525] ( 36.7%) | Loss: 0.8577 | Speed: 102.8 batches/s | ETA: 05:54\n",
      "  Train: [21200/57525] ( 36.9%) | Loss: 0.8573 | Speed: 102.8 batches/s | ETA: 05:53\n",
      "  Train: [21300/57525] ( 37.0%) | Loss: 0.8568 | Speed: 102.7 batches/s | ETA: 05:52\n",
      "  Train: [21400/57525] ( 37.2%) | Loss: 0.8564 | Speed: 102.7 batches/s | ETA: 05:51\n",
      "  Train: [21500/57525] ( 37.4%) | Loss: 0.8561 | Speed: 102.7 batches/s | ETA: 05:50\n",
      "  Train: [21600/57525] ( 37.5%) | Loss: 0.8559 | Speed: 102.7 batches/s | ETA: 05:49\n",
      "  Train: [21700/57525] ( 37.7%) | Loss: 0.8555 | Speed: 102.7 batches/s | ETA: 05:48\n",
      "  Train: [21800/57525] ( 37.9%) | Loss: 0.8551 | Speed: 102.6 batches/s | ETA: 05:48\n",
      "  Train: [21900/57525] ( 38.1%) | Loss: 0.8547 | Speed: 102.6 batches/s | ETA: 05:47\n",
      "  Train: [22000/57525] ( 38.2%) | Loss: 0.8543 | Speed: 102.7 batches/s | ETA: 05:46\n",
      "  Train: [22100/57525] ( 38.4%) | Loss: 0.8540 | Speed: 102.7 batches/s | ETA: 05:45\n",
      "  Train: [22200/57525] ( 38.6%) | Loss: 0.8536 | Speed: 102.6 batches/s | ETA: 05:44\n",
      "  Train: [22300/57525] ( 38.8%) | Loss: 0.8532 | Speed: 102.7 batches/s | ETA: 05:43\n",
      "  Train: [22400/57525] ( 38.9%) | Loss: 0.8528 | Speed: 102.6 batches/s | ETA: 05:42\n",
      "  Train: [22500/57525] ( 39.1%) | Loss: 0.8525 | Speed: 102.6 batches/s | ETA: 05:41\n",
      "  Train: [22600/57525] ( 39.3%) | Loss: 0.8521 | Speed: 102.7 batches/s | ETA: 05:40\n",
      "  Train: [22700/57525] ( 39.5%) | Loss: 0.8519 | Speed: 102.7 batches/s | ETA: 05:39\n",
      "  Train: [22800/57525] ( 39.6%) | Loss: 0.8515 | Speed: 102.7 batches/s | ETA: 05:38\n",
      "  Train: [22900/57525] ( 39.8%) | Loss: 0.8510 | Speed: 102.7 batches/s | ETA: 05:37\n",
      "  Train: [23000/57525] ( 40.0%) | Loss: 0.8508 | Speed: 102.7 batches/s | ETA: 05:36\n",
      "  Train: [23100/57525] ( 40.2%) | Loss: 0.8504 | Speed: 102.6 batches/s | ETA: 05:35\n",
      "  Train: [23200/57525] ( 40.3%) | Loss: 0.8501 | Speed: 102.7 batches/s | ETA: 05:34\n",
      "  Train: [23300/57525] ( 40.5%) | Loss: 0.8497 | Speed: 102.7 batches/s | ETA: 05:33\n",
      "  Train: [23400/57525] ( 40.7%) | Loss: 0.8494 | Speed: 102.7 batches/s | ETA: 05:32\n",
      "  Train: [23500/57525] ( 40.9%) | Loss: 0.8490 | Speed: 102.7 batches/s | ETA: 05:31\n",
      "  Train: [23600/57525] ( 41.0%) | Loss: 0.8488 | Speed: 102.7 batches/s | ETA: 05:30\n",
      "  Train: [23700/57525] ( 41.2%) | Loss: 0.8484 | Speed: 102.7 batches/s | ETA: 05:29\n",
      "  Train: [23800/57525] ( 41.4%) | Loss: 0.8480 | Speed: 102.7 batches/s | ETA: 05:28\n",
      "  Train: [23900/57525] ( 41.5%) | Loss: 0.8476 | Speed: 102.7 batches/s | ETA: 05:27\n",
      "  Train: [24000/57525] ( 41.7%) | Loss: 0.8473 | Speed: 102.7 batches/s | ETA: 05:26\n",
      "  Train: [24100/57525] ( 41.9%) | Loss: 0.8469 | Speed: 102.7 batches/s | ETA: 05:25\n",
      "  Train: [24200/57525] ( 42.1%) | Loss: 0.8465 | Speed: 102.7 batches/s | ETA: 05:24\n",
      "  Train: [24300/57525] ( 42.2%) | Loss: 0.8461 | Speed: 102.7 batches/s | ETA: 05:23\n",
      "  Train: [24400/57525] ( 42.4%) | Loss: 0.8457 | Speed: 102.7 batches/s | ETA: 05:22\n",
      "  Train: [24500/57525] ( 42.6%) | Loss: 0.8454 | Speed: 102.7 batches/s | ETA: 05:21\n",
      "  Train: [24600/57525] ( 42.8%) | Loss: 0.8451 | Speed: 102.7 batches/s | ETA: 05:20\n",
      "  Train: [24700/57525] ( 42.9%) | Loss: 0.8448 | Speed: 102.7 batches/s | ETA: 05:19\n",
      "  Train: [24800/57525] ( 43.1%) | Loss: 0.8446 | Speed: 102.7 batches/s | ETA: 05:18\n",
      "  Train: [24900/57525] ( 43.3%) | Loss: 0.8442 | Speed: 102.7 batches/s | ETA: 05:17\n",
      "  Train: [25000/57525] ( 43.5%) | Loss: 0.8439 | Speed: 102.7 batches/s | ETA: 05:16\n",
      "  Train: [25100/57525] ( 43.6%) | Loss: 0.8435 | Speed: 102.8 batches/s | ETA: 05:15\n",
      "  Train: [25200/57525] ( 43.8%) | Loss: 0.8432 | Speed: 102.8 batches/s | ETA: 05:14\n",
      "  Train: [25300/57525] ( 44.0%) | Loss: 0.8429 | Speed: 102.8 batches/s | ETA: 05:13\n",
      "  Train: [25400/57525] ( 44.2%) | Loss: 0.8425 | Speed: 102.8 batches/s | ETA: 05:12\n",
      "  Train: [25500/57525] ( 44.3%) | Loss: 0.8422 | Speed: 102.8 batches/s | ETA: 05:11\n",
      "  Train: [25600/57525] ( 44.5%) | Loss: 0.8419 | Speed: 102.8 batches/s | ETA: 05:10\n",
      "  Train: [25700/57525] ( 44.7%) | Loss: 0.8415 | Speed: 102.8 batches/s | ETA: 05:09\n",
      "  Train: [25800/57525] ( 44.9%) | Loss: 0.8412 | Speed: 102.8 batches/s | ETA: 05:08\n",
      "  Train: [25900/57525] ( 45.0%) | Loss: 0.8408 | Speed: 102.8 batches/s | ETA: 05:07\n",
      "  Train: [26000/57525] ( 45.2%) | Loss: 0.8405 | Speed: 102.8 batches/s | ETA: 05:06\n",
      "  Train: [26100/57525] ( 45.4%) | Loss: 0.8402 | Speed: 102.8 batches/s | ETA: 05:05\n",
      "  Train: [26200/57525] ( 45.5%) | Loss: 0.8398 | Speed: 102.8 batches/s | ETA: 05:04\n",
      "  Train: [26300/57525] ( 45.7%) | Loss: 0.8394 | Speed: 102.8 batches/s | ETA: 05:03\n",
      "  Train: [26400/57525] ( 45.9%) | Loss: 0.8391 | Speed: 102.8 batches/s | ETA: 05:02\n",
      "  Train: [26500/57525] ( 46.1%) | Loss: 0.8388 | Speed: 102.8 batches/s | ETA: 05:01\n",
      "  Train: [26600/57525] ( 46.2%) | Loss: 0.8386 | Speed: 102.8 batches/s | ETA: 05:00\n",
      "  Train: [26700/57525] ( 46.4%) | Loss: 0.8382 | Speed: 102.8 batches/s | ETA: 04:59\n",
      "  Train: [26800/57525] ( 46.6%) | Loss: 0.8380 | Speed: 102.8 batches/s | ETA: 04:58\n",
      "  Train: [26900/57525] ( 46.8%) | Loss: 0.8376 | Speed: 102.8 batches/s | ETA: 04:57\n",
      "  Train: [27000/57525] ( 46.9%) | Loss: 0.8373 | Speed: 102.8 batches/s | ETA: 04:56\n",
      "  Train: [27100/57525] ( 47.1%) | Loss: 0.8369 | Speed: 102.8 batches/s | ETA: 04:55\n",
      "  Train: [27200/57525] ( 47.3%) | Loss: 0.8365 | Speed: 102.8 batches/s | ETA: 04:54\n",
      "  Train: [27300/57525] ( 47.5%) | Loss: 0.8362 | Speed: 102.8 batches/s | ETA: 04:53\n",
      "  Train: [27400/57525] ( 47.6%) | Loss: 0.8358 | Speed: 102.8 batches/s | ETA: 04:52\n",
      "  Train: [27500/57525] ( 47.8%) | Loss: 0.8355 | Speed: 102.8 batches/s | ETA: 04:51\n",
      "  Train: [27600/57525] ( 48.0%) | Loss: 0.8352 | Speed: 102.8 batches/s | ETA: 04:50\n",
      "  Train: [27700/57525] ( 48.2%) | Loss: 0.8349 | Speed: 102.9 batches/s | ETA: 04:49\n",
      "  Train: [27800/57525] ( 48.3%) | Loss: 0.8346 | Speed: 102.9 batches/s | ETA: 04:48\n",
      "  Train: [27900/57525] ( 48.5%) | Loss: 0.8342 | Speed: 102.9 batches/s | ETA: 04:47\n",
      "  Train: [28000/57525] ( 48.7%) | Loss: 0.8340 | Speed: 102.9 batches/s | ETA: 04:46\n",
      "  Train: [28100/57525] ( 48.8%) | Loss: 0.8337 | Speed: 102.9 batches/s | ETA: 04:45\n",
      "  Train: [28200/57525] ( 49.0%) | Loss: 0.8334 | Speed: 102.9 batches/s | ETA: 04:44\n",
      "  Train: [28300/57525] ( 49.2%) | Loss: 0.8330 | Speed: 102.9 batches/s | ETA: 04:44\n",
      "  Train: [28400/57525] ( 49.4%) | Loss: 0.8326 | Speed: 102.9 batches/s | ETA: 04:43\n",
      "  Train: [28500/57525] ( 49.5%) | Loss: 0.8324 | Speed: 102.9 batches/s | ETA: 04:42\n",
      "  Train: [28600/57525] ( 49.7%) | Loss: 0.8321 | Speed: 102.9 batches/s | ETA: 04:41\n",
      "  Train: [28700/57525] ( 49.9%) | Loss: 0.8319 | Speed: 102.9 batches/s | ETA: 04:40\n",
      "  Train: [28800/57525] ( 50.1%) | Loss: 0.8316 | Speed: 102.9 batches/s | ETA: 04:39\n",
      "  Train: [28900/57525] ( 50.2%) | Loss: 0.8313 | Speed: 102.9 batches/s | ETA: 04:38\n",
      "  Train: [29000/57525] ( 50.4%) | Loss: 0.8310 | Speed: 102.9 batches/s | ETA: 04:37\n",
      "  Train: [29100/57525] ( 50.6%) | Loss: 0.8307 | Speed: 102.9 batches/s | ETA: 04:36\n",
      "  Train: [29200/57525] ( 50.8%) | Loss: 0.8303 | Speed: 102.9 batches/s | ETA: 04:35\n",
      "  Train: [29300/57525] ( 50.9%) | Loss: 0.8300 | Speed: 102.9 batches/s | ETA: 04:34\n",
      "  Train: [29400/57525] ( 51.1%) | Loss: 0.8297 | Speed: 102.9 batches/s | ETA: 04:33\n",
      "  Train: [29500/57525] ( 51.3%) | Loss: 0.8294 | Speed: 102.9 batches/s | ETA: 04:32\n",
      "  Train: [29600/57525] ( 51.5%) | Loss: 0.8293 | Speed: 102.9 batches/s | ETA: 04:31\n",
      "  Train: [29700/57525] ( 51.6%) | Loss: 0.8291 | Speed: 102.9 batches/s | ETA: 04:30\n",
      "  Train: [29800/57525] ( 51.8%) | Loss: 0.8287 | Speed: 102.9 batches/s | ETA: 04:29\n",
      "  Train: [29900/57525] ( 52.0%) | Loss: 0.8285 | Speed: 102.9 batches/s | ETA: 04:28\n",
      "  Train: [30000/57525] ( 52.2%) | Loss: 0.8282 | Speed: 102.9 batches/s | ETA: 04:27\n",
      "  Train: [30100/57525] ( 52.3%) | Loss: 0.8279 | Speed: 102.9 batches/s | ETA: 04:26\n",
      "  Train: [30200/57525] ( 52.5%) | Loss: 0.8276 | Speed: 102.9 batches/s | ETA: 04:25\n",
      "  Train: [30300/57525] ( 52.7%) | Loss: 0.8273 | Speed: 102.9 batches/s | ETA: 04:24\n",
      "  Train: [30400/57525] ( 52.8%) | Loss: 0.8270 | Speed: 102.9 batches/s | ETA: 04:23\n",
      "  Train: [30500/57525] ( 53.0%) | Loss: 0.8267 | Speed: 102.9 batches/s | ETA: 04:22\n",
      "  Train: [30600/57525] ( 53.2%) | Loss: 0.8265 | Speed: 102.9 batches/s | ETA: 04:21\n",
      "  Train: [30700/57525] ( 53.4%) | Loss: 0.8263 | Speed: 102.9 batches/s | ETA: 04:20\n",
      "  Train: [30800/57525] ( 53.5%) | Loss: 0.8260 | Speed: 102.9 batches/s | ETA: 04:19\n",
      "  Train: [30900/57525] ( 53.7%) | Loss: 0.8256 | Speed: 102.9 batches/s | ETA: 04:18\n",
      "  Train: [31000/57525] ( 53.9%) | Loss: 0.8253 | Speed: 102.9 batches/s | ETA: 04:17\n",
      "  Train: [31100/57525] ( 54.1%) | Loss: 0.8250 | Speed: 102.9 batches/s | ETA: 04:16\n",
      "  Train: [31200/57525] ( 54.2%) | Loss: 0.8248 | Speed: 102.9 batches/s | ETA: 04:15\n",
      "  Train: [31300/57525] ( 54.4%) | Loss: 0.8246 | Speed: 102.9 batches/s | ETA: 04:14\n",
      "  Train: [31400/57525] ( 54.6%) | Loss: 0.8244 | Speed: 102.9 batches/s | ETA: 04:13\n",
      "  Train: [31500/57525] ( 54.8%) | Loss: 0.8242 | Speed: 102.9 batches/s | ETA: 04:12\n",
      "  Train: [31600/57525] ( 54.9%) | Loss: 0.8239 | Speed: 102.9 batches/s | ETA: 04:11\n",
      "  Train: [31700/57525] ( 55.1%) | Loss: 0.8235 | Speed: 102.9 batches/s | ETA: 04:10\n",
      "  Train: [31800/57525] ( 55.3%) | Loss: 0.8233 | Speed: 102.9 batches/s | ETA: 04:09\n",
      "  Train: [31900/57525] ( 55.5%) | Loss: 0.8230 | Speed: 102.9 batches/s | ETA: 04:08\n",
      "  Train: [32000/57525] ( 55.6%) | Loss: 0.8228 | Speed: 102.9 batches/s | ETA: 04:07\n",
      "  Train: [32100/57525] ( 55.8%) | Loss: 0.8225 | Speed: 102.9 batches/s | ETA: 04:06\n",
      "  Train: [32200/57525] ( 56.0%) | Loss: 0.8222 | Speed: 103.0 batches/s | ETA: 04:05\n",
      "  Train: [32300/57525] ( 56.1%) | Loss: 0.8220 | Speed: 103.0 batches/s | ETA: 04:05\n",
      "  Train: [32400/57525] ( 56.3%) | Loss: 0.8218 | Speed: 103.0 batches/s | ETA: 04:04\n",
      "  Train: [32500/57525] ( 56.5%) | Loss: 0.8215 | Speed: 103.0 batches/s | ETA: 04:03\n",
      "  Train: [32600/57525] ( 56.7%) | Loss: 0.8212 | Speed: 103.0 batches/s | ETA: 04:02\n",
      "  Train: [32700/57525] ( 56.8%) | Loss: 0.8210 | Speed: 103.0 batches/s | ETA: 04:01\n",
      "  Train: [32800/57525] ( 57.0%) | Loss: 0.8208 | Speed: 103.0 batches/s | ETA: 04:00\n",
      "  Train: [32900/57525] ( 57.2%) | Loss: 0.8204 | Speed: 103.0 batches/s | ETA: 03:59\n",
      "  Train: [33000/57525] ( 57.4%) | Loss: 0.8202 | Speed: 103.0 batches/s | ETA: 03:58\n",
      "  Train: [33100/57525] ( 57.5%) | Loss: 0.8198 | Speed: 103.0 batches/s | ETA: 03:57\n",
      "  Train: [33200/57525] ( 57.7%) | Loss: 0.8196 | Speed: 103.1 batches/s | ETA: 03:56\n",
      "  Train: [33300/57525] ( 57.9%) | Loss: 0.8194 | Speed: 103.1 batches/s | ETA: 03:55\n",
      "  Train: [33400/57525] ( 58.1%) | Loss: 0.8191 | Speed: 103.1 batches/s | ETA: 03:54\n",
      "  Train: [33500/57525] ( 58.2%) | Loss: 0.8190 | Speed: 103.1 batches/s | ETA: 03:53\n",
      "  Train: [33600/57525] ( 58.4%) | Loss: 0.8188 | Speed: 103.1 batches/s | ETA: 03:52\n",
      "  Train: [33700/57525] ( 58.6%) | Loss: 0.8185 | Speed: 103.1 batches/s | ETA: 03:51\n",
      "  Train: [33800/57525] ( 58.8%) | Loss: 0.8184 | Speed: 103.1 batches/s | ETA: 03:50\n",
      "  Train: [33900/57525] ( 58.9%) | Loss: 0.8182 | Speed: 103.1 batches/s | ETA: 03:49\n",
      "  Train: [34000/57525] ( 59.1%) | Loss: 0.8180 | Speed: 103.1 batches/s | ETA: 03:48\n",
      "  Train: [34100/57525] ( 59.3%) | Loss: 0.8177 | Speed: 103.1 batches/s | ETA: 03:47\n",
      "  Train: [34200/57525] ( 59.5%) | Loss: 0.8174 | Speed: 103.1 batches/s | ETA: 03:46\n",
      "  Train: [34300/57525] ( 59.6%) | Loss: 0.8171 | Speed: 103.1 batches/s | ETA: 03:45\n",
      "  Train: [34400/57525] ( 59.8%) | Loss: 0.8169 | Speed: 103.1 batches/s | ETA: 03:44\n",
      "  Train: [34500/57525] ( 60.0%) | Loss: 0.8166 | Speed: 103.1 batches/s | ETA: 03:43\n",
      "  Train: [34600/57525] ( 60.1%) | Loss: 0.8164 | Speed: 103.1 batches/s | ETA: 03:42\n",
      "  Train: [34700/57525] ( 60.3%) | Loss: 0.8161 | Speed: 103.1 batches/s | ETA: 03:41\n",
      "  Train: [34800/57525] ( 60.5%) | Loss: 0.8159 | Speed: 103.1 batches/s | ETA: 03:40\n",
      "  Train: [34900/57525] ( 60.7%) | Loss: 0.8157 | Speed: 103.1 batches/s | ETA: 03:39\n",
      "  Train: [35000/57525] ( 60.8%) | Loss: 0.8156 | Speed: 103.1 batches/s | ETA: 03:38\n",
      "  Train: [35100/57525] ( 61.0%) | Loss: 0.8153 | Speed: 103.1 batches/s | ETA: 03:37\n",
      "  Train: [35200/57525] ( 61.2%) | Loss: 0.8151 | Speed: 103.1 batches/s | ETA: 03:36\n",
      "  Train: [35300/57525] ( 61.4%) | Loss: 0.8148 | Speed: 103.1 batches/s | ETA: 03:35\n",
      "  Train: [35400/57525] ( 61.5%) | Loss: 0.8146 | Speed: 103.1 batches/s | ETA: 03:34\n",
      "  Train: [35500/57525] ( 61.7%) | Loss: 0.8144 | Speed: 103.1 batches/s | ETA: 03:33\n",
      "  Train: [35600/57525] ( 61.9%) | Loss: 0.8142 | Speed: 103.1 batches/s | ETA: 03:32\n",
      "  Train: [35700/57525] ( 62.1%) | Loss: 0.8139 | Speed: 103.1 batches/s | ETA: 03:31\n",
      "  Train: [35800/57525] ( 62.2%) | Loss: 0.8137 | Speed: 103.1 batches/s | ETA: 03:30\n",
      "  Train: [35900/57525] ( 62.4%) | Loss: 0.8135 | Speed: 103.1 batches/s | ETA: 03:29\n",
      "  Train: [36000/57525] ( 62.6%) | Loss: 0.8132 | Speed: 103.1 batches/s | ETA: 03:28\n",
      "  Train: [36100/57525] ( 62.8%) | Loss: 0.8130 | Speed: 103.1 batches/s | ETA: 03:27\n",
      "  Train: [36200/57525] ( 62.9%) | Loss: 0.8128 | Speed: 103.1 batches/s | ETA: 03:26\n",
      "  Train: [36300/57525] ( 63.1%) | Loss: 0.8125 | Speed: 103.1 batches/s | ETA: 03:25\n",
      "  Train: [36400/57525] ( 63.3%) | Loss: 0.8123 | Speed: 103.1 batches/s | ETA: 03:24\n",
      "  Train: [36500/57525] ( 63.5%) | Loss: 0.8120 | Speed: 103.1 batches/s | ETA: 03:23\n",
      "  Train: [36600/57525] ( 63.6%) | Loss: 0.8118 | Speed: 103.1 batches/s | ETA: 03:22\n",
      "  Train: [36700/57525] ( 63.8%) | Loss: 0.8115 | Speed: 103.1 batches/s | ETA: 03:21\n",
      "  Train: [36800/57525] ( 64.0%) | Loss: 0.8112 | Speed: 103.2 batches/s | ETA: 03:20\n",
      "  Train: [36900/57525] ( 64.1%) | Loss: 0.8110 | Speed: 103.2 batches/s | ETA: 03:19\n",
      "  Train: [37000/57525] ( 64.3%) | Loss: 0.8109 | Speed: 103.2 batches/s | ETA: 03:18\n",
      "  Train: [37100/57525] ( 64.5%) | Loss: 0.8107 | Speed: 103.2 batches/s | ETA: 03:17\n",
      "  Train: [37200/57525] ( 64.7%) | Loss: 0.8105 | Speed: 103.2 batches/s | ETA: 03:17\n",
      "  Train: [37300/57525] ( 64.8%) | Loss: 0.8103 | Speed: 103.2 batches/s | ETA: 03:16\n",
      "  Train: [37400/57525] ( 65.0%) | Loss: 0.8100 | Speed: 103.2 batches/s | ETA: 03:15\n",
      "  Train: [37500/57525] ( 65.2%) | Loss: 0.8098 | Speed: 103.2 batches/s | ETA: 03:14\n",
      "  Train: [37600/57525] ( 65.4%) | Loss: 0.8096 | Speed: 103.2 batches/s | ETA: 03:13\n",
      "  Train: [37700/57525] ( 65.5%) | Loss: 0.8094 | Speed: 103.2 batches/s | ETA: 03:12\n",
      "  Train: [37800/57525] ( 65.7%) | Loss: 0.8092 | Speed: 103.2 batches/s | ETA: 03:11\n",
      "  Train: [37900/57525] ( 65.9%) | Loss: 0.8089 | Speed: 103.2 batches/s | ETA: 03:10\n",
      "  Train: [38000/57525] ( 66.1%) | Loss: 0.8086 | Speed: 103.2 batches/s | ETA: 03:09\n",
      "  Train: [38100/57525] ( 66.2%) | Loss: 0.8084 | Speed: 103.2 batches/s | ETA: 03:08\n",
      "  Train: [38200/57525] ( 66.4%) | Loss: 0.8081 | Speed: 103.2 batches/s | ETA: 03:07\n",
      "  Train: [38300/57525] ( 66.6%) | Loss: 0.8079 | Speed: 103.2 batches/s | ETA: 03:06\n",
      "  Train: [38400/57525] ( 66.8%) | Loss: 0.8077 | Speed: 103.2 batches/s | ETA: 03:05\n",
      "  Train: [38500/57525] ( 66.9%) | Loss: 0.8074 | Speed: 103.2 batches/s | ETA: 03:04\n",
      "  Train: [38600/57525] ( 67.1%) | Loss: 0.8072 | Speed: 103.2 batches/s | ETA: 03:03\n",
      "  Train: [38700/57525] ( 67.3%) | Loss: 0.8070 | Speed: 103.2 batches/s | ETA: 03:02\n",
      "  Train: [38800/57525] ( 67.4%) | Loss: 0.8069 | Speed: 103.2 batches/s | ETA: 03:01\n",
      "  Train: [38900/57525] ( 67.6%) | Loss: 0.8066 | Speed: 103.2 batches/s | ETA: 03:00\n",
      "  Train: [39000/57525] ( 67.8%) | Loss: 0.8065 | Speed: 103.2 batches/s | ETA: 02:59\n",
      "  Train: [39100/57525] ( 68.0%) | Loss: 0.8063 | Speed: 103.2 batches/s | ETA: 02:58\n",
      "  Train: [39200/57525] ( 68.1%) | Loss: 0.8061 | Speed: 103.2 batches/s | ETA: 02:57\n",
      "  Train: [39300/57525] ( 68.3%) | Loss: 0.8059 | Speed: 103.1 batches/s | ETA: 02:56\n",
      "  Train: [39400/57525] ( 68.5%) | Loss: 0.8057 | Speed: 103.1 batches/s | ETA: 02:55\n",
      "  Train: [39500/57525] ( 68.7%) | Loss: 0.8054 | Speed: 103.1 batches/s | ETA: 02:54\n",
      "  Train: [39600/57525] ( 68.8%) | Loss: 0.8052 | Speed: 103.1 batches/s | ETA: 02:53\n",
      "  Train: [39700/57525] ( 69.0%) | Loss: 0.8051 | Speed: 103.1 batches/s | ETA: 02:52\n",
      "  Train: [39800/57525] ( 69.2%) | Loss: 0.8048 | Speed: 103.1 batches/s | ETA: 02:51\n",
      "  Train: [39900/57525] ( 69.4%) | Loss: 0.8047 | Speed: 103.1 batches/s | ETA: 02:51\n",
      "  Train: [40000/57525] ( 69.5%) | Loss: 0.8045 | Speed: 103.0 batches/s | ETA: 02:50\n",
      "  Train: [40100/57525] ( 69.7%) | Loss: 0.8044 | Speed: 103.0 batches/s | ETA: 02:49\n",
      "  Train: [40200/57525] ( 69.9%) | Loss: 0.8042 | Speed: 103.0 batches/s | ETA: 02:48\n",
      "  Train: [40300/57525] ( 70.1%) | Loss: 0.8040 | Speed: 103.0 batches/s | ETA: 02:47\n",
      "  Train: [40400/57525] ( 70.2%) | Loss: 0.8038 | Speed: 103.0 batches/s | ETA: 02:46\n",
      "  Train: [40500/57525] ( 70.4%) | Loss: 0.8036 | Speed: 103.0 batches/s | ETA: 02:45\n",
      "  Train: [40600/57525] ( 70.6%) | Loss: 0.8034 | Speed: 102.9 batches/s | ETA: 02:44\n",
      "  Train: [40700/57525] ( 70.8%) | Loss: 0.8032 | Speed: 102.9 batches/s | ETA: 02:43\n",
      "  Train: [40800/57525] ( 70.9%) | Loss: 0.8030 | Speed: 102.9 batches/s | ETA: 02:42\n",
      "  Train: [40900/57525] ( 71.1%) | Loss: 0.8028 | Speed: 102.9 batches/s | ETA: 02:41\n",
      "  Train: [41000/57525] ( 71.3%) | Loss: 0.8026 | Speed: 102.9 batches/s | ETA: 02:40\n",
      "  Train: [41100/57525] ( 71.4%) | Loss: 0.8024 | Speed: 102.9 batches/s | ETA: 02:39\n",
      "  Train: [41200/57525] ( 71.6%) | Loss: 0.8022 | Speed: 102.9 batches/s | ETA: 02:38\n",
      "  Train: [41300/57525] ( 71.8%) | Loss: 0.8020 | Speed: 102.8 batches/s | ETA: 02:37\n",
      "  Train: [41400/57525] ( 72.0%) | Loss: 0.8018 | Speed: 102.8 batches/s | ETA: 02:36\n",
      "  Train: [41500/57525] ( 72.1%) | Loss: 0.8016 | Speed: 102.8 batches/s | ETA: 02:35\n",
      "  Train: [41600/57525] ( 72.3%) | Loss: 0.8014 | Speed: 102.8 batches/s | ETA: 02:34\n",
      "  Train: [41700/57525] ( 72.5%) | Loss: 0.8012 | Speed: 102.8 batches/s | ETA: 02:33\n",
      "  Train: [41800/57525] ( 72.7%) | Loss: 0.8010 | Speed: 102.8 batches/s | ETA: 02:32\n",
      "  Train: [41900/57525] ( 72.8%) | Loss: 0.8008 | Speed: 102.8 batches/s | ETA: 02:32\n",
      "  Train: [42000/57525] ( 73.0%) | Loss: 0.8007 | Speed: 102.7 batches/s | ETA: 02:31\n",
      "  Train: [42100/57525] ( 73.2%) | Loss: 0.8005 | Speed: 102.7 batches/s | ETA: 02:30\n",
      "  Train: [42200/57525] ( 73.4%) | Loss: 0.8003 | Speed: 102.7 batches/s | ETA: 02:29\n",
      "  Train: [42300/57525] ( 73.5%) | Loss: 0.8001 | Speed: 102.7 batches/s | ETA: 02:28\n",
      "  Train: [42400/57525] ( 73.7%) | Loss: 0.8000 | Speed: 102.7 batches/s | ETA: 02:27\n",
      "  Train: [42500/57525] ( 73.9%) | Loss: 0.7998 | Speed: 102.7 batches/s | ETA: 02:26\n",
      "  Train: [42600/57525] ( 74.1%) | Loss: 0.7996 | Speed: 102.6 batches/s | ETA: 02:25\n",
      "  Train: [42700/57525] ( 74.2%) | Loss: 0.7994 | Speed: 102.6 batches/s | ETA: 02:24\n",
      "  Train: [42800/57525] ( 74.4%) | Loss: 0.7993 | Speed: 102.6 batches/s | ETA: 02:23\n",
      "  Train: [42900/57525] ( 74.6%) | Loss: 0.7991 | Speed: 102.6 batches/s | ETA: 02:22\n",
      "  Train: [43000/57525] ( 74.8%) | Loss: 0.7989 | Speed: 102.6 batches/s | ETA: 02:21\n",
      "  Train: [43100/57525] ( 74.9%) | Loss: 0.7987 | Speed: 102.6 batches/s | ETA: 02:20\n",
      "  Train: [43200/57525] ( 75.1%) | Loss: 0.7985 | Speed: 102.6 batches/s | ETA: 02:19\n",
      "  Train: [43300/57525] ( 75.3%) | Loss: 0.7983 | Speed: 102.5 batches/s | ETA: 02:18\n",
      "  Train: [43400/57525] ( 75.4%) | Loss: 0.7982 | Speed: 102.5 batches/s | ETA: 02:17\n",
      "  Train: [43500/57525] ( 75.6%) | Loss: 0.7980 | Speed: 102.5 batches/s | ETA: 02:16\n",
      "  Train: [43600/57525] ( 75.8%) | Loss: 0.7977 | Speed: 102.5 batches/s | ETA: 02:15\n",
      "  Train: [43700/57525] ( 76.0%) | Loss: 0.7975 | Speed: 102.5 batches/s | ETA: 02:14\n",
      "  Train: [43800/57525] ( 76.1%) | Loss: 0.7973 | Speed: 102.5 batches/s | ETA: 02:13\n",
      "  Train: [43900/57525] ( 76.3%) | Loss: 0.7971 | Speed: 102.5 batches/s | ETA: 02:12\n",
      "  Train: [44000/57525] ( 76.5%) | Loss: 0.7970 | Speed: 102.5 batches/s | ETA: 02:11\n",
      "  Train: [44100/57525] ( 76.7%) | Loss: 0.7967 | Speed: 102.5 batches/s | ETA: 02:10\n",
      "  Train: [44200/57525] ( 76.8%) | Loss: 0.7966 | Speed: 102.5 batches/s | ETA: 02:09\n",
      "  Train: [44300/57525] ( 77.0%) | Loss: 0.7964 | Speed: 102.5 batches/s | ETA: 02:08\n",
      "  Train: [44400/57525] ( 77.2%) | Loss: 0.7962 | Speed: 102.5 batches/s | ETA: 02:08\n",
      "  Train: [44500/57525] ( 77.4%) | Loss: 0.7961 | Speed: 102.5 batches/s | ETA: 02:07\n",
      "  Train: [44600/57525] ( 77.5%) | Loss: 0.7959 | Speed: 102.5 batches/s | ETA: 02:06\n",
      "  Train: [44700/57525] ( 77.7%) | Loss: 0.7957 | Speed: 102.5 batches/s | ETA: 02:05\n",
      "  Train: [44800/57525] ( 77.9%) | Loss: 0.7956 | Speed: 102.5 batches/s | ETA: 02:04\n",
      "  Train: [44900/57525] ( 78.1%) | Loss: 0.7954 | Speed: 102.5 batches/s | ETA: 02:03\n",
      "  Train: [45000/57525] ( 78.2%) | Loss: 0.7952 | Speed: 102.5 batches/s | ETA: 02:02\n",
      "  Train: [45100/57525] ( 78.4%) | Loss: 0.7950 | Speed: 102.5 batches/s | ETA: 02:01\n",
      "  Train: [45200/57525] ( 78.6%) | Loss: 0.7948 | Speed: 102.4 batches/s | ETA: 02:00\n",
      "  Train: [45300/57525] ( 78.7%) | Loss: 0.7947 | Speed: 102.4 batches/s | ETA: 01:59\n",
      "  Train: [45400/57525] ( 78.9%) | Loss: 0.7945 | Speed: 102.4 batches/s | ETA: 01:58\n",
      "  Train: [45500/57525] ( 79.1%) | Loss: 0.7942 | Speed: 102.4 batches/s | ETA: 01:57\n",
      "  Train: [45600/57525] ( 79.3%) | Loss: 0.7940 | Speed: 102.4 batches/s | ETA: 01:56\n",
      "  Train: [45700/57525] ( 79.4%) | Loss: 0.7939 | Speed: 102.4 batches/s | ETA: 01:55\n",
      "  Train: [45800/57525] ( 79.6%) | Loss: 0.7937 | Speed: 102.3 batches/s | ETA: 01:54\n",
      "  Train: [45900/57525] ( 79.8%) | Loss: 0.7936 | Speed: 102.3 batches/s | ETA: 01:53\n",
      "  Train: [46000/57525] ( 80.0%) | Loss: 0.7934 | Speed: 102.3 batches/s | ETA: 01:52\n",
      "  Train: [46100/57525] ( 80.1%) | Loss: 0.7932 | Speed: 102.3 batches/s | ETA: 01:51\n",
      "  Train: [46200/57525] ( 80.3%) | Loss: 0.7931 | Speed: 102.3 batches/s | ETA: 01:50\n",
      "  Train: [46300/57525] ( 80.5%) | Loss: 0.7929 | Speed: 102.3 batches/s | ETA: 01:49\n",
      "  Train: [46400/57525] ( 80.7%) | Loss: 0.7928 | Speed: 102.3 batches/s | ETA: 01:48\n",
      "  Train: [46500/57525] ( 80.8%) | Loss: 0.7926 | Speed: 102.3 batches/s | ETA: 01:47\n",
      "  Train: [46600/57525] ( 81.0%) | Loss: 0.7925 | Speed: 102.3 batches/s | ETA: 01:46\n",
      "  Train: [46700/57525] ( 81.2%) | Loss: 0.7923 | Speed: 102.3 batches/s | ETA: 01:45\n",
      "  Train: [46800/57525] ( 81.4%) | Loss: 0.7921 | Speed: 102.3 batches/s | ETA: 01:44\n",
      "  Train: [46900/57525] ( 81.5%) | Loss: 0.7919 | Speed: 102.3 batches/s | ETA: 01:43\n",
      "  Train: [47000/57525] ( 81.7%) | Loss: 0.7917 | Speed: 102.2 batches/s | ETA: 01:42\n",
      "  Train: [47100/57525] ( 81.9%) | Loss: 0.7916 | Speed: 102.2 batches/s | ETA: 01:41\n",
      "  Train: [47200/57525] ( 82.1%) | Loss: 0.7914 | Speed: 102.2 batches/s | ETA: 01:40\n",
      "  Train: [47300/57525] ( 82.2%) | Loss: 0.7912 | Speed: 102.2 batches/s | ETA: 01:40\n",
      "  Train: [47400/57525] ( 82.4%) | Loss: 0.7911 | Speed: 102.2 batches/s | ETA: 01:39\n",
      "  Train: [47500/57525] ( 82.6%) | Loss: 0.7909 | Speed: 102.2 batches/s | ETA: 01:38\n",
      "  Train: [47600/57525] ( 82.7%) | Loss: 0.7908 | Speed: 102.2 batches/s | ETA: 01:37\n",
      "  Train: [47700/57525] ( 82.9%) | Loss: 0.7906 | Speed: 102.2 batches/s | ETA: 01:36\n",
      "  Train: [47800/57525] ( 83.1%) | Loss: 0.7905 | Speed: 102.2 batches/s | ETA: 01:35\n",
      "  Train: [47900/57525] ( 83.3%) | Loss: 0.7903 | Speed: 102.2 batches/s | ETA: 01:34\n",
      "  Train: [48000/57525] ( 83.4%) | Loss: 0.7901 | Speed: 102.2 batches/s | ETA: 01:33\n",
      "  Train: [48100/57525] ( 83.6%) | Loss: 0.7899 | Speed: 102.1 batches/s | ETA: 01:32\n",
      "  Train: [48200/57525] ( 83.8%) | Loss: 0.7898 | Speed: 102.1 batches/s | ETA: 01:31\n",
      "  Train: [48300/57525] ( 84.0%) | Loss: 0.7897 | Speed: 102.1 batches/s | ETA: 01:30\n",
      "  Train: [48400/57525] ( 84.1%) | Loss: 0.7895 | Speed: 102.1 batches/s | ETA: 01:29\n",
      "  Train: [48500/57525] ( 84.3%) | Loss: 0.7894 | Speed: 102.1 batches/s | ETA: 01:28\n",
      "  Train: [48600/57525] ( 84.5%) | Loss: 0.7892 | Speed: 102.1 batches/s | ETA: 01:27\n",
      "  Train: [48700/57525] ( 84.7%) | Loss: 0.7890 | Speed: 102.1 batches/s | ETA: 01:26\n",
      "  Train: [48800/57525] ( 84.8%) | Loss: 0.7889 | Speed: 102.1 batches/s | ETA: 01:25\n",
      "  Train: [48900/57525] ( 85.0%) | Loss: 0.7888 | Speed: 102.1 batches/s | ETA: 01:24\n",
      "  Train: [49000/57525] ( 85.2%) | Loss: 0.7886 | Speed: 102.1 batches/s | ETA: 01:23\n",
      "  Train: [49100/57525] ( 85.4%) | Loss: 0.7884 | Speed: 102.0 batches/s | ETA: 01:22\n",
      "  Train: [49200/57525] ( 85.5%) | Loss: 0.7883 | Speed: 102.0 batches/s | ETA: 01:21\n",
      "  Train: [49300/57525] ( 85.7%) | Loss: 0.7882 | Speed: 102.0 batches/s | ETA: 01:20\n",
      "  Train: [49400/57525] ( 85.9%) | Loss: 0.7880 | Speed: 102.0 batches/s | ETA: 01:19\n",
      "  Train: [49500/57525] ( 86.0%) | Loss: 0.7878 | Speed: 102.0 batches/s | ETA: 01:18\n",
      "  Train: [49600/57525] ( 86.2%) | Loss: 0.7877 | Speed: 102.0 batches/s | ETA: 01:17\n",
      "  Train: [49700/57525] ( 86.4%) | Loss: 0.7876 | Speed: 102.0 batches/s | ETA: 01:16\n",
      "  Train: [49800/57525] ( 86.6%) | Loss: 0.7874 | Speed: 102.0 batches/s | ETA: 01:15\n",
      "  Train: [49900/57525] ( 86.7%) | Loss: 0.7872 | Speed: 102.0 batches/s | ETA: 01:14\n",
      "  Train: [50000/57525] ( 86.9%) | Loss: 0.7871 | Speed: 102.0 batches/s | ETA: 01:13\n",
      "  Train: [50100/57525] ( 87.1%) | Loss: 0.7869 | Speed: 102.0 batches/s | ETA: 01:12\n",
      "  Train: [50200/57525] ( 87.3%) | Loss: 0.7868 | Speed: 102.0 batches/s | ETA: 01:11\n",
      "  Train: [50300/57525] ( 87.4%) | Loss: 0.7867 | Speed: 102.0 batches/s | ETA: 01:10\n",
      "  Train: [50400/57525] ( 87.6%) | Loss: 0.7865 | Speed: 102.0 batches/s | ETA: 01:09\n",
      "  Train: [50500/57525] ( 87.8%) | Loss: 0.7864 | Speed: 102.0 batches/s | ETA: 01:08\n",
      "  Train: [50600/57525] ( 88.0%) | Loss: 0.7862 | Speed: 102.0 batches/s | ETA: 01:07\n",
      "  Train: [50700/57525] ( 88.1%) | Loss: 0.7861 | Speed: 102.0 batches/s | ETA: 01:06\n",
      "  Train: [50800/57525] ( 88.3%) | Loss: 0.7860 | Speed: 102.0 batches/s | ETA: 01:05\n",
      "  Train: [50900/57525] ( 88.5%) | Loss: 0.7858 | Speed: 102.0 batches/s | ETA: 01:04\n",
      "  Train: [51000/57525] ( 88.7%) | Loss: 0.7856 | Speed: 102.0 batches/s | ETA: 01:03\n",
      "  Train: [51100/57525] ( 88.8%) | Loss: 0.7855 | Speed: 102.0 batches/s | ETA: 01:03\n",
      "  Train: [51200/57525] ( 89.0%) | Loss: 0.7854 | Speed: 102.0 batches/s | ETA: 01:02\n",
      "  Train: [51300/57525] ( 89.2%) | Loss: 0.7853 | Speed: 101.9 batches/s | ETA: 01:01\n",
      "  Train: [51400/57525] ( 89.4%) | Loss: 0.7852 | Speed: 101.9 batches/s | ETA: 01:00\n",
      "  Train: [51500/57525] ( 89.5%) | Loss: 0.7850 | Speed: 101.9 batches/s | ETA: 00:59\n",
      "  Train: [51600/57525] ( 89.7%) | Loss: 0.7849 | Speed: 101.9 batches/s | ETA: 00:58\n",
      "  Train: [51700/57525] ( 89.9%) | Loss: 0.7848 | Speed: 101.9 batches/s | ETA: 00:57\n",
      "  Train: [51800/57525] ( 90.0%) | Loss: 0.7846 | Speed: 101.9 batches/s | ETA: 00:56\n",
      "  Train: [51900/57525] ( 90.2%) | Loss: 0.7845 | Speed: 101.9 batches/s | ETA: 00:55\n",
      "  Train: [52000/57525] ( 90.4%) | Loss: 0.7843 | Speed: 101.8 batches/s | ETA: 00:54\n",
      "  Train: [52100/57525] ( 90.6%) | Loss: 0.7841 | Speed: 101.8 batches/s | ETA: 00:53\n",
      "  Train: [52200/57525] ( 90.7%) | Loss: 0.7840 | Speed: 101.8 batches/s | ETA: 00:52\n",
      "  Train: [52300/57525] ( 90.9%) | Loss: 0.7838 | Speed: 101.8 batches/s | ETA: 00:51\n",
      "  Train: [52400/57525] ( 91.1%) | Loss: 0.7837 | Speed: 101.8 batches/s | ETA: 00:50\n",
      "  Train: [52500/57525] ( 91.3%) | Loss: 0.7836 | Speed: 101.8 batches/s | ETA: 00:49\n",
      "  Train: [52600/57525] ( 91.4%) | Loss: 0.7834 | Speed: 101.8 batches/s | ETA: 00:48\n",
      "  Train: [52700/57525] ( 91.6%) | Loss: 0.7832 | Speed: 101.8 batches/s | ETA: 00:47\n",
      "  Train: [52800/57525] ( 91.8%) | Loss: 0.7831 | Speed: 101.8 batches/s | ETA: 00:46\n",
      "  Train: [52900/57525] ( 92.0%) | Loss: 0.7830 | Speed: 101.8 batches/s | ETA: 00:45\n",
      "  Train: [53000/57525] ( 92.1%) | Loss: 0.7828 | Speed: 101.8 batches/s | ETA: 00:44\n",
      "  Train: [53100/57525] ( 92.3%) | Loss: 0.7827 | Speed: 101.8 batches/s | ETA: 00:43\n",
      "  Train: [53200/57525] ( 92.5%) | Loss: 0.7826 | Speed: 101.8 batches/s | ETA: 00:42\n",
      "  Train: [53300/57525] ( 92.7%) | Loss: 0.7825 | Speed: 101.8 batches/s | ETA: 00:41\n",
      "  Train: [53400/57525] ( 92.8%) | Loss: 0.7823 | Speed: 101.8 batches/s | ETA: 00:40\n",
      "  Train: [53500/57525] ( 93.0%) | Loss: 0.7822 | Speed: 101.8 batches/s | ETA: 00:39\n",
      "  Train: [53600/57525] ( 93.2%) | Loss: 0.7820 | Speed: 101.8 batches/s | ETA: 00:38\n",
      "  Train: [53700/57525] ( 93.4%) | Loss: 0.7819 | Speed: 101.8 batches/s | ETA: 00:37\n",
      "  Train: [53800/57525] ( 93.5%) | Loss: 0.7817 | Speed: 101.7 batches/s | ETA: 00:36\n",
      "  Train: [53900/57525] ( 93.7%) | Loss: 0.7815 | Speed: 101.7 batches/s | ETA: 00:35\n",
      "  Train: [54000/57525] ( 93.9%) | Loss: 0.7814 | Speed: 101.7 batches/s | ETA: 00:34\n",
      "  Train: [54100/57525] ( 94.0%) | Loss: 0.7812 | Speed: 101.7 batches/s | ETA: 00:33\n",
      "  Train: [54200/57525] ( 94.2%) | Loss: 0.7811 | Speed: 101.7 batches/s | ETA: 00:32\n",
      "  Train: [54300/57525] ( 94.4%) | Loss: 0.7810 | Speed: 101.7 batches/s | ETA: 00:31\n",
      "  Train: [54400/57525] ( 94.6%) | Loss: 0.7808 | Speed: 101.7 batches/s | ETA: 00:30\n",
      "  Train: [54500/57525] ( 94.7%) | Loss: 0.7807 | Speed: 101.7 batches/s | ETA: 00:29\n",
      "  Train: [54600/57525] ( 94.9%) | Loss: 0.7805 | Speed: 101.7 batches/s | ETA: 00:28\n",
      "  Train: [54700/57525] ( 95.1%) | Loss: 0.7804 | Speed: 101.6 batches/s | ETA: 00:27\n",
      "  Train: [54800/57525] ( 95.3%) | Loss: 0.7802 | Speed: 101.6 batches/s | ETA: 00:26\n",
      "  Train: [54900/57525] ( 95.4%) | Loss: 0.7801 | Speed: 101.6 batches/s | ETA: 00:25\n",
      "  Train: [55000/57525] ( 95.6%) | Loss: 0.7800 | Speed: 101.6 batches/s | ETA: 00:24\n",
      "  Train: [55100/57525] ( 95.8%) | Loss: 0.7799 | Speed: 101.6 batches/s | ETA: 00:23\n",
      "  Train: [55200/57525] ( 96.0%) | Loss: 0.7798 | Speed: 101.6 batches/s | ETA: 00:22\n",
      "  Train: [55300/57525] ( 96.1%) | Loss: 0.7796 | Speed: 101.6 batches/s | ETA: 00:21\n",
      "  Train: [55400/57525] ( 96.3%) | Loss: 0.7795 | Speed: 101.6 batches/s | ETA: 00:20\n",
      "  Train: [55500/57525] ( 96.5%) | Loss: 0.7794 | Speed: 101.6 batches/s | ETA: 00:19\n",
      "  Train: [55600/57525] ( 96.7%) | Loss: 0.7792 | Speed: 101.6 batches/s | ETA: 00:18\n",
      "  Train: [55700/57525] ( 96.8%) | Loss: 0.7790 | Speed: 101.6 batches/s | ETA: 00:17\n",
      "  Train: [55800/57525] ( 97.0%) | Loss: 0.7789 | Speed: 101.6 batches/s | ETA: 00:16\n",
      "  Train: [55900/57525] ( 97.2%) | Loss: 0.7787 | Speed: 101.6 batches/s | ETA: 00:16\n",
      "  Train: [56000/57525] ( 97.3%) | Loss: 0.7786 | Speed: 101.6 batches/s | ETA: 00:15\n",
      "  Train: [56100/57525] ( 97.5%) | Loss: 0.7785 | Speed: 101.6 batches/s | ETA: 00:14\n",
      "  Train: [56200/57525] ( 97.7%) | Loss: 0.7784 | Speed: 101.6 batches/s | ETA: 00:13\n",
      "  Train: [56300/57525] ( 97.9%) | Loss: 0.7782 | Speed: 101.5 batches/s | ETA: 00:12\n",
      "  Train: [56400/57525] ( 98.0%) | Loss: 0.7781 | Speed: 101.5 batches/s | ETA: 00:11\n",
      "  Train: [56500/57525] ( 98.2%) | Loss: 0.7779 | Speed: 101.5 batches/s | ETA: 00:10\n",
      "  Train: [56600/57525] ( 98.4%) | Loss: 0.7778 | Speed: 101.5 batches/s | ETA: 00:09\n",
      "  Train: [56700/57525] ( 98.6%) | Loss: 0.7777 | Speed: 101.5 batches/s | ETA: 00:08\n",
      "  Train: [56800/57525] ( 98.7%) | Loss: 0.7775 | Speed: 101.5 batches/s | ETA: 00:07\n",
      "  Train: [56900/57525] ( 98.9%) | Loss: 0.7774 | Speed: 101.5 batches/s | ETA: 00:06\n",
      "  Train: [57000/57525] ( 99.1%) | Loss: 0.7772 | Speed: 101.5 batches/s | ETA: 00:05\n",
      "  Train: [57100/57525] ( 99.3%) | Loss: 0.7771 | Speed: 101.5 batches/s | ETA: 00:04\n",
      "  Train: [57200/57525] ( 99.4%) | Loss: 0.7769 | Speed: 101.5 batches/s | ETA: 00:03\n",
      "  Train: [57300/57525] ( 99.6%) | Loss: 0.7768 | Speed: 101.5 batches/s | ETA: 00:02\n",
      "  Train: [57400/57525] ( 99.8%) | Loss: 0.7766 | Speed: 101.5 batches/s | ETA: 00:01\n",
      "  Train: [57500/57525] (100.0%) | Loss: 0.7765 | Speed: 101.5 batches/s | ETA: 00:00\n",
      "  Running validation...\n",
      "    Val: [  50/7191] (  0.7%)\n",
      "    Val: [ 100/7191] (  1.4%)\n",
      "    Val: [ 150/7191] (  2.1%)\n",
      "    Val: [ 200/7191] (  2.8%)\n",
      "    Val: [ 250/7191] (  3.5%)\n",
      "    Val: [ 300/7191] (  4.2%)\n",
      "    Val: [ 350/7191] (  4.9%)\n",
      "    Val: [ 400/7191] (  5.6%)\n",
      "    Val: [ 450/7191] (  6.3%)\n",
      "    Val: [ 500/7191] (  7.0%)\n",
      "    Val: [ 550/7191] (  7.6%)\n",
      "    Val: [ 600/7191] (  8.3%)\n",
      "    Val: [ 650/7191] (  9.0%)\n",
      "    Val: [ 700/7191] (  9.7%)\n",
      "    Val: [ 750/7191] ( 10.4%)\n",
      "    Val: [ 800/7191] ( 11.1%)\n",
      "    Val: [ 850/7191] ( 11.8%)\n",
      "    Val: [ 900/7191] ( 12.5%)\n",
      "    Val: [ 950/7191] ( 13.2%)\n",
      "    Val: [1000/7191] ( 13.9%)\n",
      "    Val: [1050/7191] ( 14.6%)\n",
      "    Val: [1100/7191] ( 15.3%)\n",
      "    Val: [1150/7191] ( 16.0%)\n",
      "    Val: [1200/7191] ( 16.7%)\n",
      "    Val: [1250/7191] ( 17.4%)\n",
      "    Val: [1300/7191] ( 18.1%)\n",
      "    Val: [1350/7191] ( 18.8%)\n",
      "    Val: [1400/7191] ( 19.5%)\n",
      "    Val: [1450/7191] ( 20.2%)\n",
      "    Val: [1500/7191] ( 20.9%)\n",
      "    Val: [1550/7191] ( 21.6%)\n",
      "    Val: [1600/7191] ( 22.3%)\n",
      "    Val: [1650/7191] ( 22.9%)\n",
      "    Val: [1700/7191] ( 23.6%)\n",
      "    Val: [1750/7191] ( 24.3%)\n",
      "    Val: [1800/7191] ( 25.0%)\n",
      "    Val: [1850/7191] ( 25.7%)\n",
      "    Val: [1900/7191] ( 26.4%)\n",
      "    Val: [1950/7191] ( 27.1%)\n",
      "    Val: [2000/7191] ( 27.8%)\n",
      "    Val: [2050/7191] ( 28.5%)\n",
      "    Val: [2100/7191] ( 29.2%)\n",
      "    Val: [2150/7191] ( 29.9%)\n",
      "    Val: [2200/7191] ( 30.6%)\n",
      "    Val: [2250/7191] ( 31.3%)\n",
      "    Val: [2300/7191] ( 32.0%)\n",
      "    Val: [2350/7191] ( 32.7%)\n",
      "    Val: [2400/7191] ( 33.4%)\n",
      "    Val: [2450/7191] ( 34.1%)\n",
      "    Val: [2500/7191] ( 34.8%)\n",
      "    Val: [2550/7191] ( 35.5%)\n",
      "    Val: [2600/7191] ( 36.2%)\n",
      "    Val: [2650/7191] ( 36.9%)\n",
      "    Val: [2700/7191] ( 37.5%)\n",
      "    Val: [2750/7191] ( 38.2%)\n",
      "    Val: [2800/7191] ( 38.9%)\n",
      "    Val: [2850/7191] ( 39.6%)\n",
      "    Val: [2900/7191] ( 40.3%)\n",
      "    Val: [2950/7191] ( 41.0%)\n",
      "    Val: [3000/7191] ( 41.7%)\n",
      "    Val: [3050/7191] ( 42.4%)\n",
      "    Val: [3100/7191] ( 43.1%)\n",
      "    Val: [3150/7191] ( 43.8%)\n",
      "    Val: [3200/7191] ( 44.5%)\n",
      "    Val: [3250/7191] ( 45.2%)\n",
      "    Val: [3300/7191] ( 45.9%)\n",
      "    Val: [3350/7191] ( 46.6%)\n",
      "    Val: [3400/7191] ( 47.3%)\n",
      "    Val: [3450/7191] ( 48.0%)\n",
      "    Val: [3500/7191] ( 48.7%)\n",
      "    Val: [3550/7191] ( 49.4%)\n",
      "    Val: [3600/7191] ( 50.1%)\n",
      "    Val: [3650/7191] ( 50.8%)\n",
      "    Val: [3700/7191] ( 51.5%)\n",
      "    Val: [3750/7191] ( 52.1%)\n",
      "    Val: [3800/7191] ( 52.8%)\n",
      "    Val: [3850/7191] ( 53.5%)\n",
      "    Val: [3900/7191] ( 54.2%)\n",
      "    Val: [3950/7191] ( 54.9%)\n",
      "    Val: [4000/7191] ( 55.6%)\n",
      "    Val: [4050/7191] ( 56.3%)\n",
      "    Val: [4100/7191] ( 57.0%)\n",
      "    Val: [4150/7191] ( 57.7%)\n",
      "    Val: [4200/7191] ( 58.4%)\n",
      "    Val: [4250/7191] ( 59.1%)\n",
      "    Val: [4300/7191] ( 59.8%)\n",
      "    Val: [4350/7191] ( 60.5%)\n",
      "    Val: [4400/7191] ( 61.2%)\n",
      "    Val: [4450/7191] ( 61.9%)\n",
      "    Val: [4500/7191] ( 62.6%)\n",
      "    Val: [4550/7191] ( 63.3%)\n",
      "    Val: [4600/7191] ( 64.0%)\n",
      "    Val: [4650/7191] ( 64.7%)\n",
      "    Val: [4700/7191] ( 65.4%)\n",
      "    Val: [4750/7191] ( 66.1%)\n",
      "    Val: [4800/7191] ( 66.8%)\n",
      "    Val: [4850/7191] ( 67.4%)\n",
      "    Val: [4900/7191] ( 68.1%)\n",
      "    Val: [4950/7191] ( 68.8%)\n",
      "    Val: [5000/7191] ( 69.5%)\n",
      "    Val: [5050/7191] ( 70.2%)\n",
      "    Val: [5100/7191] ( 70.9%)\n",
      "    Val: [5150/7191] ( 71.6%)\n",
      "    Val: [5200/7191] ( 72.3%)\n",
      "    Val: [5250/7191] ( 73.0%)\n",
      "    Val: [5300/7191] ( 73.7%)\n",
      "    Val: [5350/7191] ( 74.4%)\n",
      "    Val: [5400/7191] ( 75.1%)\n",
      "    Val: [5450/7191] ( 75.8%)\n",
      "    Val: [5500/7191] ( 76.5%)\n",
      "    Val: [5550/7191] ( 77.2%)\n",
      "    Val: [5600/7191] ( 77.9%)\n",
      "    Val: [5650/7191] ( 78.6%)\n",
      "    Val: [5700/7191] ( 79.3%)\n",
      "    Val: [5750/7191] ( 80.0%)\n",
      "    Val: [5800/7191] ( 80.7%)\n",
      "    Val: [5850/7191] ( 81.4%)\n",
      "    Val: [5900/7191] ( 82.0%)\n",
      "    Val: [5950/7191] ( 82.7%)\n",
      "    Val: [6000/7191] ( 83.4%)\n",
      "    Val: [6050/7191] ( 84.1%)\n",
      "    Val: [6100/7191] ( 84.8%)\n",
      "    Val: [6150/7191] ( 85.5%)\n",
      "    Val: [6200/7191] ( 86.2%)\n",
      "    Val: [6250/7191] ( 86.9%)\n",
      "    Val: [6300/7191] ( 87.6%)\n",
      "    Val: [6350/7191] ( 88.3%)\n",
      "    Val: [6400/7191] ( 89.0%)\n",
      "    Val: [6450/7191] ( 89.7%)\n",
      "    Val: [6500/7191] ( 90.4%)\n",
      "    Val: [6550/7191] ( 91.1%)\n",
      "    Val: [6600/7191] ( 91.8%)\n",
      "    Val: [6650/7191] ( 92.5%)\n",
      "    Val: [6700/7191] ( 93.2%)\n",
      "    Val: [6750/7191] ( 93.9%)\n",
      "    Val: [6800/7191] ( 94.6%)\n",
      "    Val: [6850/7191] ( 95.3%)\n",
      "    Val: [6900/7191] ( 96.0%)\n",
      "    Val: [6950/7191] ( 96.6%)\n",
      "    Val: [7000/7191] ( 97.3%)\n",
      "    Val: [7050/7191] ( 98.0%)\n",
      "    Val: [7100/7191] ( 98.7%)\n",
      "    Val: [7150/7191] ( 99.4%)\n",
      "\n",
      "  Epoch 01 Summary:\n",
      "    Train MSE (norm): 0.7765 | Val MSE (norm): 0.6735\n",
      "    Time: Train=9.4min, Val=0.3min, Total=9.8min\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Epoch 02/3\n",
      "============================================================\n",
      "  Train: [  100/57525] (  0.2%) | Loss: 0.7028 | Speed: 51.0 batches/s | ETA: 18:45\n",
      "  Train: [  200/57525] (  0.3%) | Loss: 0.7068 | Speed: 67.4 batches/s | ETA: 14:10\n",
      "  Train: [  300/57525] (  0.5%) | Loss: 0.7134 | Speed: 75.7 batches/s | ETA: 12:35\n",
      "  Train: [  400/57525] (  0.7%) | Loss: 0.7095 | Speed: 80.5 batches/s | ETA: 11:49\n",
      "  Train: [  500/57525] (  0.9%) | Loss: 0.7068 | Speed: 83.7 batches/s | ETA: 11:21\n",
      "  Train: [  600/57525] (  1.0%) | Loss: 0.7053 | Speed: 85.9 batches/s | ETA: 11:02\n",
      "  Train: [  700/57525] (  1.2%) | Loss: 0.7072 | Speed: 87.2 batches/s | ETA: 10:51\n",
      "  Train: [  800/57525] (  1.4%) | Loss: 0.7096 | Speed: 88.1 batches/s | ETA: 10:43\n",
      "  Train: [  900/57525] (  1.6%) | Loss: 0.7075 | Speed: 88.9 batches/s | ETA: 10:36\n",
      "  Train: [ 1000/57525] (  1.7%) | Loss: 0.7071 | Speed: 89.6 batches/s | ETA: 10:30\n",
      "  Train: [ 1100/57525] (  1.9%) | Loss: 0.7075 | Speed: 90.4 batches/s | ETA: 10:24\n",
      "  Train: [ 1200/57525] (  2.1%) | Loss: 0.7079 | Speed: 91.0 batches/s | ETA: 10:19\n",
      "  Train: [ 1300/57525] (  2.3%) | Loss: 0.7060 | Speed: 91.5 batches/s | ETA: 10:14\n",
      "  Train: [ 1400/57525] (  2.4%) | Loss: 0.7045 | Speed: 92.3 batches/s | ETA: 10:08\n",
      "  Train: [ 1500/57525] (  2.6%) | Loss: 0.7045 | Speed: 92.9 batches/s | ETA: 10:03\n",
      "  Train: [ 1600/57525] (  2.8%) | Loss: 0.7041 | Speed: 93.5 batches/s | ETA: 09:58\n",
      "  Train: [ 1700/57525] (  3.0%) | Loss: 0.7040 | Speed: 93.9 batches/s | ETA: 09:54\n",
      "  Train: [ 1800/57525] (  3.1%) | Loss: 0.7036 | Speed: 94.3 batches/s | ETA: 09:50\n",
      "  Train: [ 1900/57525] (  3.3%) | Loss: 0.7012 | Speed: 94.6 batches/s | ETA: 09:47\n",
      "  Train: [ 2000/57525] (  3.5%) | Loss: 0.7010 | Speed: 94.6 batches/s | ETA: 09:46\n",
      "  Train: [ 2100/57525] (  3.7%) | Loss: 0.7012 | Speed: 94.6 batches/s | ETA: 09:45\n",
      "  Train: [ 2200/57525] (  3.8%) | Loss: 0.7012 | Speed: 94.6 batches/s | ETA: 09:44\n",
      "  Train: [ 2300/57525] (  4.0%) | Loss: 0.7009 | Speed: 94.6 batches/s | ETA: 09:43\n",
      "  Train: [ 2400/57525] (  4.2%) | Loss: 0.7008 | Speed: 94.7 batches/s | ETA: 09:41\n",
      "  Train: [ 2500/57525] (  4.3%) | Loss: 0.7000 | Speed: 94.8 batches/s | ETA: 09:40\n",
      "  Train: [ 2600/57525] (  4.5%) | Loss: 0.6993 | Speed: 94.9 batches/s | ETA: 09:38\n",
      "  Train: [ 2700/57525] (  4.7%) | Loss: 0.6991 | Speed: 95.0 batches/s | ETA: 09:36\n",
      "  Train: [ 2800/57525] (  4.9%) | Loss: 0.6987 | Speed: 95.2 batches/s | ETA: 09:34\n",
      "  Train: [ 2900/57525] (  5.0%) | Loss: 0.6998 | Speed: 95.5 batches/s | ETA: 09:32\n",
      "  Train: [ 3000/57525] (  5.2%) | Loss: 0.7001 | Speed: 95.6 batches/s | ETA: 09:30\n",
      "  Train: [ 3100/57525] (  5.4%) | Loss: 0.6999 | Speed: 95.7 batches/s | ETA: 09:28\n",
      "  Train: [ 3200/57525] (  5.6%) | Loss: 0.7004 | Speed: 95.6 batches/s | ETA: 09:28\n",
      "  Train: [ 3300/57525] (  5.7%) | Loss: 0.7000 | Speed: 95.6 batches/s | ETA: 09:27\n",
      "  Train: [ 3400/57525] (  5.9%) | Loss: 0.6996 | Speed: 95.8 batches/s | ETA: 09:25\n",
      "  Train: [ 3500/57525] (  6.1%) | Loss: 0.6992 | Speed: 95.8 batches/s | ETA: 09:24\n",
      "  Train: [ 3600/57525] (  6.3%) | Loss: 0.6990 | Speed: 95.8 batches/s | ETA: 09:23\n",
      "  Train: [ 3700/57525] (  6.4%) | Loss: 0.6991 | Speed: 95.7 batches/s | ETA: 09:22\n",
      "  Train: [ 3800/57525] (  6.6%) | Loss: 0.6992 | Speed: 95.7 batches/s | ETA: 09:21\n",
      "  Train: [ 3900/57525] (  6.8%) | Loss: 0.6994 | Speed: 95.9 batches/s | ETA: 09:19\n",
      "  Train: [ 4000/57525] (  7.0%) | Loss: 0.6999 | Speed: 96.0 batches/s | ETA: 09:17\n",
      "  Train: [ 4100/57525] (  7.1%) | Loss: 0.6999 | Speed: 96.1 batches/s | ETA: 09:15\n",
      "  Train: [ 4200/57525] (  7.3%) | Loss: 0.6996 | Speed: 96.2 batches/s | ETA: 09:14\n",
      "  Train: [ 4300/57525] (  7.5%) | Loss: 0.6996 | Speed: 96.4 batches/s | ETA: 09:12\n",
      "  Train: [ 4400/57525] (  7.6%) | Loss: 0.6998 | Speed: 96.4 batches/s | ETA: 09:11\n",
      "  Train: [ 4500/57525] (  7.8%) | Loss: 0.7003 | Speed: 96.4 batches/s | ETA: 09:10\n",
      "  Train: [ 4600/57525] (  8.0%) | Loss: 0.7002 | Speed: 96.4 batches/s | ETA: 09:09\n",
      "  Train: [ 4700/57525] (  8.2%) | Loss: 0.7005 | Speed: 96.4 batches/s | ETA: 09:08\n",
      "  Train: [ 4800/57525] (  8.3%) | Loss: 0.7001 | Speed: 96.4 batches/s | ETA: 09:07\n",
      "  Train: [ 4900/57525] (  8.5%) | Loss: 0.7002 | Speed: 96.4 batches/s | ETA: 09:06\n",
      "  Train: [ 5000/57525] (  8.7%) | Loss: 0.7003 | Speed: 96.4 batches/s | ETA: 09:04\n",
      "  Train: [ 5100/57525] (  8.9%) | Loss: 0.7005 | Speed: 96.5 batches/s | ETA: 09:03\n",
      "  Train: [ 5200/57525] (  9.0%) | Loss: 0.7003 | Speed: 96.6 batches/s | ETA: 09:01\n",
      "  Train: [ 5300/57525] (  9.2%) | Loss: 0.7002 | Speed: 96.7 batches/s | ETA: 08:59\n",
      "  Train: [ 5400/57525] (  9.4%) | Loss: 0.7000 | Speed: 96.8 batches/s | ETA: 08:58\n",
      "  Train: [ 5500/57525] (  9.6%) | Loss: 0.7003 | Speed: 96.9 batches/s | ETA: 08:56\n",
      "  Train: [ 5600/57525] (  9.7%) | Loss: 0.7001 | Speed: 97.0 batches/s | ETA: 08:55\n",
      "  Train: [ 5700/57525] (  9.9%) | Loss: 0.6998 | Speed: 96.9 batches/s | ETA: 08:54\n",
      "  Train: [ 5800/57525] ( 10.1%) | Loss: 0.6994 | Speed: 96.9 batches/s | ETA: 08:53\n",
      "  Train: [ 5900/57525] ( 10.3%) | Loss: 0.6995 | Speed: 96.9 batches/s | ETA: 08:52\n",
      "  Train: [ 6000/57525] ( 10.4%) | Loss: 0.6994 | Speed: 96.8 batches/s | ETA: 08:52\n",
      "  Train: [ 6100/57525] ( 10.6%) | Loss: 0.6991 | Speed: 96.8 batches/s | ETA: 08:51\n",
      "  Train: [ 6200/57525] ( 10.8%) | Loss: 0.6990 | Speed: 96.8 batches/s | ETA: 08:50\n",
      "  Train: [ 6300/57525] ( 11.0%) | Loss: 0.6993 | Speed: 96.8 batches/s | ETA: 08:49\n",
      "  Train: [ 6400/57525] ( 11.1%) | Loss: 0.6992 | Speed: 96.8 batches/s | ETA: 08:48\n",
      "  Train: [ 6500/57525] ( 11.3%) | Loss: 0.6994 | Speed: 96.8 batches/s | ETA: 08:47\n",
      "  Train: [ 6600/57525] ( 11.5%) | Loss: 0.6991 | Speed: 96.8 batches/s | ETA: 08:45\n",
      "  Train: [ 6700/57525] ( 11.6%) | Loss: 0.6986 | Speed: 96.8 batches/s | ETA: 08:44\n",
      "  Train: [ 6800/57525] ( 11.8%) | Loss: 0.6988 | Speed: 96.8 batches/s | ETA: 08:43\n",
      "  Train: [ 6900/57525] ( 12.0%) | Loss: 0.6989 | Speed: 96.8 batches/s | ETA: 08:42\n",
      "  Train: [ 7000/57525] ( 12.2%) | Loss: 0.6987 | Speed: 96.8 batches/s | ETA: 08:42\n",
      "  Train: [ 7100/57525] ( 12.3%) | Loss: 0.6985 | Speed: 96.7 batches/s | ETA: 08:41\n",
      "  Train: [ 7200/57525] ( 12.5%) | Loss: 0.6983 | Speed: 96.7 batches/s | ETA: 08:40\n",
      "  Train: [ 7300/57525] ( 12.7%) | Loss: 0.6983 | Speed: 96.7 batches/s | ETA: 08:39\n",
      "  Train: [ 7400/57525] ( 12.9%) | Loss: 0.6984 | Speed: 96.6 batches/s | ETA: 08:38\n",
      "  Train: [ 7500/57525] ( 13.0%) | Loss: 0.6983 | Speed: 96.7 batches/s | ETA: 08:37\n",
      "  Train: [ 7600/57525] ( 13.2%) | Loss: 0.6985 | Speed: 96.7 batches/s | ETA: 08:36\n",
      "  Train: [ 7700/57525] ( 13.4%) | Loss: 0.6985 | Speed: 96.8 batches/s | ETA: 08:34\n",
      "  Train: [ 7800/57525] ( 13.6%) | Loss: 0.6983 | Speed: 96.9 batches/s | ETA: 08:33\n",
      "  Train: [ 7900/57525] ( 13.7%) | Loss: 0.6983 | Speed: 96.9 batches/s | ETA: 08:32\n",
      "  Train: [ 8000/57525] ( 13.9%) | Loss: 0.6984 | Speed: 97.0 batches/s | ETA: 08:30\n",
      "  Train: [ 8100/57525] ( 14.1%) | Loss: 0.6984 | Speed: 97.0 batches/s | ETA: 08:29\n",
      "  Train: [ 8200/57525] ( 14.3%) | Loss: 0.6984 | Speed: 97.0 batches/s | ETA: 08:28\n",
      "  Train: [ 8300/57525] ( 14.4%) | Loss: 0.6982 | Speed: 97.0 batches/s | ETA: 08:27\n",
      "  Train: [ 8400/57525] ( 14.6%) | Loss: 0.6982 | Speed: 97.0 batches/s | ETA: 08:26\n",
      "  Train: [ 8500/57525] ( 14.8%) | Loss: 0.6981 | Speed: 97.1 batches/s | ETA: 08:25\n",
      "  Train: [ 8600/57525] ( 15.0%) | Loss: 0.6981 | Speed: 97.1 batches/s | ETA: 08:23\n",
      "  Train: [ 8700/57525] ( 15.1%) | Loss: 0.6982 | Speed: 97.1 batches/s | ETA: 08:22\n",
      "  Train: [ 8800/57525] ( 15.3%) | Loss: 0.6982 | Speed: 97.1 batches/s | ETA: 08:21\n",
      "  Train: [ 8900/57525] ( 15.5%) | Loss: 0.6981 | Speed: 97.1 batches/s | ETA: 08:20\n",
      "  Train: [ 9000/57525] ( 15.6%) | Loss: 0.6978 | Speed: 97.1 batches/s | ETA: 08:19\n",
      "  Train: [ 9100/57525] ( 15.8%) | Loss: 0.6980 | Speed: 97.1 batches/s | ETA: 08:18\n",
      "  Train: [ 9200/57525] ( 16.0%) | Loss: 0.6980 | Speed: 97.1 batches/s | ETA: 08:17\n",
      "  Train: [ 9300/57525] ( 16.2%) | Loss: 0.6978 | Speed: 97.1 batches/s | ETA: 08:16\n",
      "  Train: [ 9400/57525] ( 16.3%) | Loss: 0.6978 | Speed: 97.1 batches/s | ETA: 08:15\n",
      "  Train: [ 9500/57525] ( 16.5%) | Loss: 0.6979 | Speed: 97.0 batches/s | ETA: 08:14\n",
      "  Train: [ 9600/57525] ( 16.7%) | Loss: 0.6977 | Speed: 97.0 batches/s | ETA: 08:14\n",
      "  Train: [ 9700/57525] ( 16.9%) | Loss: 0.6977 | Speed: 96.9 batches/s | ETA: 08:13\n",
      "  Train: [ 9800/57525] ( 17.0%) | Loss: 0.6976 | Speed: 96.9 batches/s | ETA: 08:12\n",
      "  Train: [ 9900/57525] ( 17.2%) | Loss: 0.6976 | Speed: 96.9 batches/s | ETA: 08:11\n",
      "  Train: [10000/57525] ( 17.4%) | Loss: 0.6974 | Speed: 97.0 batches/s | ETA: 08:10\n",
      "  Train: [10100/57525] ( 17.6%) | Loss: 0.6974 | Speed: 97.0 batches/s | ETA: 08:08\n",
      "  Train: [10200/57525] ( 17.7%) | Loss: 0.6975 | Speed: 97.0 batches/s | ETA: 08:07\n",
      "  Train: [10300/57525] ( 17.9%) | Loss: 0.6974 | Speed: 97.0 batches/s | ETA: 08:06\n",
      "  Train: [10400/57525] ( 18.1%) | Loss: 0.6974 | Speed: 97.1 batches/s | ETA: 08:05\n",
      "  Train: [10500/57525] ( 18.3%) | Loss: 0.6971 | Speed: 97.1 batches/s | ETA: 08:04\n",
      "  Train: [10600/57525] ( 18.4%) | Loss: 0.6969 | Speed: 97.1 batches/s | ETA: 08:03\n",
      "  Train: [10700/57525] ( 18.6%) | Loss: 0.6968 | Speed: 97.1 batches/s | ETA: 08:02\n",
      "  Train: [10800/57525] ( 18.8%) | Loss: 0.6969 | Speed: 97.2 batches/s | ETA: 08:00\n",
      "  Train: [10900/57525] ( 18.9%) | Loss: 0.6968 | Speed: 97.2 batches/s | ETA: 07:59\n",
      "  Train: [11000/57525] ( 19.1%) | Loss: 0.6967 | Speed: 97.2 batches/s | ETA: 07:58\n",
      "  Train: [11100/57525] ( 19.3%) | Loss: 0.6968 | Speed: 97.2 batches/s | ETA: 07:57\n",
      "  Train: [11200/57525] ( 19.5%) | Loss: 0.6969 | Speed: 97.3 batches/s | ETA: 07:56\n",
      "  Train: [11300/57525] ( 19.6%) | Loss: 0.6969 | Speed: 97.3 batches/s | ETA: 07:55\n",
      "  Train: [11400/57525] ( 19.8%) | Loss: 0.6966 | Speed: 97.4 batches/s | ETA: 07:53\n",
      "  Train: [11500/57525] ( 20.0%) | Loss: 0.6967 | Speed: 97.4 batches/s | ETA: 07:52\n",
      "  Train: [11600/57525] ( 20.2%) | Loss: 0.6966 | Speed: 97.4 batches/s | ETA: 07:51\n",
      "  Train: [11700/57525] ( 20.3%) | Loss: 0.6964 | Speed: 97.5 batches/s | ETA: 07:50\n",
      "  Train: [11800/57525] ( 20.5%) | Loss: 0.6962 | Speed: 97.4 batches/s | ETA: 07:49\n",
      "  Train: [11900/57525] ( 20.7%) | Loss: 0.6962 | Speed: 97.4 batches/s | ETA: 07:48\n",
      "  Train: [12000/57525] ( 20.9%) | Loss: 0.6961 | Speed: 97.4 batches/s | ETA: 07:47\n",
      "  Train: [12100/57525] ( 21.0%) | Loss: 0.6960 | Speed: 97.4 batches/s | ETA: 07:46\n",
      "  Train: [12200/57525] ( 21.2%) | Loss: 0.6960 | Speed: 97.3 batches/s | ETA: 07:45\n",
      "  Train: [12300/57525] ( 21.4%) | Loss: 0.6958 | Speed: 97.3 batches/s | ETA: 07:44\n",
      "  Train: [12400/57525] ( 21.6%) | Loss: 0.6959 | Speed: 97.3 batches/s | ETA: 07:43\n",
      "  Train: [12500/57525] ( 21.7%) | Loss: 0.6958 | Speed: 97.3 batches/s | ETA: 07:42\n",
      "  Train: [12600/57525] ( 21.9%) | Loss: 0.6956 | Speed: 97.3 batches/s | ETA: 07:41\n",
      "  Train: [12700/57525] ( 22.1%) | Loss: 0.6956 | Speed: 97.3 batches/s | ETA: 07:40\n",
      "  Train: [12800/57525] ( 22.3%) | Loss: 0.6956 | Speed: 97.3 batches/s | ETA: 07:39\n",
      "  Train: [12900/57525] ( 22.4%) | Loss: 0.6956 | Speed: 97.3 batches/s | ETA: 07:38\n",
      "  Train: [13000/57525] ( 22.6%) | Loss: 0.6955 | Speed: 97.3 batches/s | ETA: 07:37\n",
      "  Train: [13100/57525] ( 22.8%) | Loss: 0.6958 | Speed: 97.3 batches/s | ETA: 07:36\n",
      "  Train: [13200/57525] ( 22.9%) | Loss: 0.6958 | Speed: 97.3 batches/s | ETA: 07:35\n",
      "  Train: [13300/57525] ( 23.1%) | Loss: 0.6958 | Speed: 97.3 batches/s | ETA: 07:34\n",
      "  Train: [13400/57525] ( 23.3%) | Loss: 0.6956 | Speed: 97.3 batches/s | ETA: 07:33\n",
      "  Train: [13500/57525] ( 23.5%) | Loss: 0.6955 | Speed: 97.3 batches/s | ETA: 07:32\n",
      "  Train: [13600/57525] ( 23.6%) | Loss: 0.6954 | Speed: 97.3 batches/s | ETA: 07:31\n",
      "  Train: [13700/57525] ( 23.8%) | Loss: 0.6953 | Speed: 97.3 batches/s | ETA: 07:30\n",
      "  Train: [13800/57525] ( 24.0%) | Loss: 0.6952 | Speed: 97.4 batches/s | ETA: 07:29\n",
      "  Train: [13900/57525] ( 24.2%) | Loss: 0.6951 | Speed: 97.4 batches/s | ETA: 07:27\n",
      "  Train: [14000/57525] ( 24.3%) | Loss: 0.6950 | Speed: 97.4 batches/s | ETA: 07:26\n",
      "  Train: [14100/57525] ( 24.5%) | Loss: 0.6950 | Speed: 97.5 batches/s | ETA: 07:25\n",
      "  Train: [14200/57525] ( 24.7%) | Loss: 0.6949 | Speed: 97.5 batches/s | ETA: 07:24\n",
      "  Train: [14300/57525] ( 24.9%) | Loss: 0.6949 | Speed: 97.5 batches/s | ETA: 07:23\n",
      "  Train: [14400/57525] ( 25.0%) | Loss: 0.6949 | Speed: 97.4 batches/s | ETA: 07:22\n",
      "  Train: [14500/57525] ( 25.2%) | Loss: 0.6948 | Speed: 97.4 batches/s | ETA: 07:21\n",
      "  Train: [14600/57525] ( 25.4%) | Loss: 0.6948 | Speed: 97.4 batches/s | ETA: 07:20\n",
      "  Train: [14700/57525] ( 25.6%) | Loss: 0.6948 | Speed: 97.4 batches/s | ETA: 07:19\n",
      "  Train: [14800/57525] ( 25.7%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:18\n",
      "  Train: [14900/57525] ( 25.9%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:17\n",
      "  Train: [15000/57525] ( 26.1%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:16\n",
      "  Train: [15100/57525] ( 26.2%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:15\n",
      "  Train: [15200/57525] ( 26.4%) | Loss: 0.6945 | Speed: 97.4 batches/s | ETA: 07:14\n",
      "  Train: [15300/57525] ( 26.6%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:13\n",
      "  Train: [15400/57525] ( 26.8%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:12\n",
      "  Train: [15500/57525] ( 26.9%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:11\n",
      "  Train: [15600/57525] ( 27.1%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:10\n",
      "  Train: [15700/57525] ( 27.3%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:09\n",
      "  Train: [15800/57525] ( 27.5%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:08\n",
      "  Train: [15900/57525] ( 27.6%) | Loss: 0.6947 | Speed: 97.4 batches/s | ETA: 07:07\n",
      "  Train: [16000/57525] ( 27.8%) | Loss: 0.6947 | Speed: 97.3 batches/s | ETA: 07:06\n",
      "  Train: [16100/57525] ( 28.0%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:05\n",
      "  Train: [16200/57525] ( 28.2%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:04\n",
      "  Train: [16300/57525] ( 28.3%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:03\n",
      "  Train: [16400/57525] ( 28.5%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 07:02\n",
      "  Train: [16500/57525] ( 28.7%) | Loss: 0.6947 | Speed: 97.4 batches/s | ETA: 07:01\n",
      "  Train: [16600/57525] ( 28.9%) | Loss: 0.6949 | Speed: 97.4 batches/s | ETA: 07:00\n",
      "  Train: [16700/57525] ( 29.0%) | Loss: 0.6949 | Speed: 97.4 batches/s | ETA: 06:59\n",
      "  Train: [16800/57525] ( 29.2%) | Loss: 0.6948 | Speed: 97.4 batches/s | ETA: 06:58\n",
      "  Train: [16900/57525] ( 29.4%) | Loss: 0.6946 | Speed: 97.4 batches/s | ETA: 06:56\n",
      "  Train: [17000/57525] ( 29.6%) | Loss: 0.6945 | Speed: 97.4 batches/s | ETA: 06:55\n",
      "  Train: [17100/57525] ( 29.7%) | Loss: 0.6944 | Speed: 97.4 batches/s | ETA: 06:54\n",
      "  Train: [17200/57525] ( 29.9%) | Loss: 0.6943 | Speed: 97.4 batches/s | ETA: 06:53\n",
      "  Train: [17300/57525] ( 30.1%) | Loss: 0.6943 | Speed: 97.5 batches/s | ETA: 06:52\n",
      "  Train: [17400/57525] ( 30.2%) | Loss: 0.6942 | Speed: 97.5 batches/s | ETA: 06:51\n",
      "  Train: [17500/57525] ( 30.4%) | Loss: 0.6942 | Speed: 97.5 batches/s | ETA: 06:50\n",
      "  Train: [17600/57525] ( 30.6%) | Loss: 0.6942 | Speed: 97.5 batches/s | ETA: 06:49\n",
      "  Train: [17700/57525] ( 30.8%) | Loss: 0.6942 | Speed: 97.6 batches/s | ETA: 06:48\n",
      "  Train: [17800/57525] ( 30.9%) | Loss: 0.6942 | Speed: 97.6 batches/s | ETA: 06:47\n",
      "  Train: [17900/57525] ( 31.1%) | Loss: 0.6943 | Speed: 97.6 batches/s | ETA: 06:45\n",
      "  Train: [18000/57525] ( 31.3%) | Loss: 0.6942 | Speed: 97.6 batches/s | ETA: 06:44\n",
      "  Train: [18100/57525] ( 31.5%) | Loss: 0.6942 | Speed: 97.6 batches/s | ETA: 06:43\n",
      "  Train: [18200/57525] ( 31.6%) | Loss: 0.6944 | Speed: 97.6 batches/s | ETA: 06:43\n",
      "  Train: [18300/57525] ( 31.8%) | Loss: 0.6943 | Speed: 97.5 batches/s | ETA: 06:42\n",
      "  Train: [18400/57525] ( 32.0%) | Loss: 0.6943 | Speed: 97.5 batches/s | ETA: 06:41\n",
      "  Train: [18500/57525] ( 32.2%) | Loss: 0.6941 | Speed: 97.5 batches/s | ETA: 06:40\n",
      "  Train: [18600/57525] ( 32.3%) | Loss: 0.6940 | Speed: 97.5 batches/s | ETA: 06:39\n",
      "  Train: [18700/57525] ( 32.5%) | Loss: 0.6939 | Speed: 97.5 batches/s | ETA: 06:38\n",
      "  Train: [18800/57525] ( 32.7%) | Loss: 0.6939 | Speed: 97.5 batches/s | ETA: 06:37\n",
      "  Train: [18900/57525] ( 32.9%) | Loss: 0.6938 | Speed: 97.5 batches/s | ETA: 06:36\n",
      "  Train: [19000/57525] ( 33.0%) | Loss: 0.6938 | Speed: 97.5 batches/s | ETA: 06:35\n",
      "  Train: [19100/57525] ( 33.2%) | Loss: 0.6936 | Speed: 97.5 batches/s | ETA: 06:34\n",
      "  Train: [19200/57525] ( 33.4%) | Loss: 0.6935 | Speed: 97.5 batches/s | ETA: 06:33\n",
      "  Train: [19300/57525] ( 33.6%) | Loss: 0.6936 | Speed: 97.5 batches/s | ETA: 06:32\n",
      "  Train: [19400/57525] ( 33.7%) | Loss: 0.6937 | Speed: 97.4 batches/s | ETA: 06:31\n",
      "  Train: [19500/57525] ( 33.9%) | Loss: 0.6937 | Speed: 97.4 batches/s | ETA: 06:30\n",
      "  Train: [19600/57525] ( 34.1%) | Loss: 0.6938 | Speed: 97.4 batches/s | ETA: 06:29\n",
      "  Train: [19700/57525] ( 34.2%) | Loss: 0.6938 | Speed: 97.4 batches/s | ETA: 06:28\n",
      "  Train: [19800/57525] ( 34.4%) | Loss: 0.6939 | Speed: 97.4 batches/s | ETA: 06:27\n",
      "  Train: [19900/57525] ( 34.6%) | Loss: 0.6938 | Speed: 97.4 batches/s | ETA: 06:26\n",
      "  Train: [20000/57525] ( 34.8%) | Loss: 0.6937 | Speed: 97.5 batches/s | ETA: 06:25\n",
      "  Train: [20100/57525] ( 34.9%) | Loss: 0.6936 | Speed: 97.5 batches/s | ETA: 06:23\n",
      "  Train: [20200/57525] ( 35.1%) | Loss: 0.6937 | Speed: 97.5 batches/s | ETA: 06:22\n",
      "  Train: [20300/57525] ( 35.3%) | Loss: 0.6935 | Speed: 97.5 batches/s | ETA: 06:21\n",
      "  Train: [20400/57525] ( 35.5%) | Loss: 0.6935 | Speed: 97.5 batches/s | ETA: 06:20\n",
      "  Train: [20500/57525] ( 35.6%) | Loss: 0.6934 | Speed: 97.5 batches/s | ETA: 06:19\n",
      "  Train: [20600/57525] ( 35.8%) | Loss: 0.6933 | Speed: 97.5 batches/s | ETA: 06:18\n",
      "  Train: [20700/57525] ( 36.0%) | Loss: 0.6933 | Speed: 97.5 batches/s | ETA: 06:17\n",
      "  Train: [20800/57525] ( 36.2%) | Loss: 0.6933 | Speed: 97.5 batches/s | ETA: 06:16\n",
      "  Train: [20900/57525] ( 36.3%) | Loss: 0.6933 | Speed: 97.5 batches/s | ETA: 06:15\n",
      "  Train: [21000/57525] ( 36.5%) | Loss: 0.6933 | Speed: 97.6 batches/s | ETA: 06:14\n",
      "  Train: [21100/57525] ( 36.7%) | Loss: 0.6933 | Speed: 97.6 batches/s | ETA: 06:13\n",
      "  Train: [21200/57525] ( 36.9%) | Loss: 0.6931 | Speed: 97.6 batches/s | ETA: 06:12\n",
      "  Train: [21300/57525] ( 37.0%) | Loss: 0.6930 | Speed: 97.6 batches/s | ETA: 06:11\n",
      "  Train: [21400/57525] ( 37.2%) | Loss: 0.6930 | Speed: 97.6 batches/s | ETA: 06:10\n",
      "  Train: [21500/57525] ( 37.4%) | Loss: 0.6929 | Speed: 97.6 batches/s | ETA: 06:09\n",
      "  Train: [21600/57525] ( 37.5%) | Loss: 0.6930 | Speed: 97.6 batches/s | ETA: 06:08\n",
      "  Train: [21700/57525] ( 37.7%) | Loss: 0.6930 | Speed: 97.5 batches/s | ETA: 06:07\n",
      "  Train: [21800/57525] ( 37.9%) | Loss: 0.6930 | Speed: 97.5 batches/s | ETA: 06:06\n",
      "  Train: [21900/57525] ( 38.1%) | Loss: 0.6930 | Speed: 97.5 batches/s | ETA: 06:05\n",
      "  Train: [22000/57525] ( 38.2%) | Loss: 0.6930 | Speed: 97.5 batches/s | ETA: 06:04\n",
      "  Train: [22100/57525] ( 38.4%) | Loss: 0.6929 | Speed: 97.5 batches/s | ETA: 06:03\n",
      "  Train: [22200/57525] ( 38.6%) | Loss: 0.6927 | Speed: 97.5 batches/s | ETA: 06:02\n",
      "  Train: [22300/57525] ( 38.8%) | Loss: 0.6926 | Speed: 97.5 batches/s | ETA: 06:01\n",
      "  Train: [22400/57525] ( 38.9%) | Loss: 0.6925 | Speed: 97.5 batches/s | ETA: 06:00\n",
      "  Train: [22500/57525] ( 39.1%) | Loss: 0.6924 | Speed: 97.5 batches/s | ETA: 05:59\n",
      "  Train: [22600/57525] ( 39.3%) | Loss: 0.6923 | Speed: 97.5 batches/s | ETA: 05:58\n",
      "  Train: [22700/57525] ( 39.5%) | Loss: 0.6922 | Speed: 97.5 batches/s | ETA: 05:57\n",
      "  Train: [22800/57525] ( 39.6%) | Loss: 0.6921 | Speed: 97.6 batches/s | ETA: 05:55\n",
      "  Train: [22900/57525] ( 39.8%) | Loss: 0.6920 | Speed: 97.6 batches/s | ETA: 05:54\n",
      "  Train: [23000/57525] ( 40.0%) | Loss: 0.6921 | Speed: 97.6 batches/s | ETA: 05:53\n",
      "  Train: [23100/57525] ( 40.2%) | Loss: 0.6919 | Speed: 97.7 batches/s | ETA: 05:52\n",
      "  Train: [23200/57525] ( 40.3%) | Loss: 0.6919 | Speed: 97.7 batches/s | ETA: 05:51\n",
      "  Train: [23300/57525] ( 40.5%) | Loss: 0.6919 | Speed: 97.7 batches/s | ETA: 05:50\n",
      "  Train: [23400/57525] ( 40.7%) | Loss: 0.6919 | Speed: 97.7 batches/s | ETA: 05:49\n",
      "  Train: [23500/57525] ( 40.9%) | Loss: 0.6918 | Speed: 97.7 batches/s | ETA: 05:48\n",
      "  Train: [23600/57525] ( 41.0%) | Loss: 0.6918 | Speed: 97.7 batches/s | ETA: 05:47\n",
      "  Train: [23700/57525] ( 41.2%) | Loss: 0.6918 | Speed: 97.6 batches/s | ETA: 05:46\n",
      "  Train: [23800/57525] ( 41.4%) | Loss: 0.6917 | Speed: 97.6 batches/s | ETA: 05:45\n",
      "  Train: [23900/57525] ( 41.5%) | Loss: 0.6917 | Speed: 97.6 batches/s | ETA: 05:44\n",
      "  Train: [24000/57525] ( 41.7%) | Loss: 0.6917 | Speed: 97.5 batches/s | ETA: 05:43\n",
      "  Train: [24100/57525] ( 41.9%) | Loss: 0.6916 | Speed: 97.5 batches/s | ETA: 05:42\n",
      "  Train: [24200/57525] ( 42.1%) | Loss: 0.6917 | Speed: 97.5 batches/s | ETA: 05:41\n",
      "  Train: [24300/57525] ( 42.2%) | Loss: 0.6917 | Speed: 97.4 batches/s | ETA: 05:41\n",
      "  Train: [24400/57525] ( 42.4%) | Loss: 0.6916 | Speed: 97.4 batches/s | ETA: 05:40\n",
      "  Train: [24500/57525] ( 42.6%) | Loss: 0.6916 | Speed: 97.3 batches/s | ETA: 05:39\n",
      "  Train: [24600/57525] ( 42.8%) | Loss: 0.6914 | Speed: 97.3 batches/s | ETA: 05:38\n",
      "  Train: [24700/57525] ( 42.9%) | Loss: 0.6913 | Speed: 97.3 batches/s | ETA: 05:37\n",
      "  Train: [24800/57525] ( 43.1%) | Loss: 0.6913 | Speed: 97.3 batches/s | ETA: 05:36\n",
      "  Train: [24900/57525] ( 43.3%) | Loss: 0.6913 | Speed: 97.2 batches/s | ETA: 05:35\n",
      "  Train: [25000/57525] ( 43.5%) | Loss: 0.6913 | Speed: 97.2 batches/s | ETA: 05:34\n",
      "  Train: [25100/57525] ( 43.6%) | Loss: 0.6913 | Speed: 97.2 batches/s | ETA: 05:33\n",
      "  Train: [25200/57525] ( 43.8%) | Loss: 0.6912 | Speed: 97.2 batches/s | ETA: 05:32\n",
      "  Train: [25300/57525] ( 44.0%) | Loss: 0.6911 | Speed: 97.1 batches/s | ETA: 05:31\n",
      "  Train: [25400/57525] ( 44.2%) | Loss: 0.6911 | Speed: 97.1 batches/s | ETA: 05:30\n",
      "  Train: [25500/57525] ( 44.3%) | Loss: 0.6911 | Speed: 97.1 batches/s | ETA: 05:29\n",
      "  Train: [25600/57525] ( 44.5%) | Loss: 0.6911 | Speed: 97.1 batches/s | ETA: 05:28\n",
      "  Train: [25700/57525] ( 44.7%) | Loss: 0.6910 | Speed: 97.1 batches/s | ETA: 05:27\n",
      "  Train: [25800/57525] ( 44.9%) | Loss: 0.6910 | Speed: 97.1 batches/s | ETA: 05:26\n",
      "  Train: [25900/57525] ( 45.0%) | Loss: 0.6909 | Speed: 97.1 batches/s | ETA: 05:25\n",
      "  Train: [26000/57525] ( 45.2%) | Loss: 0.6909 | Speed: 97.1 batches/s | ETA: 05:24\n",
      "  Train: [26100/57525] ( 45.4%) | Loss: 0.6908 | Speed: 97.2 batches/s | ETA: 05:23\n",
      "  Train: [26200/57525] ( 45.5%) | Loss: 0.6906 | Speed: 97.2 batches/s | ETA: 05:22\n",
      "  Train: [26300/57525] ( 45.7%) | Loss: 0.6906 | Speed: 97.2 batches/s | ETA: 05:21\n",
      "  Train: [26400/57525] ( 45.9%) | Loss: 0.6905 | Speed: 97.2 batches/s | ETA: 05:20\n",
      "  Train: [26500/57525] ( 46.1%) | Loss: 0.6905 | Speed: 97.2 batches/s | ETA: 05:19\n",
      "  Train: [26600/57525] ( 46.2%) | Loss: 0.6903 | Speed: 97.2 batches/s | ETA: 05:18\n",
      "  Train: [26700/57525] ( 46.4%) | Loss: 0.6903 | Speed: 97.2 batches/s | ETA: 05:17\n",
      "  Train: [26800/57525] ( 46.6%) | Loss: 0.6903 | Speed: 97.2 batches/s | ETA: 05:16\n",
      "  Train: [26900/57525] ( 46.8%) | Loss: 0.6903 | Speed: 97.2 batches/s | ETA: 05:15\n",
      "  Train: [27000/57525] ( 46.9%) | Loss: 0.6904 | Speed: 97.2 batches/s | ETA: 05:14\n",
      "  Train: [27100/57525] ( 47.1%) | Loss: 0.6904 | Speed: 97.2 batches/s | ETA: 05:13\n",
      "  Train: [27200/57525] ( 47.3%) | Loss: 0.6902 | Speed: 97.2 batches/s | ETA: 05:11\n",
      "  Train: [27300/57525] ( 47.5%) | Loss: 0.6902 | Speed: 97.2 batches/s | ETA: 05:10\n",
      "  Train: [27400/57525] ( 47.6%) | Loss: 0.6901 | Speed: 97.2 batches/s | ETA: 05:09\n",
      "  Train: [27500/57525] ( 47.8%) | Loss: 0.6900 | Speed: 97.2 batches/s | ETA: 05:08\n",
      "  Train: [27600/57525] ( 48.0%) | Loss: 0.6900 | Speed: 97.2 batches/s | ETA: 05:07\n",
      "  Train: [27700/57525] ( 48.2%) | Loss: 0.6899 | Speed: 97.2 batches/s | ETA: 05:06\n",
      "  Train: [27800/57525] ( 48.3%) | Loss: 0.6899 | Speed: 97.2 batches/s | ETA: 05:05\n",
      "  Train: [27900/57525] ( 48.5%) | Loss: 0.6899 | Speed: 97.2 batches/s | ETA: 05:04\n",
      "  Train: [28000/57525] ( 48.7%) | Loss: 0.6898 | Speed: 97.2 batches/s | ETA: 05:03\n",
      "  Train: [28100/57525] ( 48.8%) | Loss: 0.6898 | Speed: 97.2 batches/s | ETA: 05:02\n",
      "  Train: [28200/57525] ( 49.0%) | Loss: 0.6897 | Speed: 97.2 batches/s | ETA: 05:01\n",
      "  Train: [28300/57525] ( 49.2%) | Loss: 0.6896 | Speed: 97.2 batches/s | ETA: 05:00\n",
      "  Train: [28400/57525] ( 49.4%) | Loss: 0.6895 | Speed: 97.2 batches/s | ETA: 04:59\n",
      "  Train: [28500/57525] ( 49.5%) | Loss: 0.6896 | Speed: 97.2 batches/s | ETA: 04:58\n",
      "  Train: [28600/57525] ( 49.7%) | Loss: 0.6896 | Speed: 97.2 batches/s | ETA: 04:57\n",
      "  Train: [28700/57525] ( 49.9%) | Loss: 0.6896 | Speed: 97.2 batches/s | ETA: 04:56\n",
      "  Train: [28800/57525] ( 50.1%) | Loss: 0.6895 | Speed: 97.2 batches/s | ETA: 04:55\n",
      "  Train: [28900/57525] ( 50.2%) | Loss: 0.6896 | Speed: 97.2 batches/s | ETA: 04:54\n",
      "  Train: [29000/57525] ( 50.4%) | Loss: 0.6894 | Speed: 97.2 batches/s | ETA: 04:53\n",
      "  Train: [29100/57525] ( 50.6%) | Loss: 0.6894 | Speed: 97.2 batches/s | ETA: 04:52\n",
      "  Train: [29200/57525] ( 50.8%) | Loss: 0.6894 | Speed: 97.2 batches/s | ETA: 04:51\n",
      "  Train: [29300/57525] ( 50.9%) | Loss: 0.6895 | Speed: 97.2 batches/s | ETA: 04:50\n",
      "  Train: [29400/57525] ( 51.1%) | Loss: 0.6894 | Speed: 97.2 batches/s | ETA: 04:49\n",
      "  Train: [29500/57525] ( 51.3%) | Loss: 0.6893 | Speed: 97.2 batches/s | ETA: 04:48\n",
      "  Train: [29600/57525] ( 51.5%) | Loss: 0.6892 | Speed: 97.3 batches/s | ETA: 04:47\n",
      "  Train: [29700/57525] ( 51.6%) | Loss: 0.6892 | Speed: 97.3 batches/s | ETA: 04:46\n",
      "  Train: [29800/57525] ( 51.8%) | Loss: 0.6892 | Speed: 97.3 batches/s | ETA: 04:44\n",
      "  Train: [29900/57525] ( 52.0%) | Loss: 0.6891 | Speed: 97.3 batches/s | ETA: 04:43\n",
      "  Train: [30000/57525] ( 52.2%) | Loss: 0.6891 | Speed: 97.3 batches/s | ETA: 04:42\n",
      "  Train: [30100/57525] ( 52.3%) | Loss: 0.6892 | Speed: 97.3 batches/s | ETA: 04:41\n",
      "  Train: [30200/57525] ( 52.5%) | Loss: 0.6892 | Speed: 97.3 batches/s | ETA: 04:40\n",
      "  Train: [30300/57525] ( 52.7%) | Loss: 0.6891 | Speed: 97.3 batches/s | ETA: 04:39\n",
      "  Train: [30400/57525] ( 52.8%) | Loss: 0.6891 | Speed: 97.3 batches/s | ETA: 04:38\n",
      "  Train: [30500/57525] ( 53.0%) | Loss: 0.6891 | Speed: 97.3 batches/s | ETA: 04:37\n",
      "  Train: [30600/57525] ( 53.2%) | Loss: 0.6890 | Speed: 97.3 batches/s | ETA: 04:36\n",
      "  Train: [30700/57525] ( 53.4%) | Loss: 0.6889 | Speed: 97.3 batches/s | ETA: 04:35\n",
      "  Train: [30800/57525] ( 53.5%) | Loss: 0.6889 | Speed: 97.3 batches/s | ETA: 04:34\n",
      "  Train: [30900/57525] ( 53.7%) | Loss: 0.6888 | Speed: 97.3 batches/s | ETA: 04:33\n",
      "  Train: [31000/57525] ( 53.9%) | Loss: 0.6888 | Speed: 97.3 batches/s | ETA: 04:32\n",
      "  Train: [31100/57525] ( 54.1%) | Loss: 0.6887 | Speed: 97.3 batches/s | ETA: 04:31\n",
      "  Train: [31200/57525] ( 54.2%) | Loss: 0.6886 | Speed: 97.3 batches/s | ETA: 04:30\n",
      "  Train: [31300/57525] ( 54.4%) | Loss: 0.6886 | Speed: 97.3 batches/s | ETA: 04:29\n",
      "  Train: [31400/57525] ( 54.6%) | Loss: 0.6885 | Speed: 97.3 batches/s | ETA: 04:28\n",
      "  Train: [31500/57525] ( 54.8%) | Loss: 0.6886 | Speed: 97.3 batches/s | ETA: 04:27\n",
      "  Train: [31600/57525] ( 54.9%) | Loss: 0.6885 | Speed: 97.3 batches/s | ETA: 04:26\n",
      "  Train: [31700/57525] ( 55.1%) | Loss: 0.6886 | Speed: 97.3 batches/s | ETA: 04:25\n",
      "  Train: [31800/57525] ( 55.3%) | Loss: 0.6885 | Speed: 97.3 batches/s | ETA: 04:24\n",
      "  Train: [31900/57525] ( 55.5%) | Loss: 0.6885 | Speed: 97.3 batches/s | ETA: 04:23\n",
      "  Train: [32000/57525] ( 55.6%) | Loss: 0.6884 | Speed: 97.3 batches/s | ETA: 04:22\n",
      "  Train: [32100/57525] ( 55.8%) | Loss: 0.6882 | Speed: 97.3 batches/s | ETA: 04:21\n",
      "  Train: [32200/57525] ( 56.0%) | Loss: 0.6882 | Speed: 97.3 batches/s | ETA: 04:20\n",
      "  Train: [32300/57525] ( 56.1%) | Loss: 0.6881 | Speed: 97.3 batches/s | ETA: 04:19\n",
      "  Train: [32400/57525] ( 56.3%) | Loss: 0.6881 | Speed: 97.3 batches/s | ETA: 04:18\n",
      "  Train: [32500/57525] ( 56.5%) | Loss: 0.6881 | Speed: 97.3 batches/s | ETA: 04:17\n",
      "  Train: [32600/57525] ( 56.7%) | Loss: 0.6881 | Speed: 97.4 batches/s | ETA: 04:16\n",
      "  Train: [32700/57525] ( 56.8%) | Loss: 0.6880 | Speed: 97.4 batches/s | ETA: 04:14\n",
      "  Train: [32800/57525] ( 57.0%) | Loss: 0.6879 | Speed: 97.4 batches/s | ETA: 04:13\n",
      "  Train: [32900/57525] ( 57.2%) | Loss: 0.6879 | Speed: 97.4 batches/s | ETA: 04:12\n",
      "  Train: [33000/57525] ( 57.4%) | Loss: 0.6878 | Speed: 97.5 batches/s | ETA: 04:11\n",
      "  Train: [33100/57525] ( 57.5%) | Loss: 0.6878 | Speed: 97.5 batches/s | ETA: 04:10\n",
      "  Train: [33200/57525] ( 57.7%) | Loss: 0.6878 | Speed: 97.5 batches/s | ETA: 04:09\n",
      "  Train: [33300/57525] ( 57.9%) | Loss: 0.6877 | Speed: 97.5 batches/s | ETA: 04:08\n",
      "  Train: [33400/57525] ( 58.1%) | Loss: 0.6877 | Speed: 97.6 batches/s | ETA: 04:07\n",
      "  Train: [33500/57525] ( 58.2%) | Loss: 0.6876 | Speed: 97.6 batches/s | ETA: 04:06\n",
      "  Train: [33600/57525] ( 58.4%) | Loss: 0.6876 | Speed: 97.6 batches/s | ETA: 04:05\n",
      "  Train: [33700/57525] ( 58.6%) | Loss: 0.6875 | Speed: 97.6 batches/s | ETA: 04:04\n",
      "  Train: [33800/57525] ( 58.8%) | Loss: 0.6875 | Speed: 97.6 batches/s | ETA: 04:02\n",
      "  Train: [33900/57525] ( 58.9%) | Loss: 0.6875 | Speed: 97.7 batches/s | ETA: 04:01\n",
      "  Train: [34000/57525] ( 59.1%) | Loss: 0.6875 | Speed: 97.7 batches/s | ETA: 04:00\n",
      "  Train: [34100/57525] ( 59.3%) | Loss: 0.6875 | Speed: 97.7 batches/s | ETA: 03:59\n",
      "  Train: [34200/57525] ( 59.5%) | Loss: 0.6875 | Speed: 97.7 batches/s | ETA: 03:58\n",
      "  Train: [34300/57525] ( 59.6%) | Loss: 0.6874 | Speed: 97.8 batches/s | ETA: 03:57\n",
      "  Train: [34400/57525] ( 59.8%) | Loss: 0.6874 | Speed: 97.8 batches/s | ETA: 03:56\n",
      "  Train: [34500/57525] ( 60.0%) | Loss: 0.6874 | Speed: 97.8 batches/s | ETA: 03:55\n",
      "  Train: [34600/57525] ( 60.1%) | Loss: 0.6873 | Speed: 97.8 batches/s | ETA: 03:54\n",
      "  Train: [34700/57525] ( 60.3%) | Loss: 0.6873 | Speed: 97.8 batches/s | ETA: 03:53\n",
      "  Train: [34800/57525] ( 60.5%) | Loss: 0.6873 | Speed: 97.9 batches/s | ETA: 03:52\n",
      "  Train: [34900/57525] ( 60.7%) | Loss: 0.6873 | Speed: 97.9 batches/s | ETA: 03:51\n",
      "  Train: [35000/57525] ( 60.8%) | Loss: 0.6872 | Speed: 97.9 batches/s | ETA: 03:50\n",
      "  Train: [35100/57525] ( 61.0%) | Loss: 0.6872 | Speed: 97.9 batches/s | ETA: 03:48\n",
      "  Train: [35200/57525] ( 61.2%) | Loss: 0.6871 | Speed: 98.0 batches/s | ETA: 03:47\n",
      "  Train: [35300/57525] ( 61.4%) | Loss: 0.6871 | Speed: 98.0 batches/s | ETA: 03:46\n",
      "  Train: [35400/57525] ( 61.5%) | Loss: 0.6870 | Speed: 98.0 batches/s | ETA: 03:45\n",
      "  Train: [35500/57525] ( 61.7%) | Loss: 0.6870 | Speed: 98.0 batches/s | ETA: 03:44\n",
      "  Train: [35600/57525] ( 61.9%) | Loss: 0.6870 | Speed: 98.0 batches/s | ETA: 03:43\n",
      "  Train: [35700/57525] ( 62.1%) | Loss: 0.6869 | Speed: 98.0 batches/s | ETA: 03:42\n",
      "  Train: [35800/57525] ( 62.2%) | Loss: 0.6869 | Speed: 98.1 batches/s | ETA: 03:41\n",
      "  Train: [35900/57525] ( 62.4%) | Loss: 0.6869 | Speed: 98.1 batches/s | ETA: 03:40\n",
      "  Train: [36000/57525] ( 62.6%) | Loss: 0.6868 | Speed: 98.1 batches/s | ETA: 03:39\n",
      "  Train: [36100/57525] ( 62.8%) | Loss: 0.6868 | Speed: 98.1 batches/s | ETA: 03:38\n",
      "  Train: [36200/57525] ( 62.9%) | Loss: 0.6868 | Speed: 98.1 batches/s | ETA: 03:37\n",
      "  Train: [36300/57525] ( 63.1%) | Loss: 0.6867 | Speed: 98.1 batches/s | ETA: 03:36\n",
      "  Train: [36400/57525] ( 63.3%) | Loss: 0.6867 | Speed: 98.1 batches/s | ETA: 03:35\n",
      "  Train: [36500/57525] ( 63.5%) | Loss: 0.6867 | Speed: 98.1 batches/s | ETA: 03:34\n",
      "  Train: [36600/57525] ( 63.6%) | Loss: 0.6866 | Speed: 98.1 batches/s | ETA: 03:33\n",
      "  Train: [36700/57525] ( 63.8%) | Loss: 0.6866 | Speed: 98.2 batches/s | ETA: 03:32\n",
      "  Train: [36800/57525] ( 64.0%) | Loss: 0.6866 | Speed: 98.2 batches/s | ETA: 03:31\n",
      "  Train: [36900/57525] ( 64.1%) | Loss: 0.6865 | Speed: 98.2 batches/s | ETA: 03:30\n",
      "  Train: [37000/57525] ( 64.3%) | Loss: 0.6865 | Speed: 98.2 batches/s | ETA: 03:29\n",
      "  Train: [37100/57525] ( 64.5%) | Loss: 0.6865 | Speed: 98.2 batches/s | ETA: 03:28\n",
      "  Train: [37200/57525] ( 64.7%) | Loss: 0.6865 | Speed: 98.2 batches/s | ETA: 03:27\n",
      "  Train: [37300/57525] ( 64.8%) | Loss: 0.6864 | Speed: 98.2 batches/s | ETA: 03:25\n",
      "  Train: [37400/57525] ( 65.0%) | Loss: 0.6864 | Speed: 98.2 batches/s | ETA: 03:24\n",
      "  Train: [37500/57525] ( 65.2%) | Loss: 0.6864 | Speed: 98.2 batches/s | ETA: 03:23\n",
      "  Train: [37600/57525] ( 65.4%) | Loss: 0.6863 | Speed: 98.2 batches/s | ETA: 03:22\n",
      "  Train: [37700/57525] ( 65.5%) | Loss: 0.6863 | Speed: 98.2 batches/s | ETA: 03:21\n",
      "  Train: [37800/57525] ( 65.7%) | Loss: 0.6863 | Speed: 98.2 batches/s | ETA: 03:20\n",
      "  Train: [37900/57525] ( 65.9%) | Loss: 0.6862 | Speed: 98.2 batches/s | ETA: 03:19\n",
      "  Train: [38000/57525] ( 66.1%) | Loss: 0.6862 | Speed: 98.2 batches/s | ETA: 03:18\n",
      "  Train: [38100/57525] ( 66.2%) | Loss: 0.6861 | Speed: 98.2 batches/s | ETA: 03:17\n",
      "  Train: [38200/57525] ( 66.4%) | Loss: 0.6861 | Speed: 98.2 batches/s | ETA: 03:16\n",
      "  Train: [38300/57525] ( 66.6%) | Loss: 0.6861 | Speed: 98.2 batches/s | ETA: 03:15\n",
      "  Train: [38400/57525] ( 66.8%) | Loss: 0.6861 | Speed: 98.2 batches/s | ETA: 03:14\n",
      "  Train: [38500/57525] ( 66.9%) | Loss: 0.6860 | Speed: 98.2 batches/s | ETA: 03:13\n",
      "  Train: [38600/57525] ( 67.1%) | Loss: 0.6860 | Speed: 98.2 batches/s | ETA: 03:12\n",
      "  Train: [38700/57525] ( 67.3%) | Loss: 0.6859 | Speed: 98.2 batches/s | ETA: 03:11\n",
      "  Train: [38800/57525] ( 67.4%) | Loss: 0.6859 | Speed: 98.2 batches/s | ETA: 03:10\n",
      "  Train: [38900/57525] ( 67.6%) | Loss: 0.6859 | Speed: 98.2 batches/s | ETA: 03:09\n",
      "  Train: [39000/57525] ( 67.8%) | Loss: 0.6858 | Speed: 98.2 batches/s | ETA: 03:08\n",
      "  Train: [39100/57525] ( 68.0%) | Loss: 0.6858 | Speed: 98.3 batches/s | ETA: 03:07\n",
      "  Train: [39200/57525] ( 68.1%) | Loss: 0.6858 | Speed: 98.3 batches/s | ETA: 03:06\n",
      "  Train: [39300/57525] ( 68.3%) | Loss: 0.6857 | Speed: 98.3 batches/s | ETA: 03:05\n",
      "  Train: [39400/57525] ( 68.5%) | Loss: 0.6857 | Speed: 98.3 batches/s | ETA: 03:04\n",
      "  Train: [39500/57525] ( 68.7%) | Loss: 0.6857 | Speed: 98.3 batches/s | ETA: 03:03\n",
      "  Train: [39600/57525] ( 68.8%) | Loss: 0.6857 | Speed: 98.3 batches/s | ETA: 03:02\n",
      "  Train: [39700/57525] ( 69.0%) | Loss: 0.6857 | Speed: 98.4 batches/s | ETA: 03:01\n",
      "  Train: [39800/57525] ( 69.2%) | Loss: 0.6856 | Speed: 98.4 batches/s | ETA: 03:00\n",
      "  Train: [39900/57525] ( 69.4%) | Loss: 0.6855 | Speed: 98.4 batches/s | ETA: 02:59\n",
      "  Train: [40000/57525] ( 69.5%) | Loss: 0.6855 | Speed: 98.4 batches/s | ETA: 02:58\n",
      "  Train: [40100/57525] ( 69.7%) | Loss: 0.6855 | Speed: 98.4 batches/s | ETA: 02:57\n",
      "  Train: [40200/57525] ( 69.9%) | Loss: 0.6854 | Speed: 98.4 batches/s | ETA: 02:55\n",
      "  Train: [40300/57525] ( 70.1%) | Loss: 0.6854 | Speed: 98.5 batches/s | ETA: 02:54\n",
      "  Train: [40400/57525] ( 70.2%) | Loss: 0.6854 | Speed: 98.5 batches/s | ETA: 02:53\n",
      "  Train: [40500/57525] ( 70.4%) | Loss: 0.6854 | Speed: 98.5 batches/s | ETA: 02:52\n",
      "  Train: [40600/57525] ( 70.6%) | Loss: 0.6853 | Speed: 98.5 batches/s | ETA: 02:51\n",
      "  Train: [40700/57525] ( 70.8%) | Loss: 0.6853 | Speed: 98.5 batches/s | ETA: 02:50\n",
      "  Train: [40800/57525] ( 70.9%) | Loss: 0.6854 | Speed: 98.5 batches/s | ETA: 02:49\n",
      "  Train: [40900/57525] ( 71.1%) | Loss: 0.6854 | Speed: 98.5 batches/s | ETA: 02:48\n",
      "  Train: [41000/57525] ( 71.3%) | Loss: 0.6852 | Speed: 98.6 batches/s | ETA: 02:47\n",
      "  Train: [41100/57525] ( 71.4%) | Loss: 0.6852 | Speed: 98.6 batches/s | ETA: 02:46\n",
      "  Train: [41200/57525] ( 71.6%) | Loss: 0.6851 | Speed: 98.6 batches/s | ETA: 02:45\n",
      "  Train: [41300/57525] ( 71.8%) | Loss: 0.6851 | Speed: 98.6 batches/s | ETA: 02:44\n",
      "  Train: [41400/57525] ( 72.0%) | Loss: 0.6850 | Speed: 98.6 batches/s | ETA: 02:43\n",
      "  Train: [41500/57525] ( 72.1%) | Loss: 0.6850 | Speed: 98.6 batches/s | ETA: 02:42\n",
      "  Train: [41600/57525] ( 72.3%) | Loss: 0.6850 | Speed: 98.6 batches/s | ETA: 02:41\n",
      "  Train: [41700/57525] ( 72.5%) | Loss: 0.6850 | Speed: 98.7 batches/s | ETA: 02:40\n",
      "  Train: [41800/57525] ( 72.7%) | Loss: 0.6849 | Speed: 98.7 batches/s | ETA: 02:39\n",
      "  Train: [41900/57525] ( 72.8%) | Loss: 0.6849 | Speed: 98.7 batches/s | ETA: 02:38\n",
      "  Train: [42000/57525] ( 73.0%) | Loss: 0.6848 | Speed: 98.7 batches/s | ETA: 02:37\n",
      "  Train: [42100/57525] ( 73.2%) | Loss: 0.6848 | Speed: 98.7 batches/s | ETA: 02:36\n",
      "  Train: [42200/57525] ( 73.4%) | Loss: 0.6848 | Speed: 98.8 batches/s | ETA: 02:35\n",
      "  Train: [42300/57525] ( 73.5%) | Loss: 0.6848 | Speed: 98.8 batches/s | ETA: 02:34\n",
      "  Train: [42400/57525] ( 73.7%) | Loss: 0.6847 | Speed: 98.8 batches/s | ETA: 02:33\n",
      "  Train: [42500/57525] ( 73.9%) | Loss: 0.6847 | Speed: 98.8 batches/s | ETA: 02:32\n",
      "  Train: [42600/57525] ( 74.1%) | Loss: 0.6847 | Speed: 98.8 batches/s | ETA: 02:31\n",
      "  Train: [42700/57525] ( 74.2%) | Loss: 0.6846 | Speed: 98.8 batches/s | ETA: 02:30\n",
      "  Train: [42800/57525] ( 74.4%) | Loss: 0.6846 | Speed: 98.8 batches/s | ETA: 02:28\n",
      "  Train: [42900/57525] ( 74.6%) | Loss: 0.6845 | Speed: 98.9 batches/s | ETA: 02:27\n",
      "  Train: [43000/57525] ( 74.8%) | Loss: 0.6844 | Speed: 98.9 batches/s | ETA: 02:26\n",
      "  Train: [43100/57525] ( 74.9%) | Loss: 0.6844 | Speed: 98.9 batches/s | ETA: 02:25\n",
      "  Train: [43200/57525] ( 75.1%) | Loss: 0.6844 | Speed: 98.9 batches/s | ETA: 02:24\n",
      "  Train: [43300/57525] ( 75.3%) | Loss: 0.6844 | Speed: 98.9 batches/s | ETA: 02:23\n",
      "  Train: [43400/57525] ( 75.4%) | Loss: 0.6843 | Speed: 98.9 batches/s | ETA: 02:22\n",
      "  Train: [43500/57525] ( 75.6%) | Loss: 0.6843 | Speed: 99.0 batches/s | ETA: 02:21\n",
      "  Train: [43600/57525] ( 75.8%) | Loss: 0.6842 | Speed: 99.0 batches/s | ETA: 02:20\n",
      "  Train: [43700/57525] ( 76.0%) | Loss: 0.6842 | Speed: 99.0 batches/s | ETA: 02:19\n",
      "  Train: [43800/57525] ( 76.1%) | Loss: 0.6842 | Speed: 99.0 batches/s | ETA: 02:18\n",
      "  Train: [43900/57525] ( 76.3%) | Loss: 0.6842 | Speed: 99.0 batches/s | ETA: 02:17\n",
      "  Train: [44000/57525] ( 76.5%) | Loss: 0.6841 | Speed: 99.0 batches/s | ETA: 02:16\n",
      "  Train: [44100/57525] ( 76.7%) | Loss: 0.6841 | Speed: 99.0 batches/s | ETA: 02:15\n",
      "  Train: [44200/57525] ( 76.8%) | Loss: 0.6840 | Speed: 99.1 batches/s | ETA: 02:14\n",
      "  Train: [44300/57525] ( 77.0%) | Loss: 0.6840 | Speed: 99.1 batches/s | ETA: 02:13\n",
      "  Train: [44400/57525] ( 77.2%) | Loss: 0.6840 | Speed: 99.1 batches/s | ETA: 02:12\n",
      "  Train: [44500/57525] ( 77.4%) | Loss: 0.6840 | Speed: 99.1 batches/s | ETA: 02:11\n",
      "  Train: [44600/57525] ( 77.5%) | Loss: 0.6839 | Speed: 99.1 batches/s | ETA: 02:10\n",
      "  Train: [44700/57525] ( 77.7%) | Loss: 0.6839 | Speed: 99.1 batches/s | ETA: 02:09\n",
      "  Train: [44800/57525] ( 77.9%) | Loss: 0.6838 | Speed: 99.1 batches/s | ETA: 02:08\n",
      "  Train: [44900/57525] ( 78.1%) | Loss: 0.6838 | Speed: 99.1 batches/s | ETA: 02:07\n",
      "  Train: [45000/57525] ( 78.2%) | Loss: 0.6837 | Speed: 99.1 batches/s | ETA: 02:06\n",
      "  Train: [45100/57525] ( 78.4%) | Loss: 0.6837 | Speed: 99.2 batches/s | ETA: 02:05\n",
      "  Train: [45200/57525] ( 78.6%) | Loss: 0.6836 | Speed: 99.2 batches/s | ETA: 02:04\n",
      "  Train: [45300/57525] ( 78.7%) | Loss: 0.6836 | Speed: 99.2 batches/s | ETA: 02:03\n",
      "  Train: [45400/57525] ( 78.9%) | Loss: 0.6835 | Speed: 99.2 batches/s | ETA: 02:02\n",
      "  Train: [45500/57525] ( 79.1%) | Loss: 0.6835 | Speed: 99.2 batches/s | ETA: 02:01\n",
      "  Train: [45600/57525] ( 79.3%) | Loss: 0.6835 | Speed: 99.1 batches/s | ETA: 02:00\n",
      "  Train: [45700/57525] ( 79.4%) | Loss: 0.6835 | Speed: 99.1 batches/s | ETA: 01:59\n",
      "  Train: [45800/57525] ( 79.6%) | Loss: 0.6834 | Speed: 99.1 batches/s | ETA: 01:58\n",
      "  Train: [45900/57525] ( 79.8%) | Loss: 0.6834 | Speed: 99.1 batches/s | ETA: 01:57\n",
      "  Train: [46000/57525] ( 80.0%) | Loss: 0.6833 | Speed: 99.1 batches/s | ETA: 01:56\n",
      "  Train: [46100/57525] ( 80.1%) | Loss: 0.6833 | Speed: 99.1 batches/s | ETA: 01:55\n",
      "  Train: [46200/57525] ( 80.3%) | Loss: 0.6832 | Speed: 99.1 batches/s | ETA: 01:54\n",
      "  Train: [46300/57525] ( 80.5%) | Loss: 0.6831 | Speed: 99.1 batches/s | ETA: 01:53\n",
      "  Train: [46400/57525] ( 80.7%) | Loss: 0.6831 | Speed: 99.1 batches/s | ETA: 01:52\n",
      "  Train: [46500/57525] ( 80.8%) | Loss: 0.6831 | Speed: 99.1 batches/s | ETA: 01:51\n",
      "  Train: [46600/57525] ( 81.0%) | Loss: 0.6830 | Speed: 99.1 batches/s | ETA: 01:50\n",
      "  Train: [46700/57525] ( 81.2%) | Loss: 0.6830 | Speed: 99.1 batches/s | ETA: 01:49\n",
      "  Train: [46800/57525] ( 81.4%) | Loss: 0.6830 | Speed: 99.1 batches/s | ETA: 01:48\n",
      "  Train: [46900/57525] ( 81.5%) | Loss: 0.6830 | Speed: 99.1 batches/s | ETA: 01:47\n",
      "  Train: [47000/57525] ( 81.7%) | Loss: 0.6829 | Speed: 99.1 batches/s | ETA: 01:46\n",
      "  Train: [47100/57525] ( 81.9%) | Loss: 0.6829 | Speed: 99.1 batches/s | ETA: 01:45\n",
      "  Train: [47200/57525] ( 82.1%) | Loss: 0.6829 | Speed: 99.1 batches/s | ETA: 01:44\n",
      "  Train: [47300/57525] ( 82.2%) | Loss: 0.6829 | Speed: 99.1 batches/s | ETA: 01:43\n",
      "  Train: [47400/57525] ( 82.4%) | Loss: 0.6829 | Speed: 99.1 batches/s | ETA: 01:42\n",
      "  Train: [47500/57525] ( 82.6%) | Loss: 0.6828 | Speed: 99.1 batches/s | ETA: 01:41\n",
      "  Train: [47600/57525] ( 82.7%) | Loss: 0.6827 | Speed: 99.1 batches/s | ETA: 01:40\n",
      "  Train: [47700/57525] ( 82.9%) | Loss: 0.6827 | Speed: 99.1 batches/s | ETA: 01:39\n",
      "  Train: [47800/57525] ( 83.1%) | Loss: 0.6827 | Speed: 99.1 batches/s | ETA: 01:38\n",
      "  Train: [47900/57525] ( 83.3%) | Loss: 0.6826 | Speed: 99.1 batches/s | ETA: 01:37\n",
      "  Train: [48000/57525] ( 83.4%) | Loss: 0.6826 | Speed: 99.1 batches/s | ETA: 01:36\n",
      "  Train: [48100/57525] ( 83.6%) | Loss: 0.6826 | Speed: 99.1 batches/s | ETA: 01:35\n",
      "  Train: [48200/57525] ( 83.8%) | Loss: 0.6826 | Speed: 99.1 batches/s | ETA: 01:34\n",
      "  Train: [48300/57525] ( 84.0%) | Loss: 0.6825 | Speed: 99.1 batches/s | ETA: 01:33\n",
      "  Train: [48400/57525] ( 84.1%) | Loss: 0.6824 | Speed: 99.1 batches/s | ETA: 01:32\n",
      "  Train: [48500/57525] ( 84.3%) | Loss: 0.6824 | Speed: 99.1 batches/s | ETA: 01:31\n",
      "  Train: [48600/57525] ( 84.5%) | Loss: 0.6824 | Speed: 99.1 batches/s | ETA: 01:30\n",
      "  Train: [48700/57525] ( 84.7%) | Loss: 0.6824 | Speed: 99.1 batches/s | ETA: 01:29\n",
      "  Train: [48800/57525] ( 84.8%) | Loss: 0.6824 | Speed: 99.1 batches/s | ETA: 01:28\n",
      "  Train: [48900/57525] ( 85.0%) | Loss: 0.6824 | Speed: 99.1 batches/s | ETA: 01:27\n",
      "  Train: [49000/57525] ( 85.2%) | Loss: 0.6824 | Speed: 99.1 batches/s | ETA: 01:25\n",
      "  Train: [49100/57525] ( 85.4%) | Loss: 0.6823 | Speed: 99.1 batches/s | ETA: 01:24\n",
      "  Train: [49200/57525] ( 85.5%) | Loss: 0.6823 | Speed: 99.1 batches/s | ETA: 01:23\n",
      "  Train: [49300/57525] ( 85.7%) | Loss: 0.6823 | Speed: 99.1 batches/s | ETA: 01:22\n",
      "  Train: [49400/57525] ( 85.9%) | Loss: 0.6823 | Speed: 99.1 batches/s | ETA: 01:21\n",
      "  Train: [49500/57525] ( 86.0%) | Loss: 0.6822 | Speed: 99.1 batches/s | ETA: 01:20\n",
      "  Train: [49600/57525] ( 86.2%) | Loss: 0.6822 | Speed: 99.1 batches/s | ETA: 01:19\n",
      "  Train: [49700/57525] ( 86.4%) | Loss: 0.6821 | Speed: 99.1 batches/s | ETA: 01:18\n",
      "  Train: [49800/57525] ( 86.6%) | Loss: 0.6821 | Speed: 99.1 batches/s | ETA: 01:17\n",
      "  Train: [49900/57525] ( 86.7%) | Loss: 0.6821 | Speed: 99.1 batches/s | ETA: 01:16\n",
      "  Train: [50000/57525] ( 86.9%) | Loss: 0.6821 | Speed: 99.1 batches/s | ETA: 01:15\n",
      "  Train: [50100/57525] ( 87.1%) | Loss: 0.6820 | Speed: 99.1 batches/s | ETA: 01:14\n",
      "  Train: [50200/57525] ( 87.3%) | Loss: 0.6820 | Speed: 99.1 batches/s | ETA: 01:13\n",
      "  Train: [50300/57525] ( 87.4%) | Loss: 0.6820 | Speed: 99.1 batches/s | ETA: 01:12\n",
      "  Train: [50400/57525] ( 87.6%) | Loss: 0.6819 | Speed: 99.1 batches/s | ETA: 01:11\n",
      "  Train: [50500/57525] ( 87.8%) | Loss: 0.6819 | Speed: 99.1 batches/s | ETA: 01:10\n",
      "  Train: [50600/57525] ( 88.0%) | Loss: 0.6819 | Speed: 99.1 batches/s | ETA: 01:09\n",
      "  Train: [50700/57525] ( 88.1%) | Loss: 0.6819 | Speed: 99.1 batches/s | ETA: 01:08\n",
      "  Train: [50800/57525] ( 88.3%) | Loss: 0.6818 | Speed: 99.1 batches/s | ETA: 01:07\n",
      "  Train: [50900/57525] ( 88.5%) | Loss: 0.6818 | Speed: 99.1 batches/s | ETA: 01:06\n",
      "  Train: [51000/57525] ( 88.7%) | Loss: 0.6817 | Speed: 99.1 batches/s | ETA: 01:05\n",
      "  Train: [51100/57525] ( 88.8%) | Loss: 0.6817 | Speed: 99.1 batches/s | ETA: 01:04\n",
      "  Train: [51200/57525] ( 89.0%) | Loss: 0.6817 | Speed: 99.1 batches/s | ETA: 01:03\n",
      "  Train: [51300/57525] ( 89.2%) | Loss: 0.6816 | Speed: 99.1 batches/s | ETA: 01:02\n",
      "  Train: [51400/57525] ( 89.4%) | Loss: 0.6816 | Speed: 99.1 batches/s | ETA: 01:01\n",
      "  Train: [51500/57525] ( 89.5%) | Loss: 0.6816 | Speed: 99.1 batches/s | ETA: 01:00\n",
      "  Train: [51600/57525] ( 89.7%) | Loss: 0.6815 | Speed: 99.1 batches/s | ETA: 00:59\n",
      "  Train: [51700/57525] ( 89.9%) | Loss: 0.6815 | Speed: 99.1 batches/s | ETA: 00:58\n",
      "  Train: [51800/57525] ( 90.0%) | Loss: 0.6815 | Speed: 99.1 batches/s | ETA: 00:57\n",
      "  Train: [51900/57525] ( 90.2%) | Loss: 0.6815 | Speed: 99.1 batches/s | ETA: 00:56\n",
      "  Train: [52000/57525] ( 90.4%) | Loss: 0.6815 | Speed: 99.1 batches/s | ETA: 00:55\n",
      "  Train: [52100/57525] ( 90.6%) | Loss: 0.6815 | Speed: 99.1 batches/s | ETA: 00:54\n",
      "  Train: [52200/57525] ( 90.7%) | Loss: 0.6814 | Speed: 99.1 batches/s | ETA: 00:53\n",
      "  Train: [52300/57525] ( 90.9%) | Loss: 0.6814 | Speed: 99.1 batches/s | ETA: 00:52\n",
      "  Train: [52400/57525] ( 91.1%) | Loss: 0.6814 | Speed: 99.1 batches/s | ETA: 00:51\n",
      "  Train: [52500/57525] ( 91.3%) | Loss: 0.6814 | Speed: 99.1 batches/s | ETA: 00:50\n",
      "  Train: [52600/57525] ( 91.4%) | Loss: 0.6813 | Speed: 99.1 batches/s | ETA: 00:49\n",
      "  Train: [52700/57525] ( 91.6%) | Loss: 0.6813 | Speed: 99.1 batches/s | ETA: 00:48\n",
      "  Train: [52800/57525] ( 91.8%) | Loss: 0.6813 | Speed: 99.0 batches/s | ETA: 00:47\n",
      "  Train: [52900/57525] ( 92.0%) | Loss: 0.6813 | Speed: 99.0 batches/s | ETA: 00:46\n",
      "  Train: [53000/57525] ( 92.1%) | Loss: 0.6812 | Speed: 99.0 batches/s | ETA: 00:45\n",
      "  Train: [53100/57525] ( 92.3%) | Loss: 0.6812 | Speed: 99.0 batches/s | ETA: 00:44\n",
      "  Train: [53200/57525] ( 92.5%) | Loss: 0.6812 | Speed: 99.0 batches/s | ETA: 00:43\n",
      "  Train: [53300/57525] ( 92.7%) | Loss: 0.6812 | Speed: 99.0 batches/s | ETA: 00:42\n",
      "  Train: [53400/57525] ( 92.8%) | Loss: 0.6811 | Speed: 99.0 batches/s | ETA: 00:41\n",
      "  Train: [53500/57525] ( 93.0%) | Loss: 0.6811 | Speed: 99.0 batches/s | ETA: 00:40\n",
      "  Train: [53600/57525] ( 93.2%) | Loss: 0.6811 | Speed: 99.0 batches/s | ETA: 00:39\n",
      "  Train: [53700/57525] ( 93.4%) | Loss: 0.6811 | Speed: 99.0 batches/s | ETA: 00:38\n",
      "  Train: [53800/57525] ( 93.5%) | Loss: 0.6811 | Speed: 99.0 batches/s | ETA: 00:37\n",
      "  Train: [53900/57525] ( 93.7%) | Loss: 0.6810 | Speed: 99.0 batches/s | ETA: 00:36\n",
      "  Train: [54000/57525] ( 93.9%) | Loss: 0.6811 | Speed: 99.0 batches/s | ETA: 00:35\n",
      "  Train: [54100/57525] ( 94.0%) | Loss: 0.6810 | Speed: 99.0 batches/s | ETA: 00:34\n",
      "  Train: [54200/57525] ( 94.2%) | Loss: 0.6810 | Speed: 99.0 batches/s | ETA: 00:33\n",
      "  Train: [54300/57525] ( 94.4%) | Loss: 0.6810 | Speed: 99.0 batches/s | ETA: 00:32\n",
      "  Train: [54400/57525] ( 94.6%) | Loss: 0.6809 | Speed: 99.0 batches/s | ETA: 00:31\n",
      "  Train: [54500/57525] ( 94.7%) | Loss: 0.6809 | Speed: 99.0 batches/s | ETA: 00:30\n",
      "  Train: [54600/57525] ( 94.9%) | Loss: 0.6809 | Speed: 99.0 batches/s | ETA: 00:29\n",
      "  Train: [54700/57525] ( 95.1%) | Loss: 0.6809 | Speed: 99.0 batches/s | ETA: 00:28\n",
      "  Train: [54800/57525] ( 95.3%) | Loss: 0.6808 | Speed: 99.0 batches/s | ETA: 00:27\n",
      "  Train: [54900/57525] ( 95.4%) | Loss: 0.6808 | Speed: 99.0 batches/s | ETA: 00:26\n",
      "  Train: [55000/57525] ( 95.6%) | Loss: 0.6808 | Speed: 99.0 batches/s | ETA: 00:25\n",
      "  Train: [55100/57525] ( 95.8%) | Loss: 0.6808 | Speed: 99.0 batches/s | ETA: 00:24\n",
      "  Train: [55200/57525] ( 96.0%) | Loss: 0.6808 | Speed: 99.0 batches/s | ETA: 00:23\n",
      "  Train: [55300/57525] ( 96.1%) | Loss: 0.6807 | Speed: 99.0 batches/s | ETA: 00:22\n",
      "  Train: [55400/57525] ( 96.3%) | Loss: 0.6807 | Speed: 99.0 batches/s | ETA: 00:21\n",
      "  Train: [55500/57525] ( 96.5%) | Loss: 0.6806 | Speed: 99.0 batches/s | ETA: 00:20\n",
      "  Train: [55600/57525] ( 96.7%) | Loss: 0.6806 | Speed: 99.0 batches/s | ETA: 00:19\n",
      "  Train: [55700/57525] ( 96.8%) | Loss: 0.6806 | Speed: 99.0 batches/s | ETA: 00:18\n",
      "  Train: [55800/57525] ( 97.0%) | Loss: 0.6806 | Speed: 99.0 batches/s | ETA: 00:17\n",
      "  Train: [55900/57525] ( 97.2%) | Loss: 0.6806 | Speed: 99.0 batches/s | ETA: 00:16\n",
      "  Train: [56000/57525] ( 97.3%) | Loss: 0.6805 | Speed: 99.0 batches/s | ETA: 00:15\n",
      "  Train: [56100/57525] ( 97.5%) | Loss: 0.6805 | Speed: 99.0 batches/s | ETA: 00:14\n",
      "  Train: [56200/57525] ( 97.7%) | Loss: 0.6804 | Speed: 99.0 batches/s | ETA: 00:13\n",
      "  Train: [56300/57525] ( 97.9%) | Loss: 0.6804 | Speed: 99.0 batches/s | ETA: 00:12\n",
      "  Train: [56400/57525] ( 98.0%) | Loss: 0.6804 | Speed: 99.0 batches/s | ETA: 00:11\n",
      "  Train: [56500/57525] ( 98.2%) | Loss: 0.6804 | Speed: 99.0 batches/s | ETA: 00:10\n",
      "  Train: [56600/57525] ( 98.4%) | Loss: 0.6804 | Speed: 99.0 batches/s | ETA: 00:09\n",
      "  Train: [56700/57525] ( 98.6%) | Loss: 0.6804 | Speed: 99.0 batches/s | ETA: 00:08\n",
      "  Train: [56800/57525] ( 98.7%) | Loss: 0.6803 | Speed: 98.9 batches/s | ETA: 00:07\n",
      "  Train: [56900/57525] ( 98.9%) | Loss: 0.6803 | Speed: 99.0 batches/s | ETA: 00:06\n",
      "  Train: [57000/57525] ( 99.1%) | Loss: 0.6803 | Speed: 99.0 batches/s | ETA: 00:05\n",
      "  Train: [57100/57525] ( 99.3%) | Loss: 0.6803 | Speed: 99.0 batches/s | ETA: 00:04\n",
      "  Train: [57200/57525] ( 99.4%) | Loss: 0.6803 | Speed: 99.0 batches/s | ETA: 00:03\n",
      "  Train: [57300/57525] ( 99.6%) | Loss: 0.6802 | Speed: 99.0 batches/s | ETA: 00:02\n",
      "  Train: [57400/57525] ( 99.8%) | Loss: 0.6802 | Speed: 99.0 batches/s | ETA: 00:01\n",
      "  Train: [57500/57525] (100.0%) | Loss: 0.6802 | Speed: 99.0 batches/s | ETA: 00:00\n",
      "  Running validation...\n",
      "    Val: [  50/7191] (  0.7%)\n",
      "    Val: [ 100/7191] (  1.4%)\n",
      "    Val: [ 150/7191] (  2.1%)\n",
      "    Val: [ 200/7191] (  2.8%)\n",
      "    Val: [ 250/7191] (  3.5%)\n",
      "    Val: [ 300/7191] (  4.2%)\n",
      "    Val: [ 350/7191] (  4.9%)\n",
      "    Val: [ 400/7191] (  5.6%)\n",
      "    Val: [ 450/7191] (  6.3%)\n",
      "    Val: [ 500/7191] (  7.0%)\n",
      "    Val: [ 550/7191] (  7.6%)\n",
      "    Val: [ 600/7191] (  8.3%)\n",
      "    Val: [ 650/7191] (  9.0%)\n",
      "    Val: [ 700/7191] (  9.7%)\n",
      "    Val: [ 750/7191] ( 10.4%)\n",
      "    Val: [ 800/7191] ( 11.1%)\n",
      "    Val: [ 850/7191] ( 11.8%)\n",
      "    Val: [ 900/7191] ( 12.5%)\n",
      "    Val: [ 950/7191] ( 13.2%)\n",
      "    Val: [1000/7191] ( 13.9%)\n",
      "    Val: [1050/7191] ( 14.6%)\n",
      "    Val: [1100/7191] ( 15.3%)\n",
      "    Val: [1150/7191] ( 16.0%)\n",
      "    Val: [1200/7191] ( 16.7%)\n",
      "    Val: [1250/7191] ( 17.4%)\n",
      "    Val: [1300/7191] ( 18.1%)\n",
      "    Val: [1350/7191] ( 18.8%)\n",
      "    Val: [1400/7191] ( 19.5%)\n",
      "    Val: [1450/7191] ( 20.2%)\n",
      "    Val: [1500/7191] ( 20.9%)\n",
      "    Val: [1550/7191] ( 21.6%)\n",
      "    Val: [1600/7191] ( 22.3%)\n",
      "    Val: [1650/7191] ( 22.9%)\n",
      "    Val: [1700/7191] ( 23.6%)\n",
      "    Val: [1750/7191] ( 24.3%)\n",
      "    Val: [1800/7191] ( 25.0%)\n",
      "    Val: [1850/7191] ( 25.7%)\n",
      "    Val: [1900/7191] ( 26.4%)\n",
      "    Val: [1950/7191] ( 27.1%)\n",
      "    Val: [2000/7191] ( 27.8%)\n",
      "    Val: [2050/7191] ( 28.5%)\n",
      "    Val: [2100/7191] ( 29.2%)\n",
      "    Val: [2150/7191] ( 29.9%)\n",
      "    Val: [2200/7191] ( 30.6%)\n",
      "    Val: [2250/7191] ( 31.3%)\n",
      "    Val: [2300/7191] ( 32.0%)\n",
      "    Val: [2350/7191] ( 32.7%)\n",
      "    Val: [2400/7191] ( 33.4%)\n",
      "    Val: [2450/7191] ( 34.1%)\n",
      "    Val: [2500/7191] ( 34.8%)\n",
      "    Val: [2550/7191] ( 35.5%)\n",
      "    Val: [2600/7191] ( 36.2%)\n",
      "    Val: [2650/7191] ( 36.9%)\n",
      "    Val: [2700/7191] ( 37.5%)\n",
      "    Val: [2750/7191] ( 38.2%)\n",
      "    Val: [2800/7191] ( 38.9%)\n",
      "    Val: [2850/7191] ( 39.6%)\n",
      "    Val: [2900/7191] ( 40.3%)\n",
      "    Val: [2950/7191] ( 41.0%)\n",
      "    Val: [3000/7191] ( 41.7%)\n",
      "    Val: [3050/7191] ( 42.4%)\n",
      "    Val: [3100/7191] ( 43.1%)\n",
      "    Val: [3150/7191] ( 43.8%)\n",
      "    Val: [3200/7191] ( 44.5%)\n",
      "    Val: [3250/7191] ( 45.2%)\n",
      "    Val: [3300/7191] ( 45.9%)\n",
      "    Val: [3350/7191] ( 46.6%)\n",
      "    Val: [3400/7191] ( 47.3%)\n",
      "    Val: [3450/7191] ( 48.0%)\n",
      "    Val: [3500/7191] ( 48.7%)\n",
      "    Val: [3550/7191] ( 49.4%)\n",
      "    Val: [3600/7191] ( 50.1%)\n",
      "    Val: [3650/7191] ( 50.8%)\n",
      "    Val: [3700/7191] ( 51.5%)\n",
      "    Val: [3750/7191] ( 52.1%)\n",
      "    Val: [3800/7191] ( 52.8%)\n",
      "    Val: [3850/7191] ( 53.5%)\n",
      "    Val: [3900/7191] ( 54.2%)\n",
      "    Val: [3950/7191] ( 54.9%)\n",
      "    Val: [4000/7191] ( 55.6%)\n",
      "    Val: [4050/7191] ( 56.3%)\n",
      "    Val: [4100/7191] ( 57.0%)\n",
      "    Val: [4150/7191] ( 57.7%)\n",
      "    Val: [4200/7191] ( 58.4%)\n",
      "    Val: [4250/7191] ( 59.1%)\n",
      "    Val: [4300/7191] ( 59.8%)\n",
      "    Val: [4350/7191] ( 60.5%)\n",
      "    Val: [4400/7191] ( 61.2%)\n",
      "    Val: [4450/7191] ( 61.9%)\n",
      "    Val: [4500/7191] ( 62.6%)\n",
      "    Val: [4550/7191] ( 63.3%)\n",
      "    Val: [4600/7191] ( 64.0%)\n",
      "    Val: [4650/7191] ( 64.7%)\n",
      "    Val: [4700/7191] ( 65.4%)\n",
      "    Val: [4750/7191] ( 66.1%)\n",
      "    Val: [4800/7191] ( 66.8%)\n",
      "    Val: [4850/7191] ( 67.4%)\n",
      "    Val: [4900/7191] ( 68.1%)\n",
      "    Val: [4950/7191] ( 68.8%)\n",
      "    Val: [5000/7191] ( 69.5%)\n",
      "    Val: [5050/7191] ( 70.2%)\n",
      "    Val: [5100/7191] ( 70.9%)\n",
      "    Val: [5150/7191] ( 71.6%)\n",
      "    Val: [5200/7191] ( 72.3%)\n",
      "    Val: [5250/7191] ( 73.0%)\n",
      "    Val: [5300/7191] ( 73.7%)\n",
      "    Val: [5350/7191] ( 74.4%)\n",
      "    Val: [5400/7191] ( 75.1%)\n",
      "    Val: [5450/7191] ( 75.8%)\n",
      "    Val: [5500/7191] ( 76.5%)\n",
      "    Val: [5550/7191] ( 77.2%)\n",
      "    Val: [5600/7191] ( 77.9%)\n",
      "    Val: [5650/7191] ( 78.6%)\n",
      "    Val: [5700/7191] ( 79.3%)\n",
      "    Val: [5750/7191] ( 80.0%)\n",
      "    Val: [5800/7191] ( 80.7%)\n",
      "    Val: [5850/7191] ( 81.4%)\n",
      "    Val: [5900/7191] ( 82.0%)\n",
      "    Val: [5950/7191] ( 82.7%)\n",
      "    Val: [6000/7191] ( 83.4%)\n",
      "    Val: [6050/7191] ( 84.1%)\n",
      "    Val: [6100/7191] ( 84.8%)\n",
      "    Val: [6150/7191] ( 85.5%)\n",
      "    Val: [6200/7191] ( 86.2%)\n",
      "    Val: [6250/7191] ( 86.9%)\n",
      "    Val: [6300/7191] ( 87.6%)\n",
      "    Val: [6350/7191] ( 88.3%)\n",
      "    Val: [6400/7191] ( 89.0%)\n",
      "    Val: [6450/7191] ( 89.7%)\n",
      "    Val: [6500/7191] ( 90.4%)\n",
      "    Val: [6550/7191] ( 91.1%)\n",
      "    Val: [6600/7191] ( 91.8%)\n",
      "    Val: [6650/7191] ( 92.5%)\n",
      "    Val: [6700/7191] ( 93.2%)\n",
      "    Val: [6750/7191] ( 93.9%)\n",
      "    Val: [6800/7191] ( 94.6%)\n",
      "    Val: [6850/7191] ( 95.3%)\n",
      "    Val: [6900/7191] ( 96.0%)\n",
      "    Val: [6950/7191] ( 96.6%)\n",
      "    Val: [7000/7191] ( 97.3%)\n",
      "    Val: [7050/7191] ( 98.0%)\n",
      "    Val: [7100/7191] ( 98.7%)\n",
      "    Val: [7150/7191] ( 99.4%)\n",
      "\n",
      "  Epoch 02 Summary:\n",
      "    Train MSE (norm): 0.6802 | Val MSE (norm): 0.6357\n",
      "    Time: Train=9.7min, Val=0.3min, Total=10.0min\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Epoch 03/3\n",
      "============================================================\n",
      "  Train: [  100/57525] (  0.2%) | Loss: 0.6425 | Speed: 49.3 batches/s | ETA: 19:24\n",
      "  Train: [  200/57525] (  0.3%) | Loss: 0.6496 | Speed: 65.0 batches/s | ETA: 14:42\n",
      "  Train: [  300/57525] (  0.5%) | Loss: 0.6518 | Speed: 73.1 batches/s | ETA: 13:02\n",
      "  Train: [  400/57525] (  0.7%) | Loss: 0.6535 | Speed: 78.6 batches/s | ETA: 12:06\n",
      "  Train: [  500/57525] (  0.9%) | Loss: 0.6531 | Speed: 82.3 batches/s | ETA: 11:32\n",
      "  Train: [  600/57525] (  1.0%) | Loss: 0.6567 | Speed: 85.0 batches/s | ETA: 11:10\n",
      "  Train: [  700/57525] (  1.2%) | Loss: 0.6583 | Speed: 86.9 batches/s | ETA: 10:53\n",
      "  Train: [  800/57525] (  1.4%) | Loss: 0.6597 | Speed: 88.6 batches/s | ETA: 10:40\n",
      "  Train: [  900/57525] (  1.6%) | Loss: 0.6590 | Speed: 89.6 batches/s | ETA: 10:31\n",
      "  Train: [ 1000/57525] (  1.7%) | Loss: 0.6584 | Speed: 90.2 batches/s | ETA: 10:26\n",
      "  Train: [ 1100/57525] (  1.9%) | Loss: 0.6570 | Speed: 90.7 batches/s | ETA: 10:22\n",
      "  Train: [ 1200/57525] (  2.1%) | Loss: 0.6561 | Speed: 91.0 batches/s | ETA: 10:18\n",
      "  Train: [ 1300/57525] (  2.3%) | Loss: 0.6574 | Speed: 91.3 batches/s | ETA: 10:15\n",
      "  Train: [ 1400/57525] (  2.4%) | Loss: 0.6578 | Speed: 91.6 batches/s | ETA: 10:12\n",
      "  Train: [ 1500/57525] (  2.6%) | Loss: 0.6570 | Speed: 92.0 batches/s | ETA: 10:09\n",
      "  Train: [ 1600/57525] (  2.8%) | Loss: 0.6576 | Speed: 92.3 batches/s | ETA: 10:05\n",
      "  Train: [ 1700/57525] (  3.0%) | Loss: 0.6585 | Speed: 92.6 batches/s | ETA: 10:02\n",
      "  Train: [ 1800/57525] (  3.1%) | Loss: 0.6593 | Speed: 92.8 batches/s | ETA: 10:00\n",
      "  Train: [ 1900/57525] (  3.3%) | Loss: 0.6603 | Speed: 93.0 batches/s | ETA: 09:58\n",
      "  Train: [ 2000/57525] (  3.5%) | Loss: 0.6600 | Speed: 93.2 batches/s | ETA: 09:55\n",
      "  Train: [ 2100/57525] (  3.7%) | Loss: 0.6595 | Speed: 93.4 batches/s | ETA: 09:53\n",
      "  Train: [ 2200/57525] (  3.8%) | Loss: 0.6598 | Speed: 93.5 batches/s | ETA: 09:52\n",
      "  Train: [ 2300/57525] (  4.0%) | Loss: 0.6599 | Speed: 93.5 batches/s | ETA: 09:50\n",
      "  Train: [ 2400/57525] (  4.2%) | Loss: 0.6598 | Speed: 93.5 batches/s | ETA: 09:49\n",
      "  Train: [ 2500/57525] (  4.3%) | Loss: 0.6598 | Speed: 93.5 batches/s | ETA: 09:48\n",
      "  Train: [ 2600/57525] (  4.5%) | Loss: 0.6600 | Speed: 93.7 batches/s | ETA: 09:46\n",
      "  Train: [ 2700/57525] (  4.7%) | Loss: 0.6606 | Speed: 93.7 batches/s | ETA: 09:44\n",
      "  Train: [ 2800/57525] (  4.9%) | Loss: 0.6607 | Speed: 94.0 batches/s | ETA: 09:42\n",
      "  Train: [ 2900/57525] (  5.0%) | Loss: 0.6605 | Speed: 94.3 batches/s | ETA: 09:39\n",
      "  Train: [ 3000/57525] (  5.2%) | Loss: 0.6600 | Speed: 94.6 batches/s | ETA: 09:36\n",
      "  Train: [ 3100/57525] (  5.4%) | Loss: 0.6599 | Speed: 94.8 batches/s | ETA: 09:34\n",
      "  Train: [ 3200/57525] (  5.6%) | Loss: 0.6603 | Speed: 95.0 batches/s | ETA: 09:32\n",
      "  Train: [ 3300/57525] (  5.7%) | Loss: 0.6605 | Speed: 95.2 batches/s | ETA: 09:29\n",
      "  Train: [ 3400/57525] (  5.9%) | Loss: 0.6605 | Speed: 95.3 batches/s | ETA: 09:27\n",
      "  Train: [ 3500/57525] (  6.1%) | Loss: 0.6602 | Speed: 95.4 batches/s | ETA: 09:26\n",
      "  Train: [ 3600/57525] (  6.3%) | Loss: 0.6599 | Speed: 95.5 batches/s | ETA: 09:24\n",
      "  Train: [ 3700/57525] (  6.4%) | Loss: 0.6600 | Speed: 95.6 batches/s | ETA: 09:23\n",
      "  Train: [ 3800/57525] (  6.6%) | Loss: 0.6598 | Speed: 95.7 batches/s | ETA: 09:21\n",
      "  Train: [ 3900/57525] (  6.8%) | Loss: 0.6599 | Speed: 95.8 batches/s | ETA: 09:19\n",
      "  Train: [ 4000/57525] (  7.0%) | Loss: 0.6603 | Speed: 96.0 batches/s | ETA: 09:17\n",
      "  Train: [ 4100/57525] (  7.1%) | Loss: 0.6601 | Speed: 96.2 batches/s | ETA: 09:15\n",
      "  Train: [ 4200/57525] (  7.3%) | Loss: 0.6609 | Speed: 96.2 batches/s | ETA: 09:14\n",
      "  Train: [ 4300/57525] (  7.5%) | Loss: 0.6610 | Speed: 96.3 batches/s | ETA: 09:12\n",
      "  Train: [ 4400/57525] (  7.6%) | Loss: 0.6614 | Speed: 96.3 batches/s | ETA: 09:11\n",
      "  Train: [ 4500/57525] (  7.8%) | Loss: 0.6613 | Speed: 96.4 batches/s | ETA: 09:10\n",
      "  Train: [ 4600/57525] (  8.0%) | Loss: 0.6613 | Speed: 96.3 batches/s | ETA: 09:09\n",
      "  Train: [ 4700/57525] (  8.2%) | Loss: 0.6613 | Speed: 96.3 batches/s | ETA: 09:08\n",
      "  Train: [ 4800/57525] (  8.3%) | Loss: 0.6610 | Speed: 96.3 batches/s | ETA: 09:07\n",
      "  Train: [ 4900/57525] (  8.5%) | Loss: 0.6610 | Speed: 96.2 batches/s | ETA: 09:06\n",
      "  Train: [ 5000/57525] (  8.7%) | Loss: 0.6611 | Speed: 96.2 batches/s | ETA: 09:05\n",
      "  Train: [ 5100/57525] (  8.9%) | Loss: 0.6613 | Speed: 96.2 batches/s | ETA: 09:05\n",
      "  Train: [ 5200/57525] (  9.0%) | Loss: 0.6617 | Speed: 96.2 batches/s | ETA: 09:04\n",
      "  Train: [ 5300/57525] (  9.2%) | Loss: 0.6615 | Speed: 96.2 batches/s | ETA: 09:02\n",
      "  Train: [ 5400/57525] (  9.4%) | Loss: 0.6615 | Speed: 96.3 batches/s | ETA: 09:01\n",
      "  Train: [ 5500/57525] (  9.6%) | Loss: 0.6617 | Speed: 96.3 batches/s | ETA: 08:59\n",
      "  Train: [ 5600/57525] (  9.7%) | Loss: 0.6616 | Speed: 96.4 batches/s | ETA: 08:58\n",
      "  Train: [ 5700/57525] (  9.9%) | Loss: 0.6613 | Speed: 96.5 batches/s | ETA: 08:57\n",
      "  Train: [ 5800/57525] ( 10.1%) | Loss: 0.6616 | Speed: 96.5 batches/s | ETA: 08:55\n",
      "  Train: [ 5900/57525] ( 10.3%) | Loss: 0.6614 | Speed: 96.5 batches/s | ETA: 08:54\n",
      "  Train: [ 6000/57525] ( 10.4%) | Loss: 0.6613 | Speed: 96.6 batches/s | ETA: 08:53\n",
      "  Train: [ 6100/57525] ( 10.6%) | Loss: 0.6609 | Speed: 96.6 batches/s | ETA: 08:52\n",
      "  Train: [ 6200/57525] ( 10.8%) | Loss: 0.6610 | Speed: 96.6 batches/s | ETA: 08:51\n",
      "  Train: [ 6300/57525] ( 11.0%) | Loss: 0.6608 | Speed: 96.6 batches/s | ETA: 08:50\n",
      "  Train: [ 6400/57525] ( 11.1%) | Loss: 0.6607 | Speed: 96.7 batches/s | ETA: 08:48\n",
      "  Train: [ 6500/57525] ( 11.3%) | Loss: 0.6608 | Speed: 96.8 batches/s | ETA: 08:47\n",
      "  Train: [ 6600/57525] ( 11.5%) | Loss: 0.6608 | Speed: 96.8 batches/s | ETA: 08:45\n",
      "  Train: [ 6700/57525] ( 11.6%) | Loss: 0.6607 | Speed: 96.9 batches/s | ETA: 08:44\n",
      "  Train: [ 6800/57525] ( 11.8%) | Loss: 0.6606 | Speed: 97.0 batches/s | ETA: 08:43\n",
      "  Train: [ 6900/57525] ( 12.0%) | Loss: 0.6606 | Speed: 97.0 batches/s | ETA: 08:41\n",
      "  Train: [ 7000/57525] ( 12.2%) | Loss: 0.6607 | Speed: 97.1 batches/s | ETA: 08:40\n",
      "  Train: [ 7100/57525] ( 12.3%) | Loss: 0.6607 | Speed: 97.0 batches/s | ETA: 08:39\n",
      "  Train: [ 7200/57525] ( 12.5%) | Loss: 0.6606 | Speed: 97.0 batches/s | ETA: 08:38\n",
      "  Train: [ 7300/57525] ( 12.7%) | Loss: 0.6607 | Speed: 96.9 batches/s | ETA: 08:38\n",
      "  Train: [ 7400/57525] ( 12.9%) | Loss: 0.6606 | Speed: 96.9 batches/s | ETA: 08:37\n",
      "  Train: [ 7500/57525] ( 13.0%) | Loss: 0.6605 | Speed: 96.9 batches/s | ETA: 08:36\n",
      "  Train: [ 7600/57525] ( 13.2%) | Loss: 0.6605 | Speed: 96.9 batches/s | ETA: 08:35\n",
      "  Train: [ 7700/57525] ( 13.4%) | Loss: 0.6603 | Speed: 96.9 batches/s | ETA: 08:34\n",
      "  Train: [ 7800/57525] ( 13.6%) | Loss: 0.6604 | Speed: 96.9 batches/s | ETA: 08:33\n",
      "  Train: [ 7900/57525] ( 13.7%) | Loss: 0.6604 | Speed: 96.9 batches/s | ETA: 08:31\n",
      "  Train: [ 8000/57525] ( 13.9%) | Loss: 0.6607 | Speed: 96.9 batches/s | ETA: 08:30\n",
      "  Train: [ 8100/57525] ( 14.1%) | Loss: 0.6610 | Speed: 97.0 batches/s | ETA: 08:29\n",
      "  Train: [ 8200/57525] ( 14.3%) | Loss: 0.6612 | Speed: 97.0 batches/s | ETA: 08:28\n",
      "  Train: [ 8300/57525] ( 14.4%) | Loss: 0.6613 | Speed: 96.9 batches/s | ETA: 08:27\n",
      "  Train: [ 8400/57525] ( 14.6%) | Loss: 0.6615 | Speed: 96.9 batches/s | ETA: 08:26\n",
      "  Train: [ 8500/57525] ( 14.8%) | Loss: 0.6613 | Speed: 96.9 batches/s | ETA: 08:26\n",
      "  Train: [ 8600/57525] ( 15.0%) | Loss: 0.6613 | Speed: 96.8 batches/s | ETA: 08:25\n",
      "  Train: [ 8700/57525] ( 15.1%) | Loss: 0.6612 | Speed: 96.8 batches/s | ETA: 08:24\n",
      "  Train: [ 8800/57525] ( 15.3%) | Loss: 0.6611 | Speed: 96.8 batches/s | ETA: 08:23\n",
      "  Train: [ 8900/57525] ( 15.5%) | Loss: 0.6611 | Speed: 96.9 batches/s | ETA: 08:21\n",
      "  Train: [ 9000/57525] ( 15.6%) | Loss: 0.6609 | Speed: 96.9 batches/s | ETA: 08:20\n",
      "  Train: [ 9100/57525] ( 15.8%) | Loss: 0.6609 | Speed: 97.0 batches/s | ETA: 08:19\n",
      "  Train: [ 9200/57525] ( 16.0%) | Loss: 0.6610 | Speed: 97.0 batches/s | ETA: 08:18\n",
      "  Train: [ 9300/57525] ( 16.2%) | Loss: 0.6609 | Speed: 97.1 batches/s | ETA: 08:16\n",
      "  Train: [ 9400/57525] ( 16.3%) | Loss: 0.6611 | Speed: 97.2 batches/s | ETA: 08:15\n",
      "  Train: [ 9500/57525] ( 16.5%) | Loss: 0.6614 | Speed: 97.2 batches/s | ETA: 08:14\n",
      "  Train: [ 9600/57525] ( 16.7%) | Loss: 0.6612 | Speed: 97.2 batches/s | ETA: 08:13\n",
      "  Train: [ 9700/57525] ( 16.9%) | Loss: 0.6613 | Speed: 97.1 batches/s | ETA: 08:12\n",
      "  Train: [ 9800/57525] ( 17.0%) | Loss: 0.6612 | Speed: 97.1 batches/s | ETA: 08:11\n",
      "  Train: [ 9900/57525] ( 17.2%) | Loss: 0.6613 | Speed: 97.2 batches/s | ETA: 08:10\n",
      "  Train: [10000/57525] ( 17.4%) | Loss: 0.6614 | Speed: 97.2 batches/s | ETA: 08:09\n",
      "  Train: [10100/57525] ( 17.6%) | Loss: 0.6612 | Speed: 97.2 batches/s | ETA: 08:07\n",
      "  Train: [10200/57525] ( 17.7%) | Loss: 0.6613 | Speed: 97.2 batches/s | ETA: 08:06\n",
      "  Train: [10300/57525] ( 17.9%) | Loss: 0.6612 | Speed: 97.2 batches/s | ETA: 08:05\n",
      "  Train: [10400/57525] ( 18.1%) | Loss: 0.6611 | Speed: 97.3 batches/s | ETA: 08:04\n",
      "  Train: [10500/57525] ( 18.3%) | Loss: 0.6610 | Speed: 97.3 batches/s | ETA: 08:03\n",
      "  Train: [10600/57525] ( 18.4%) | Loss: 0.6609 | Speed: 97.4 batches/s | ETA: 08:01\n",
      "  Train: [10700/57525] ( 18.6%) | Loss: 0.6611 | Speed: 97.4 batches/s | ETA: 08:00\n",
      "  Train: [10800/57525] ( 18.8%) | Loss: 0.6610 | Speed: 97.3 batches/s | ETA: 07:59\n",
      "  Train: [10900/57525] ( 18.9%) | Loss: 0.6609 | Speed: 97.3 batches/s | ETA: 07:59\n",
      "  Train: [11000/57525] ( 19.1%) | Loss: 0.6609 | Speed: 97.3 batches/s | ETA: 07:58\n",
      "  Train: [11100/57525] ( 19.3%) | Loss: 0.6609 | Speed: 97.3 batches/s | ETA: 07:57\n",
      "  Train: [11200/57525] ( 19.5%) | Loss: 0.6610 | Speed: 97.3 batches/s | ETA: 07:56\n",
      "  Train: [11300/57525] ( 19.6%) | Loss: 0.6610 | Speed: 97.3 batches/s | ETA: 07:55\n",
      "  Train: [11400/57525] ( 19.8%) | Loss: 0.6611 | Speed: 97.3 batches/s | ETA: 07:54\n",
      "  Train: [11500/57525] ( 20.0%) | Loss: 0.6610 | Speed: 97.2 batches/s | ETA: 07:53\n",
      "  Train: [11600/57525] ( 20.2%) | Loss: 0.6612 | Speed: 97.3 batches/s | ETA: 07:52\n",
      "  Train: [11700/57525] ( 20.3%) | Loss: 0.6612 | Speed: 97.3 batches/s | ETA: 07:51\n",
      "  Train: [11800/57525] ( 20.5%) | Loss: 0.6612 | Speed: 97.3 batches/s | ETA: 07:50\n",
      "  Train: [11900/57525] ( 20.7%) | Loss: 0.6612 | Speed: 97.3 batches/s | ETA: 07:49\n",
      "  Train: [12000/57525] ( 20.9%) | Loss: 0.6612 | Speed: 97.2 batches/s | ETA: 07:48\n",
      "  Train: [12100/57525] ( 21.0%) | Loss: 0.6610 | Speed: 97.2 batches/s | ETA: 07:47\n",
      "  Train: [12200/57525] ( 21.2%) | Loss: 0.6611 | Speed: 97.2 batches/s | ETA: 07:46\n",
      "  Train: [12300/57525] ( 21.4%) | Loss: 0.6611 | Speed: 97.2 batches/s | ETA: 07:45\n",
      "  Train: [12400/57525] ( 21.6%) | Loss: 0.6612 | Speed: 97.2 batches/s | ETA: 07:44\n",
      "  Train: [12500/57525] ( 21.7%) | Loss: 0.6613 | Speed: 97.2 batches/s | ETA: 07:43\n",
      "  Train: [12600/57525] ( 21.9%) | Loss: 0.6613 | Speed: 97.2 batches/s | ETA: 07:42\n",
      "  Train: [12700/57525] ( 22.1%) | Loss: 0.6613 | Speed: 97.2 batches/s | ETA: 07:41\n",
      "  Train: [12800/57525] ( 22.3%) | Loss: 0.6612 | Speed: 97.3 batches/s | ETA: 07:39\n",
      "  Train: [12900/57525] ( 22.4%) | Loss: 0.6611 | Speed: 97.3 batches/s | ETA: 07:38\n",
      "  Train: [13000/57525] ( 22.6%) | Loss: 0.6612 | Speed: 97.3 batches/s | ETA: 07:37\n",
      "  Train: [13100/57525] ( 22.8%) | Loss: 0.6612 | Speed: 97.3 batches/s | ETA: 07:36\n",
      "  Train: [13200/57525] ( 22.9%) | Loss: 0.6611 | Speed: 97.3 batches/s | ETA: 07:35\n",
      "  Train: [13300/57525] ( 23.1%) | Loss: 0.6611 | Speed: 97.3 batches/s | ETA: 07:34\n",
      "  Train: [13400/57525] ( 23.3%) | Loss: 0.6611 | Speed: 97.4 batches/s | ETA: 07:33\n",
      "  Train: [13500/57525] ( 23.5%) | Loss: 0.6612 | Speed: 97.3 batches/s | ETA: 07:32\n",
      "  Train: [13600/57525] ( 23.6%) | Loss: 0.6613 | Speed: 97.3 batches/s | ETA: 07:31\n",
      "  Train: [13700/57525] ( 23.8%) | Loss: 0.6613 | Speed: 97.3 batches/s | ETA: 07:30\n",
      "  Train: [13800/57525] ( 24.0%) | Loss: 0.6612 | Speed: 97.3 batches/s | ETA: 07:29\n",
      "  Train: [13900/57525] ( 24.2%) | Loss: 0.6613 | Speed: 97.3 batches/s | ETA: 07:28\n",
      "  Train: [14000/57525] ( 24.3%) | Loss: 0.6612 | Speed: 97.3 batches/s | ETA: 07:27\n",
      "  Train: [14100/57525] ( 24.5%) | Loss: 0.6611 | Speed: 97.3 batches/s | ETA: 07:26\n",
      "  Train: [14200/57525] ( 24.7%) | Loss: 0.6612 | Speed: 97.3 batches/s | ETA: 07:25\n",
      "  Train: [14300/57525] ( 24.9%) | Loss: 0.6611 | Speed: 97.3 batches/s | ETA: 07:24\n",
      "  Train: [14400/57525] ( 25.0%) | Loss: 0.6611 | Speed: 97.3 batches/s | ETA: 07:23\n",
      "  Train: [14500/57525] ( 25.2%) | Loss: 0.6611 | Speed: 97.2 batches/s | ETA: 07:22\n",
      "  Train: [14600/57525] ( 25.4%) | Loss: 0.6612 | Speed: 97.2 batches/s | ETA: 07:21\n",
      "  Train: [14700/57525] ( 25.6%) | Loss: 0.6612 | Speed: 97.2 batches/s | ETA: 07:20\n",
      "  Train: [14800/57525] ( 25.7%) | Loss: 0.6612 | Speed: 97.2 batches/s | ETA: 07:19\n",
      "  Train: [14900/57525] ( 25.9%) | Loss: 0.6611 | Speed: 97.1 batches/s | ETA: 07:18\n",
      "  Train: [15000/57525] ( 26.1%) | Loss: 0.6610 | Speed: 97.2 batches/s | ETA: 07:17\n",
      "  Train: [15100/57525] ( 26.2%) | Loss: 0.6609 | Speed: 97.2 batches/s | ETA: 07:16\n",
      "  Train: [15200/57525] ( 26.4%) | Loss: 0.6608 | Speed: 97.2 batches/s | ETA: 07:15\n",
      "  Train: [15300/57525] ( 26.6%) | Loss: 0.6608 | Speed: 97.2 batches/s | ETA: 07:14\n",
      "  Train: [15400/57525] ( 26.8%) | Loss: 0.6608 | Speed: 97.3 batches/s | ETA: 07:13\n",
      "  Train: [15500/57525] ( 26.9%) | Loss: 0.6608 | Speed: 97.3 batches/s | ETA: 07:11\n",
      "  Train: [15600/57525] ( 27.1%) | Loss: 0.6609 | Speed: 97.3 batches/s | ETA: 07:10\n",
      "  Train: [15700/57525] ( 27.3%) | Loss: 0.6608 | Speed: 97.3 batches/s | ETA: 07:09\n",
      "  Train: [15800/57525] ( 27.5%) | Loss: 0.6611 | Speed: 97.3 batches/s | ETA: 07:08\n",
      "  Train: [15900/57525] ( 27.6%) | Loss: 0.6611 | Speed: 97.4 batches/s | ETA: 07:07\n",
      "  Train: [16000/57525] ( 27.8%) | Loss: 0.6612 | Speed: 97.4 batches/s | ETA: 07:06\n",
      "  Train: [16100/57525] ( 28.0%) | Loss: 0.6612 | Speed: 97.4 batches/s | ETA: 07:05\n",
      "  Train: [16200/57525] ( 28.2%) | Loss: 0.6611 | Speed: 97.4 batches/s | ETA: 07:04\n",
      "  Train: [16300/57525] ( 28.3%) | Loss: 0.6611 | Speed: 97.4 batches/s | ETA: 07:03\n",
      "  Train: [16400/57525] ( 28.5%) | Loss: 0.6610 | Speed: 97.5 batches/s | ETA: 07:01\n",
      "  Train: [16500/57525] ( 28.7%) | Loss: 0.6609 | Speed: 97.5 batches/s | ETA: 07:00\n",
      "  Train: [16600/57525] ( 28.9%) | Loss: 0.6609 | Speed: 97.5 batches/s | ETA: 06:59\n",
      "  Train: [16700/57525] ( 29.0%) | Loss: 0.6610 | Speed: 97.5 batches/s | ETA: 06:58\n",
      "  Train: [16800/57525] ( 29.2%) | Loss: 0.6610 | Speed: 97.5 batches/s | ETA: 06:57\n",
      "  Train: [16900/57525] ( 29.4%) | Loss: 0.6610 | Speed: 97.5 batches/s | ETA: 06:56\n",
      "  Train: [17000/57525] ( 29.6%) | Loss: 0.6609 | Speed: 97.5 batches/s | ETA: 06:55\n",
      "  Train: [17100/57525] ( 29.7%) | Loss: 0.6608 | Speed: 97.5 batches/s | ETA: 06:54\n",
      "  Train: [17200/57525] ( 29.9%) | Loss: 0.6608 | Speed: 97.5 batches/s | ETA: 06:53\n",
      "  Train: [17300/57525] ( 30.1%) | Loss: 0.6608 | Speed: 97.5 batches/s | ETA: 06:52\n",
      "  Train: [17400/57525] ( 30.2%) | Loss: 0.6608 | Speed: 97.5 batches/s | ETA: 06:51\n",
      "  Train: [17500/57525] ( 30.4%) | Loss: 0.6607 | Speed: 97.5 batches/s | ETA: 06:50\n",
      "  Train: [17600/57525] ( 30.6%) | Loss: 0.6607 | Speed: 97.5 batches/s | ETA: 06:49\n",
      "  Train: [17700/57525] ( 30.8%) | Loss: 0.6608 | Speed: 97.5 batches/s | ETA: 06:48\n",
      "  Train: [17800/57525] ( 30.9%) | Loss: 0.6607 | Speed: 97.5 batches/s | ETA: 06:47\n",
      "  Train: [17900/57525] ( 31.1%) | Loss: 0.6606 | Speed: 97.5 batches/s | ETA: 06:46\n",
      "  Train: [18000/57525] ( 31.3%) | Loss: 0.6605 | Speed: 97.5 batches/s | ETA: 06:45\n",
      "  Train: [18100/57525] ( 31.5%) | Loss: 0.6605 | Speed: 97.5 batches/s | ETA: 06:44\n",
      "  Train: [18200/57525] ( 31.6%) | Loss: 0.6605 | Speed: 97.5 batches/s | ETA: 06:43\n",
      "  Train: [18300/57525] ( 31.8%) | Loss: 0.6605 | Speed: 97.5 batches/s | ETA: 06:42\n",
      "  Train: [18400/57525] ( 32.0%) | Loss: 0.6605 | Speed: 97.5 batches/s | ETA: 06:41\n",
      "  Train: [18500/57525] ( 32.2%) | Loss: 0.6605 | Speed: 97.5 batches/s | ETA: 06:40\n",
      "  Train: [18600/57525] ( 32.3%) | Loss: 0.6606 | Speed: 97.5 batches/s | ETA: 06:39\n",
      "  Train: [18700/57525] ( 32.5%) | Loss: 0.6608 | Speed: 97.6 batches/s | ETA: 06:37\n",
      "  Train: [18800/57525] ( 32.7%) | Loss: 0.6608 | Speed: 97.6 batches/s | ETA: 06:36\n",
      "  Train: [18900/57525] ( 32.9%) | Loss: 0.6608 | Speed: 97.6 batches/s | ETA: 06:35\n",
      "  Train: [19000/57525] ( 33.0%) | Loss: 0.6608 | Speed: 97.6 batches/s | ETA: 06:34\n",
      "  Train: [19100/57525] ( 33.2%) | Loss: 0.6609 | Speed: 97.6 batches/s | ETA: 06:33\n",
      "  Train: [19200/57525] ( 33.4%) | Loss: 0.6609 | Speed: 97.7 batches/s | ETA: 06:32\n",
      "  Train: [19300/57525] ( 33.6%) | Loss: 0.6609 | Speed: 97.7 batches/s | ETA: 06:31\n",
      "  Train: [19400/57525] ( 33.7%) | Loss: 0.6609 | Speed: 97.6 batches/s | ETA: 06:30\n",
      "  Train: [19500/57525] ( 33.9%) | Loss: 0.6609 | Speed: 97.6 batches/s | ETA: 06:29\n",
      "  Train: [19600/57525] ( 34.1%) | Loss: 0.6609 | Speed: 97.6 batches/s | ETA: 06:28\n",
      "  Train: [19700/57525] ( 34.2%) | Loss: 0.6609 | Speed: 97.6 batches/s | ETA: 06:27\n",
      "  Train: [19800/57525] ( 34.4%) | Loss: 0.6610 | Speed: 97.6 batches/s | ETA: 06:26\n",
      "  Train: [19900/57525] ( 34.6%) | Loss: 0.6610 | Speed: 97.6 batches/s | ETA: 06:25\n",
      "  Train: [20000/57525] ( 34.8%) | Loss: 0.6609 | Speed: 97.6 batches/s | ETA: 06:24\n",
      "  Train: [20100/57525] ( 34.9%) | Loss: 0.6609 | Speed: 97.6 batches/s | ETA: 06:23\n",
      "  Train: [20200/57525] ( 35.1%) | Loss: 0.6609 | Speed: 97.6 batches/s | ETA: 06:22\n",
      "  Train: [20300/57525] ( 35.3%) | Loss: 0.6608 | Speed: 97.6 batches/s | ETA: 06:21\n",
      "  Train: [20400/57525] ( 35.5%) | Loss: 0.6608 | Speed: 97.6 batches/s | ETA: 06:20\n",
      "  Train: [20500/57525] ( 35.6%) | Loss: 0.6608 | Speed: 97.6 batches/s | ETA: 06:19\n",
      "  Train: [20600/57525] ( 35.8%) | Loss: 0.6607 | Speed: 97.6 batches/s | ETA: 06:18\n",
      "  Train: [20700/57525] ( 36.0%) | Loss: 0.6607 | Speed: 97.5 batches/s | ETA: 06:17\n",
      "  Train: [20800/57525] ( 36.2%) | Loss: 0.6607 | Speed: 97.5 batches/s | ETA: 06:16\n",
      "  Train: [20900/57525] ( 36.3%) | Loss: 0.6607 | Speed: 97.5 batches/s | ETA: 06:15\n",
      "  Train: [21000/57525] ( 36.5%) | Loss: 0.6607 | Speed: 97.5 batches/s | ETA: 06:14\n",
      "  Train: [21100/57525] ( 36.7%) | Loss: 0.6607 | Speed: 97.5 batches/s | ETA: 06:13\n",
      "  Train: [21200/57525] ( 36.9%) | Loss: 0.6606 | Speed: 97.5 batches/s | ETA: 06:12\n",
      "  Train: [21300/57525] ( 37.0%) | Loss: 0.6606 | Speed: 97.5 batches/s | ETA: 06:11\n",
      "  Train: [21400/57525] ( 37.2%) | Loss: 0.6606 | Speed: 97.6 batches/s | ETA: 06:10\n",
      "  Train: [21500/57525] ( 37.4%) | Loss: 0.6606 | Speed: 97.6 batches/s | ETA: 06:09\n",
      "  Train: [21600/57525] ( 37.5%) | Loss: 0.6605 | Speed: 97.6 batches/s | ETA: 06:08\n",
      "  Train: [21700/57525] ( 37.7%) | Loss: 0.6606 | Speed: 97.6 batches/s | ETA: 06:07\n",
      "  Train: [21800/57525] ( 37.9%) | Loss: 0.6607 | Speed: 97.6 batches/s | ETA: 06:06\n",
      "  Train: [21900/57525] ( 38.1%) | Loss: 0.6606 | Speed: 97.6 batches/s | ETA: 06:05\n",
      "  Train: [22000/57525] ( 38.2%) | Loss: 0.6607 | Speed: 97.6 batches/s | ETA: 06:04\n",
      "  Train: [22100/57525] ( 38.4%) | Loss: 0.6605 | Speed: 97.6 batches/s | ETA: 06:03\n",
      "  Train: [22200/57525] ( 38.6%) | Loss: 0.6605 | Speed: 97.6 batches/s | ETA: 06:01\n",
      "  Train: [22300/57525] ( 38.8%) | Loss: 0.6604 | Speed: 97.6 batches/s | ETA: 06:00\n",
      "  Train: [22400/57525] ( 38.9%) | Loss: 0.6604 | Speed: 97.6 batches/s | ETA: 05:59\n",
      "  Train: [22500/57525] ( 39.1%) | Loss: 0.6604 | Speed: 97.6 batches/s | ETA: 05:58\n",
      "  Train: [22600/57525] ( 39.3%) | Loss: 0.6603 | Speed: 97.7 batches/s | ETA: 05:57\n",
      "  Train: [22700/57525] ( 39.5%) | Loss: 0.6602 | Speed: 97.7 batches/s | ETA: 05:56\n",
      "  Train: [22800/57525] ( 39.6%) | Loss: 0.6602 | Speed: 97.7 batches/s | ETA: 05:55\n",
      "  Train: [22900/57525] ( 39.8%) | Loss: 0.6602 | Speed: 97.8 batches/s | ETA: 05:54\n",
      "  Train: [23000/57525] ( 40.0%) | Loss: 0.6601 | Speed: 97.8 batches/s | ETA: 05:53\n",
      "  Train: [23100/57525] ( 40.2%) | Loss: 0.6601 | Speed: 97.8 batches/s | ETA: 05:51\n",
      "  Train: [23200/57525] ( 40.3%) | Loss: 0.6601 | Speed: 97.9 batches/s | ETA: 05:50\n",
      "  Train: [23300/57525] ( 40.5%) | Loss: 0.6600 | Speed: 97.9 batches/s | ETA: 05:49\n",
      "  Train: [23400/57525] ( 40.7%) | Loss: 0.6600 | Speed: 97.9 batches/s | ETA: 05:48\n",
      "  Train: [23500/57525] ( 40.9%) | Loss: 0.6599 | Speed: 97.9 batches/s | ETA: 05:47\n",
      "  Train: [23600/57525] ( 41.0%) | Loss: 0.6597 | Speed: 98.0 batches/s | ETA: 05:46\n",
      "  Train: [23700/57525] ( 41.2%) | Loss: 0.6598 | Speed: 98.0 batches/s | ETA: 05:45\n",
      "  Train: [23800/57525] ( 41.4%) | Loss: 0.6596 | Speed: 98.0 batches/s | ETA: 05:44\n",
      "  Train: [23900/57525] ( 41.5%) | Loss: 0.6597 | Speed: 98.1 batches/s | ETA: 05:42\n",
      "  Train: [24000/57525] ( 41.7%) | Loss: 0.6597 | Speed: 98.1 batches/s | ETA: 05:41\n",
      "  Train: [24100/57525] ( 41.9%) | Loss: 0.6597 | Speed: 98.1 batches/s | ETA: 05:40\n",
      "  Train: [24200/57525] ( 42.1%) | Loss: 0.6596 | Speed: 98.1 batches/s | ETA: 05:39\n",
      "  Train: [24300/57525] ( 42.2%) | Loss: 0.6596 | Speed: 98.2 batches/s | ETA: 05:38\n",
      "  Train: [24400/57525] ( 42.4%) | Loss: 0.6597 | Speed: 98.2 batches/s | ETA: 05:37\n",
      "  Train: [24500/57525] ( 42.6%) | Loss: 0.6597 | Speed: 98.2 batches/s | ETA: 05:36\n",
      "  Train: [24600/57525] ( 42.8%) | Loss: 0.6597 | Speed: 98.2 batches/s | ETA: 05:35\n",
      "  Train: [24700/57525] ( 42.9%) | Loss: 0.6597 | Speed: 98.3 batches/s | ETA: 05:34\n",
      "  Train: [24800/57525] ( 43.1%) | Loss: 0.6597 | Speed: 98.3 batches/s | ETA: 05:32\n",
      "  Train: [24900/57525] ( 43.3%) | Loss: 0.6597 | Speed: 98.3 batches/s | ETA: 05:31\n",
      "  Train: [25000/57525] ( 43.5%) | Loss: 0.6598 | Speed: 98.3 batches/s | ETA: 05:30\n",
      "  Train: [25100/57525] ( 43.6%) | Loss: 0.6598 | Speed: 98.4 batches/s | ETA: 05:29\n",
      "  Train: [25200/57525] ( 43.8%) | Loss: 0.6598 | Speed: 98.4 batches/s | ETA: 05:28\n",
      "  Train: [25300/57525] ( 44.0%) | Loss: 0.6597 | Speed: 98.4 batches/s | ETA: 05:27\n",
      "  Train: [25400/57525] ( 44.2%) | Loss: 0.6597 | Speed: 98.4 batches/s | ETA: 05:26\n",
      "  Train: [25500/57525] ( 44.3%) | Loss: 0.6597 | Speed: 98.5 batches/s | ETA: 05:25\n",
      "  Train: [25600/57525] ( 44.5%) | Loss: 0.6597 | Speed: 98.5 batches/s | ETA: 05:24\n",
      "  Train: [25700/57525] ( 44.7%) | Loss: 0.6596 | Speed: 98.5 batches/s | ETA: 05:23\n",
      "  Train: [25800/57525] ( 44.9%) | Loss: 0.6598 | Speed: 98.5 batches/s | ETA: 05:21\n",
      "  Train: [25900/57525] ( 45.0%) | Loss: 0.6598 | Speed: 98.6 batches/s | ETA: 05:20\n",
      "  Train: [26000/57525] ( 45.2%) | Loss: 0.6598 | Speed: 98.6 batches/s | ETA: 05:19\n",
      "  Train: [26100/57525] ( 45.4%) | Loss: 0.6597 | Speed: 98.6 batches/s | ETA: 05:18\n",
      "  Train: [26200/57525] ( 45.5%) | Loss: 0.6597 | Speed: 98.6 batches/s | ETA: 05:17\n",
      "  Train: [26300/57525] ( 45.7%) | Loss: 0.6598 | Speed: 98.7 batches/s | ETA: 05:16\n",
      "  Train: [26400/57525] ( 45.9%) | Loss: 0.6598 | Speed: 98.7 batches/s | ETA: 05:15\n",
      "  Train: [26500/57525] ( 46.1%) | Loss: 0.6598 | Speed: 98.7 batches/s | ETA: 05:14\n",
      "  Train: [26600/57525] ( 46.2%) | Loss: 0.6599 | Speed: 98.7 batches/s | ETA: 05:13\n",
      "  Train: [26700/57525] ( 46.4%) | Loss: 0.6599 | Speed: 98.8 batches/s | ETA: 05:12\n",
      "  Train: [26800/57525] ( 46.6%) | Loss: 0.6598 | Speed: 98.8 batches/s | ETA: 05:11\n",
      "  Train: [26900/57525] ( 46.8%) | Loss: 0.6599 | Speed: 98.8 batches/s | ETA: 05:09\n",
      "  Train: [27000/57525] ( 46.9%) | Loss: 0.6599 | Speed: 98.8 batches/s | ETA: 05:08\n",
      "  Train: [27100/57525] ( 47.1%) | Loss: 0.6598 | Speed: 98.8 batches/s | ETA: 05:07\n",
      "  Train: [27200/57525] ( 47.3%) | Loss: 0.6598 | Speed: 98.9 batches/s | ETA: 05:06\n",
      "  Train: [27300/57525] ( 47.5%) | Loss: 0.6598 | Speed: 98.9 batches/s | ETA: 05:05\n",
      "  Train: [27400/57525] ( 47.6%) | Loss: 0.6598 | Speed: 98.9 batches/s | ETA: 05:04\n",
      "  Train: [27500/57525] ( 47.8%) | Loss: 0.6598 | Speed: 98.9 batches/s | ETA: 05:03\n",
      "  Train: [27600/57525] ( 48.0%) | Loss: 0.6597 | Speed: 98.9 batches/s | ETA: 05:02\n",
      "  Train: [27700/57525] ( 48.2%) | Loss: 0.6597 | Speed: 99.0 batches/s | ETA: 05:01\n",
      "  Train: [27800/57525] ( 48.3%) | Loss: 0.6597 | Speed: 99.0 batches/s | ETA: 05:00\n",
      "  Train: [27900/57525] ( 48.5%) | Loss: 0.6597 | Speed: 99.0 batches/s | ETA: 04:59\n",
      "  Train: [28000/57525] ( 48.7%) | Loss: 0.6596 | Speed: 99.0 batches/s | ETA: 04:58\n",
      "  Train: [28100/57525] ( 48.8%) | Loss: 0.6596 | Speed: 99.1 batches/s | ETA: 04:57\n",
      "  Train: [28200/57525] ( 49.0%) | Loss: 0.6596 | Speed: 99.1 batches/s | ETA: 04:55\n",
      "  Train: [28300/57525] ( 49.2%) | Loss: 0.6596 | Speed: 99.1 batches/s | ETA: 04:54\n",
      "  Train: [28400/57525] ( 49.4%) | Loss: 0.6596 | Speed: 99.1 batches/s | ETA: 04:53\n",
      "  Train: [28500/57525] ( 49.5%) | Loss: 0.6596 | Speed: 99.1 batches/s | ETA: 04:52\n",
      "  Train: [28600/57525] ( 49.7%) | Loss: 0.6596 | Speed: 99.2 batches/s | ETA: 04:51\n",
      "  Train: [28700/57525] ( 49.9%) | Loss: 0.6597 | Speed: 99.2 batches/s | ETA: 04:50\n",
      "  Train: [28800/57525] ( 50.1%) | Loss: 0.6596 | Speed: 99.2 batches/s | ETA: 04:49\n",
      "  Train: [28900/57525] ( 50.2%) | Loss: 0.6596 | Speed: 99.2 batches/s | ETA: 04:48\n",
      "  Train: [29000/57525] ( 50.4%) | Loss: 0.6597 | Speed: 99.2 batches/s | ETA: 04:47\n",
      "  Train: [29100/57525] ( 50.6%) | Loss: 0.6596 | Speed: 99.2 batches/s | ETA: 04:46\n",
      "  Train: [29200/57525] ( 50.8%) | Loss: 0.6597 | Speed: 99.3 batches/s | ETA: 04:45\n",
      "  Train: [29300/57525] ( 50.9%) | Loss: 0.6597 | Speed: 99.3 batches/s | ETA: 04:44\n",
      "  Train: [29400/57525] ( 51.1%) | Loss: 0.6597 | Speed: 99.3 batches/s | ETA: 04:43\n",
      "  Train: [29500/57525] ( 51.3%) | Loss: 0.6598 | Speed: 99.3 batches/s | ETA: 04:42\n",
      "  Train: [29600/57525] ( 51.5%) | Loss: 0.6598 | Speed: 99.3 batches/s | ETA: 04:41\n",
      "  Train: [29700/57525] ( 51.6%) | Loss: 0.6598 | Speed: 99.4 batches/s | ETA: 04:40\n",
      "  Train: [29800/57525] ( 51.8%) | Loss: 0.6598 | Speed: 99.4 batches/s | ETA: 04:38\n",
      "  Train: [29900/57525] ( 52.0%) | Loss: 0.6598 | Speed: 99.4 batches/s | ETA: 04:37\n",
      "  Train: [30000/57525] ( 52.2%) | Loss: 0.6598 | Speed: 99.4 batches/s | ETA: 04:36\n",
      "  Train: [30100/57525] ( 52.3%) | Loss: 0.6598 | Speed: 99.4 batches/s | ETA: 04:35\n",
      "  Train: [30200/57525] ( 52.5%) | Loss: 0.6597 | Speed: 99.5 batches/s | ETA: 04:34\n",
      "  Train: [30300/57525] ( 52.7%) | Loss: 0.6598 | Speed: 99.5 batches/s | ETA: 04:33\n",
      "  Train: [30400/57525] ( 52.8%) | Loss: 0.6598 | Speed: 99.5 batches/s | ETA: 04:32\n",
      "  Train: [30500/57525] ( 53.0%) | Loss: 0.6597 | Speed: 99.5 batches/s | ETA: 04:31\n",
      "  Train: [30600/57525] ( 53.2%) | Loss: 0.6598 | Speed: 99.5 batches/s | ETA: 04:30\n",
      "  Train: [30700/57525] ( 53.4%) | Loss: 0.6597 | Speed: 99.6 batches/s | ETA: 04:29\n",
      "  Train: [30800/57525] ( 53.5%) | Loss: 0.6598 | Speed: 99.6 batches/s | ETA: 04:28\n",
      "  Train: [30900/57525] ( 53.7%) | Loss: 0.6597 | Speed: 99.6 batches/s | ETA: 04:27\n",
      "  Train: [31000/57525] ( 53.9%) | Loss: 0.6598 | Speed: 99.6 batches/s | ETA: 04:26\n",
      "  Train: [31100/57525] ( 54.1%) | Loss: 0.6598 | Speed: 99.6 batches/s | ETA: 04:25\n",
      "  Train: [31200/57525] ( 54.2%) | Loss: 0.6598 | Speed: 99.6 batches/s | ETA: 04:24\n",
      "  Train: [31300/57525] ( 54.4%) | Loss: 0.6598 | Speed: 99.7 batches/s | ETA: 04:23\n",
      "  Train: [31400/57525] ( 54.6%) | Loss: 0.6598 | Speed: 99.7 batches/s | ETA: 04:22\n",
      "  Train: [31500/57525] ( 54.8%) | Loss: 0.6597 | Speed: 99.7 batches/s | ETA: 04:21\n",
      "  Train: [31600/57525] ( 54.9%) | Loss: 0.6597 | Speed: 99.7 batches/s | ETA: 04:20\n",
      "  Train: [31700/57525] ( 55.1%) | Loss: 0.6597 | Speed: 99.7 batches/s | ETA: 04:18\n",
      "  Train: [31800/57525] ( 55.3%) | Loss: 0.6597 | Speed: 99.7 batches/s | ETA: 04:17\n",
      "  Train: [31900/57525] ( 55.5%) | Loss: 0.6597 | Speed: 99.8 batches/s | ETA: 04:16\n",
      "  Train: [32000/57525] ( 55.6%) | Loss: 0.6596 | Speed: 99.8 batches/s | ETA: 04:15\n",
      "  Train: [32100/57525] ( 55.8%) | Loss: 0.6596 | Speed: 99.8 batches/s | ETA: 04:14\n",
      "  Train: [32200/57525] ( 56.0%) | Loss: 0.6595 | Speed: 99.8 batches/s | ETA: 04:13\n",
      "  Train: [32300/57525] ( 56.1%) | Loss: 0.6594 | Speed: 99.8 batches/s | ETA: 04:12\n",
      "  Train: [32400/57525] ( 56.3%) | Loss: 0.6595 | Speed: 99.8 batches/s | ETA: 04:11\n",
      "  Train: [32500/57525] ( 56.5%) | Loss: 0.6595 | Speed: 99.8 batches/s | ETA: 04:10\n",
      "  Train: [32600/57525] ( 56.7%) | Loss: 0.6595 | Speed: 99.9 batches/s | ETA: 04:09\n",
      "  Train: [32700/57525] ( 56.8%) | Loss: 0.6594 | Speed: 99.9 batches/s | ETA: 04:08\n",
      "  Train: [32800/57525] ( 57.0%) | Loss: 0.6593 | Speed: 99.9 batches/s | ETA: 04:07\n",
      "  Train: [32900/57525] ( 57.2%) | Loss: 0.6594 | Speed: 99.9 batches/s | ETA: 04:06\n",
      "  Train: [33000/57525] ( 57.4%) | Loss: 0.6593 | Speed: 99.9 batches/s | ETA: 04:05\n",
      "  Train: [33100/57525] ( 57.5%) | Loss: 0.6593 | Speed: 99.9 batches/s | ETA: 04:04\n",
      "  Train: [33200/57525] ( 57.7%) | Loss: 0.6594 | Speed: 99.9 batches/s | ETA: 04:03\n",
      "  Train: [33300/57525] ( 57.9%) | Loss: 0.6594 | Speed: 100.0 batches/s | ETA: 04:02\n",
      "  Train: [33400/57525] ( 58.1%) | Loss: 0.6593 | Speed: 100.0 batches/s | ETA: 04:01\n",
      "  Train: [33500/57525] ( 58.2%) | Loss: 0.6593 | Speed: 100.0 batches/s | ETA: 04:00\n",
      "  Train: [33600/57525] ( 58.4%) | Loss: 0.6593 | Speed: 100.0 batches/s | ETA: 03:59\n",
      "  Train: [33700/57525] ( 58.6%) | Loss: 0.6593 | Speed: 100.0 batches/s | ETA: 03:58\n",
      "  Train: [33800/57525] ( 58.8%) | Loss: 0.6593 | Speed: 100.0 batches/s | ETA: 03:57\n",
      "  Train: [33900/57525] ( 58.9%) | Loss: 0.6593 | Speed: 100.1 batches/s | ETA: 03:56\n",
      "  Train: [34000/57525] ( 59.1%) | Loss: 0.6592 | Speed: 100.1 batches/s | ETA: 03:55\n",
      "  Train: [34100/57525] ( 59.3%) | Loss: 0.6593 | Speed: 100.1 batches/s | ETA: 03:54\n",
      "  Train: [34200/57525] ( 59.5%) | Loss: 0.6593 | Speed: 100.1 batches/s | ETA: 03:53\n",
      "  Train: [34300/57525] ( 59.6%) | Loss: 0.6592 | Speed: 100.1 batches/s | ETA: 03:51\n",
      "  Train: [34400/57525] ( 59.8%) | Loss: 0.6592 | Speed: 100.1 batches/s | ETA: 03:50\n",
      "  Train: [34500/57525] ( 60.0%) | Loss: 0.6592 | Speed: 100.1 batches/s | ETA: 03:49\n",
      "  Train: [34600/57525] ( 60.1%) | Loss: 0.6591 | Speed: 100.2 batches/s | ETA: 03:48\n",
      "  Train: [34700/57525] ( 60.3%) | Loss: 0.6590 | Speed: 100.2 batches/s | ETA: 03:47\n",
      "  Train: [34800/57525] ( 60.5%) | Loss: 0.6591 | Speed: 100.2 batches/s | ETA: 03:46\n",
      "  Train: [34900/57525] ( 60.7%) | Loss: 0.6591 | Speed: 100.2 batches/s | ETA: 03:45\n",
      "  Train: [35000/57525] ( 60.8%) | Loss: 0.6592 | Speed: 100.2 batches/s | ETA: 03:44\n",
      "  Train: [35100/57525] ( 61.0%) | Loss: 0.6591 | Speed: 100.2 batches/s | ETA: 03:43\n",
      "  Train: [35200/57525] ( 61.2%) | Loss: 0.6591 | Speed: 100.2 batches/s | ETA: 03:42\n",
      "  Train: [35300/57525] ( 61.4%) | Loss: 0.6590 | Speed: 100.2 batches/s | ETA: 03:41\n",
      "  Train: [35400/57525] ( 61.5%) | Loss: 0.6590 | Speed: 100.3 batches/s | ETA: 03:40\n",
      "  Train: [35500/57525] ( 61.7%) | Loss: 0.6590 | Speed: 100.3 batches/s | ETA: 03:39\n",
      "  Train: [35600/57525] ( 61.9%) | Loss: 0.6590 | Speed: 100.3 batches/s | ETA: 03:38\n",
      "  Train: [35700/57525] ( 62.1%) | Loss: 0.6589 | Speed: 100.3 batches/s | ETA: 03:37\n",
      "  Train: [35800/57525] ( 62.2%) | Loss: 0.6589 | Speed: 100.3 batches/s | ETA: 03:36\n",
      "  Train: [35900/57525] ( 62.4%) | Loss: 0.6589 | Speed: 100.3 batches/s | ETA: 03:35\n",
      "  Train: [36000/57525] ( 62.6%) | Loss: 0.6589 | Speed: 100.3 batches/s | ETA: 03:34\n",
      "  Train: [36100/57525] ( 62.8%) | Loss: 0.6590 | Speed: 100.4 batches/s | ETA: 03:33\n",
      "  Train: [36200/57525] ( 62.9%) | Loss: 0.6590 | Speed: 100.4 batches/s | ETA: 03:32\n",
      "  Train: [36300/57525] ( 63.1%) | Loss: 0.6590 | Speed: 100.4 batches/s | ETA: 03:31\n",
      "  Train: [36400/57525] ( 63.3%) | Loss: 0.6589 | Speed: 100.4 batches/s | ETA: 03:30\n",
      "  Train: [36500/57525] ( 63.5%) | Loss: 0.6589 | Speed: 100.4 batches/s | ETA: 03:29\n",
      "  Train: [36600/57525] ( 63.6%) | Loss: 0.6589 | Speed: 100.4 batches/s | ETA: 03:28\n",
      "  Train: [36700/57525] ( 63.8%) | Loss: 0.6589 | Speed: 100.4 batches/s | ETA: 03:27\n",
      "  Train: [36800/57525] ( 64.0%) | Loss: 0.6589 | Speed: 100.4 batches/s | ETA: 03:26\n",
      "  Train: [36900/57525] ( 64.1%) | Loss: 0.6588 | Speed: 100.5 batches/s | ETA: 03:25\n",
      "  Train: [37000/57525] ( 64.3%) | Loss: 0.6588 | Speed: 100.5 batches/s | ETA: 03:24\n",
      "  Train: [37100/57525] ( 64.5%) | Loss: 0.6587 | Speed: 100.5 batches/s | ETA: 03:23\n",
      "  Train: [37200/57525] ( 64.7%) | Loss: 0.6587 | Speed: 100.5 batches/s | ETA: 03:22\n",
      "  Train: [37300/57525] ( 64.8%) | Loss: 0.6587 | Speed: 100.5 batches/s | ETA: 03:21\n",
      "  Train: [37400/57525] ( 65.0%) | Loss: 0.6586 | Speed: 100.5 batches/s | ETA: 03:20\n",
      "  Train: [37500/57525] ( 65.2%) | Loss: 0.6587 | Speed: 100.5 batches/s | ETA: 03:19\n",
      "  Train: [37600/57525] ( 65.4%) | Loss: 0.6586 | Speed: 100.5 batches/s | ETA: 03:18\n",
      "  Train: [37700/57525] ( 65.5%) | Loss: 0.6586 | Speed: 100.5 batches/s | ETA: 03:17\n",
      "  Train: [37800/57525] ( 65.7%) | Loss: 0.6586 | Speed: 100.6 batches/s | ETA: 03:16\n",
      "  Train: [37900/57525] ( 65.9%) | Loss: 0.6586 | Speed: 100.6 batches/s | ETA: 03:15\n",
      "  Train: [38000/57525] ( 66.1%) | Loss: 0.6586 | Speed: 100.6 batches/s | ETA: 03:14\n",
      "  Train: [38100/57525] ( 66.2%) | Loss: 0.6586 | Speed: 100.6 batches/s | ETA: 03:13\n",
      "  Train: [38200/57525] ( 66.4%) | Loss: 0.6586 | Speed: 100.6 batches/s | ETA: 03:12\n",
      "  Train: [38300/57525] ( 66.6%) | Loss: 0.6585 | Speed: 100.6 batches/s | ETA: 03:11\n",
      "  Train: [38400/57525] ( 66.8%) | Loss: 0.6585 | Speed: 100.6 batches/s | ETA: 03:10\n",
      "  Train: [38500/57525] ( 66.9%) | Loss: 0.6585 | Speed: 100.6 batches/s | ETA: 03:09\n",
      "  Train: [38600/57525] ( 67.1%) | Loss: 0.6584 | Speed: 100.6 batches/s | ETA: 03:08\n",
      "  Train: [38700/57525] ( 67.3%) | Loss: 0.6584 | Speed: 100.6 batches/s | ETA: 03:07\n",
      "  Train: [38800/57525] ( 67.4%) | Loss: 0.6585 | Speed: 100.6 batches/s | ETA: 03:06\n",
      "  Train: [38900/57525] ( 67.6%) | Loss: 0.6584 | Speed: 100.7 batches/s | ETA: 03:05\n",
      "  Train: [39000/57525] ( 67.8%) | Loss: 0.6584 | Speed: 100.7 batches/s | ETA: 03:04\n",
      "  Train: [39100/57525] ( 68.0%) | Loss: 0.6585 | Speed: 100.7 batches/s | ETA: 03:03\n",
      "  Train: [39200/57525] ( 68.1%) | Loss: 0.6584 | Speed: 100.7 batches/s | ETA: 03:01\n",
      "  Train: [39300/57525] ( 68.3%) | Loss: 0.6584 | Speed: 100.7 batches/s | ETA: 03:00\n",
      "  Train: [39400/57525] ( 68.5%) | Loss: 0.6584 | Speed: 100.7 batches/s | ETA: 02:59\n",
      "  Train: [39500/57525] ( 68.7%) | Loss: 0.6584 | Speed: 100.7 batches/s | ETA: 02:58\n",
      "  Train: [39600/57525] ( 68.8%) | Loss: 0.6584 | Speed: 100.7 batches/s | ETA: 02:57\n",
      "  Train: [39700/57525] ( 69.0%) | Loss: 0.6584 | Speed: 100.7 batches/s | ETA: 02:56\n",
      "  Train: [39800/57525] ( 69.2%) | Loss: 0.6584 | Speed: 100.8 batches/s | ETA: 02:55\n",
      "  Train: [39900/57525] ( 69.4%) | Loss: 0.6584 | Speed: 100.8 batches/s | ETA: 02:54\n",
      "  Train: [40000/57525] ( 69.5%) | Loss: 0.6584 | Speed: 100.8 batches/s | ETA: 02:53\n",
      "  Train: [40100/57525] ( 69.7%) | Loss: 0.6583 | Speed: 100.8 batches/s | ETA: 02:52\n",
      "  Train: [40200/57525] ( 69.9%) | Loss: 0.6582 | Speed: 100.8 batches/s | ETA: 02:51\n",
      "  Train: [40300/57525] ( 70.1%) | Loss: 0.6583 | Speed: 100.8 batches/s | ETA: 02:50\n",
      "  Train: [40400/57525] ( 70.2%) | Loss: 0.6583 | Speed: 100.8 batches/s | ETA: 02:49\n",
      "  Train: [40500/57525] ( 70.4%) | Loss: 0.6583 | Speed: 100.8 batches/s | ETA: 02:48\n",
      "  Train: [40600/57525] ( 70.6%) | Loss: 0.6582 | Speed: 100.8 batches/s | ETA: 02:47\n",
      "  Train: [40700/57525] ( 70.8%) | Loss: 0.6583 | Speed: 100.9 batches/s | ETA: 02:46\n",
      "  Train: [40800/57525] ( 70.9%) | Loss: 0.6582 | Speed: 100.9 batches/s | ETA: 02:45\n",
      "  Train: [40900/57525] ( 71.1%) | Loss: 0.6582 | Speed: 100.9 batches/s | ETA: 02:44\n",
      "  Train: [41000/57525] ( 71.3%) | Loss: 0.6581 | Speed: 100.9 batches/s | ETA: 02:43\n",
      "  Train: [41100/57525] ( 71.4%) | Loss: 0.6581 | Speed: 100.9 batches/s | ETA: 02:42\n",
      "  Train: [41200/57525] ( 71.6%) | Loss: 0.6581 | Speed: 100.9 batches/s | ETA: 02:41\n",
      "  Train: [41300/57525] ( 71.8%) | Loss: 0.6581 | Speed: 100.9 batches/s | ETA: 02:40\n",
      "  Train: [41400/57525] ( 72.0%) | Loss: 0.6581 | Speed: 100.9 batches/s | ETA: 02:39\n",
      "  Train: [41500/57525] ( 72.1%) | Loss: 0.6581 | Speed: 100.9 batches/s | ETA: 02:38\n",
      "  Train: [41600/57525] ( 72.3%) | Loss: 0.6582 | Speed: 100.9 batches/s | ETA: 02:37\n",
      "  Train: [41700/57525] ( 72.5%) | Loss: 0.6581 | Speed: 100.9 batches/s | ETA: 02:36\n",
      "  Train: [41800/57525] ( 72.7%) | Loss: 0.6582 | Speed: 101.0 batches/s | ETA: 02:35\n",
      "  Train: [41900/57525] ( 72.8%) | Loss: 0.6582 | Speed: 101.0 batches/s | ETA: 02:34\n",
      "  Train: [42000/57525] ( 73.0%) | Loss: 0.6582 | Speed: 101.0 batches/s | ETA: 02:33\n",
      "  Train: [42100/57525] ( 73.2%) | Loss: 0.6582 | Speed: 101.0 batches/s | ETA: 02:32\n",
      "  Train: [42200/57525] ( 73.4%) | Loss: 0.6582 | Speed: 101.0 batches/s | ETA: 02:31\n",
      "  Train: [42300/57525] ( 73.5%) | Loss: 0.6582 | Speed: 101.0 batches/s | ETA: 02:30\n",
      "  Train: [42400/57525] ( 73.7%) | Loss: 0.6581 | Speed: 101.0 batches/s | ETA: 02:29\n",
      "  Train: [42500/57525] ( 73.9%) | Loss: 0.6582 | Speed: 101.0 batches/s | ETA: 02:28\n",
      "  Train: [42600/57525] ( 74.1%) | Loss: 0.6581 | Speed: 101.0 batches/s | ETA: 02:27\n",
      "  Train: [42700/57525] ( 74.2%) | Loss: 0.6581 | Speed: 101.0 batches/s | ETA: 02:26\n",
      "  Train: [42800/57525] ( 74.4%) | Loss: 0.6581 | Speed: 101.0 batches/s | ETA: 02:25\n",
      "  Train: [42900/57525] ( 74.6%) | Loss: 0.6581 | Speed: 101.1 batches/s | ETA: 02:24\n",
      "  Train: [43000/57525] ( 74.8%) | Loss: 0.6580 | Speed: 101.1 batches/s | ETA: 02:23\n",
      "  Train: [43100/57525] ( 74.9%) | Loss: 0.6580 | Speed: 101.1 batches/s | ETA: 02:22\n",
      "  Train: [43200/57525] ( 75.1%) | Loss: 0.6580 | Speed: 101.1 batches/s | ETA: 02:21\n",
      "  Train: [43300/57525] ( 75.3%) | Loss: 0.6581 | Speed: 101.1 batches/s | ETA: 02:20\n",
      "  Train: [43400/57525] ( 75.4%) | Loss: 0.6581 | Speed: 101.1 batches/s | ETA: 02:19\n",
      "  Train: [43500/57525] ( 75.6%) | Loss: 0.6581 | Speed: 101.1 batches/s | ETA: 02:18\n",
      "  Train: [43600/57525] ( 75.8%) | Loss: 0.6581 | Speed: 101.1 batches/s | ETA: 02:17\n",
      "  Train: [43700/57525] ( 76.0%) | Loss: 0.6581 | Speed: 101.1 batches/s | ETA: 02:16\n",
      "  Train: [43800/57525] ( 76.1%) | Loss: 0.6580 | Speed: 101.1 batches/s | ETA: 02:15\n",
      "  Train: [43900/57525] ( 76.3%) | Loss: 0.6580 | Speed: 101.1 batches/s | ETA: 02:14\n",
      "  Train: [44000/57525] ( 76.5%) | Loss: 0.6580 | Speed: 101.2 batches/s | ETA: 02:13\n",
      "  Train: [44100/57525] ( 76.7%) | Loss: 0.6580 | Speed: 101.2 batches/s | ETA: 02:12\n",
      "  Train: [44200/57525] ( 76.8%) | Loss: 0.6581 | Speed: 101.2 batches/s | ETA: 02:11\n",
      "  Train: [44300/57525] ( 77.0%) | Loss: 0.6581 | Speed: 101.2 batches/s | ETA: 02:10\n",
      "  Train: [44400/57525] ( 77.2%) | Loss: 0.6581 | Speed: 101.2 batches/s | ETA: 02:09\n",
      "  Train: [44500/57525] ( 77.4%) | Loss: 0.6581 | Speed: 101.2 batches/s | ETA: 02:08\n",
      "  Train: [44600/57525] ( 77.5%) | Loss: 0.6581 | Speed: 101.2 batches/s | ETA: 02:07\n",
      "  Train: [44700/57525] ( 77.7%) | Loss: 0.6581 | Speed: 101.2 batches/s | ETA: 02:06\n",
      "  Train: [44800/57525] ( 77.9%) | Loss: 0.6581 | Speed: 101.2 batches/s | ETA: 02:05\n",
      "  Train: [44900/57525] ( 78.1%) | Loss: 0.6581 | Speed: 101.2 batches/s | ETA: 02:04\n",
      "  Train: [45000/57525] ( 78.2%) | Loss: 0.6581 | Speed: 101.2 batches/s | ETA: 02:03\n",
      "  Train: [45100/57525] ( 78.4%) | Loss: 0.6582 | Speed: 101.3 batches/s | ETA: 02:02\n",
      "  Train: [45200/57525] ( 78.6%) | Loss: 0.6582 | Speed: 101.3 batches/s | ETA: 02:01\n",
      "  Train: [45300/57525] ( 78.7%) | Loss: 0.6582 | Speed: 101.3 batches/s | ETA: 02:00\n",
      "  Train: [45400/57525] ( 78.9%) | Loss: 0.6582 | Speed: 101.3 batches/s | ETA: 01:59\n",
      "  Train: [45500/57525] ( 79.1%) | Loss: 0.6582 | Speed: 101.3 batches/s | ETA: 01:58\n",
      "  Train: [45600/57525] ( 79.3%) | Loss: 0.6582 | Speed: 101.3 batches/s | ETA: 01:57\n",
      "  Train: [45700/57525] ( 79.4%) | Loss: 0.6582 | Speed: 101.3 batches/s | ETA: 01:56\n",
      "  Train: [45800/57525] ( 79.6%) | Loss: 0.6582 | Speed: 101.3 batches/s | ETA: 01:55\n",
      "  Train: [45900/57525] ( 79.8%) | Loss: 0.6582 | Speed: 101.3 batches/s | ETA: 01:54\n",
      "  Train: [46000/57525] ( 80.0%) | Loss: 0.6582 | Speed: 101.3 batches/s | ETA: 01:53\n",
      "  Train: [46100/57525] ( 80.1%) | Loss: 0.6582 | Speed: 101.3 batches/s | ETA: 01:52\n",
      "  Train: [46200/57525] ( 80.3%) | Loss: 0.6582 | Speed: 101.3 batches/s | ETA: 01:51\n",
      "  Train: [46300/57525] ( 80.5%) | Loss: 0.6581 | Speed: 101.4 batches/s | ETA: 01:50\n",
      "  Train: [46400/57525] ( 80.7%) | Loss: 0.6581 | Speed: 101.4 batches/s | ETA: 01:49\n",
      "  Train: [46500/57525] ( 80.8%) | Loss: 0.6581 | Speed: 101.4 batches/s | ETA: 01:48\n",
      "  Train: [46600/57525] ( 81.0%) | Loss: 0.6581 | Speed: 101.4 batches/s | ETA: 01:47\n",
      "  Train: [46700/57525] ( 81.2%) | Loss: 0.6581 | Speed: 101.4 batches/s | ETA: 01:46\n",
      "  Train: [46800/57525] ( 81.4%) | Loss: 0.6581 | Speed: 101.4 batches/s | ETA: 01:45\n",
      "  Train: [46900/57525] ( 81.5%) | Loss: 0.6581 | Speed: 101.4 batches/s | ETA: 01:44\n",
      "  Train: [47000/57525] ( 81.7%) | Loss: 0.6581 | Speed: 101.4 batches/s | ETA: 01:43\n",
      "  Train: [47100/57525] ( 81.9%) | Loss: 0.6581 | Speed: 101.4 batches/s | ETA: 01:42\n",
      "  Train: [47200/57525] ( 82.1%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:41\n",
      "  Train: [47300/57525] ( 82.2%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:40\n",
      "  Train: [47400/57525] ( 82.4%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:39\n",
      "  Train: [47500/57525] ( 82.6%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:38\n",
      "  Train: [47600/57525] ( 82.7%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:37\n",
      "  Train: [47700/57525] ( 82.9%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:36\n",
      "  Train: [47800/57525] ( 83.1%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:35\n",
      "  Train: [47900/57525] ( 83.3%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:34\n",
      "  Train: [48000/57525] ( 83.4%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:33\n",
      "  Train: [48100/57525] ( 83.6%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:32\n",
      "  Train: [48200/57525] ( 83.8%) | Loss: 0.6579 | Speed: 101.4 batches/s | ETA: 01:31\n",
      "  Train: [48300/57525] ( 84.0%) | Loss: 0.6579 | Speed: 101.4 batches/s | ETA: 01:31\n",
      "  Train: [48400/57525] ( 84.1%) | Loss: 0.6579 | Speed: 101.4 batches/s | ETA: 01:30\n",
      "  Train: [48500/57525] ( 84.3%) | Loss: 0.6579 | Speed: 101.4 batches/s | ETA: 01:29\n",
      "  Train: [48600/57525] ( 84.5%) | Loss: 0.6579 | Speed: 101.4 batches/s | ETA: 01:28\n",
      "  Train: [48700/57525] ( 84.7%) | Loss: 0.6579 | Speed: 101.4 batches/s | ETA: 01:27\n",
      "  Train: [48800/57525] ( 84.8%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:26\n",
      "  Train: [48900/57525] ( 85.0%) | Loss: 0.6579 | Speed: 101.4 batches/s | ETA: 01:25\n",
      "  Train: [49000/57525] ( 85.2%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:24\n",
      "  Train: [49100/57525] ( 85.4%) | Loss: 0.6580 | Speed: 101.4 batches/s | ETA: 01:23\n",
      "  Train: [49200/57525] ( 85.5%) | Loss: 0.6579 | Speed: 101.4 batches/s | ETA: 01:22\n",
      "  Train: [49300/57525] ( 85.7%) | Loss: 0.6579 | Speed: 101.4 batches/s | ETA: 01:21\n",
      "  Train: [49400/57525] ( 85.9%) | Loss: 0.6579 | Speed: 101.4 batches/s | ETA: 01:20\n",
      "  Train: [49500/57525] ( 86.0%) | Loss: 0.6578 | Speed: 101.3 batches/s | ETA: 01:19\n",
      "  Train: [49600/57525] ( 86.2%) | Loss: 0.6578 | Speed: 101.3 batches/s | ETA: 01:18\n",
      "  Train: [49700/57525] ( 86.4%) | Loss: 0.6578 | Speed: 101.3 batches/s | ETA: 01:17\n",
      "  Train: [49800/57525] ( 86.6%) | Loss: 0.6578 | Speed: 101.3 batches/s | ETA: 01:16\n",
      "  Train: [49900/57525] ( 86.7%) | Loss: 0.6578 | Speed: 101.3 batches/s | ETA: 01:15\n",
      "  Train: [50000/57525] ( 86.9%) | Loss: 0.6578 | Speed: 101.3 batches/s | ETA: 01:14\n",
      "  Train: [50100/57525] ( 87.1%) | Loss: 0.6577 | Speed: 101.2 batches/s | ETA: 01:13\n",
      "  Train: [50200/57525] ( 87.3%) | Loss: 0.6577 | Speed: 101.2 batches/s | ETA: 01:12\n",
      "  Train: [50300/57525] ( 87.4%) | Loss: 0.6577 | Speed: 101.2 batches/s | ETA: 01:11\n",
      "  Train: [50400/57525] ( 87.6%) | Loss: 0.6578 | Speed: 101.2 batches/s | ETA: 01:10\n",
      "  Train: [50500/57525] ( 87.8%) | Loss: 0.6578 | Speed: 101.2 batches/s | ETA: 01:09\n",
      "  Train: [50600/57525] ( 88.0%) | Loss: 0.6578 | Speed: 101.2 batches/s | ETA: 01:08\n",
      "  Train: [50700/57525] ( 88.1%) | Loss: 0.6577 | Speed: 101.2 batches/s | ETA: 01:07\n",
      "  Train: [50800/57525] ( 88.3%) | Loss: 0.6578 | Speed: 101.1 batches/s | ETA: 01:06\n",
      "  Train: [50900/57525] ( 88.5%) | Loss: 0.6578 | Speed: 101.1 batches/s | ETA: 01:05\n",
      "  Train: [51000/57525] ( 88.7%) | Loss: 0.6578 | Speed: 101.1 batches/s | ETA: 01:04\n",
      "  Train: [51100/57525] ( 88.8%) | Loss: 0.6578 | Speed: 101.1 batches/s | ETA: 01:03\n",
      "  Train: [51200/57525] ( 89.0%) | Loss: 0.6578 | Speed: 101.1 batches/s | ETA: 01:02\n",
      "  Train: [51300/57525] ( 89.2%) | Loss: 0.6578 | Speed: 101.1 batches/s | ETA: 01:01\n",
      "  Train: [51400/57525] ( 89.4%) | Loss: 0.6578 | Speed: 101.1 batches/s | ETA: 01:00\n",
      "  Train: [51500/57525] ( 89.5%) | Loss: 0.6578 | Speed: 101.1 batches/s | ETA: 00:59\n",
      "  Train: [51600/57525] ( 89.7%) | Loss: 0.6577 | Speed: 101.1 batches/s | ETA: 00:58\n",
      "  Train: [51700/57525] ( 89.9%) | Loss: 0.6577 | Speed: 101.1 batches/s | ETA: 00:57\n",
      "  Train: [51800/57525] ( 90.0%) | Loss: 0.6577 | Speed: 101.1 batches/s | ETA: 00:56\n",
      "  Train: [51900/57525] ( 90.2%) | Loss: 0.6576 | Speed: 101.1 batches/s | ETA: 00:55\n",
      "  Train: [52000/57525] ( 90.4%) | Loss: 0.6576 | Speed: 101.1 batches/s | ETA: 00:54\n",
      "  Train: [52100/57525] ( 90.6%) | Loss: 0.6577 | Speed: 101.1 batches/s | ETA: 00:53\n",
      "  Train: [52200/57525] ( 90.7%) | Loss: 0.6576 | Speed: 101.0 batches/s | ETA: 00:52\n",
      "  Train: [52300/57525] ( 90.9%) | Loss: 0.6576 | Speed: 101.0 batches/s | ETA: 00:51\n",
      "  Train: [52400/57525] ( 91.1%) | Loss: 0.6576 | Speed: 101.0 batches/s | ETA: 00:50\n",
      "  Train: [52500/57525] ( 91.3%) | Loss: 0.6576 | Speed: 101.0 batches/s | ETA: 00:49\n",
      "  Train: [52600/57525] ( 91.4%) | Loss: 0.6575 | Speed: 101.0 batches/s | ETA: 00:48\n",
      "  Train: [52700/57525] ( 91.6%) | Loss: 0.6575 | Speed: 101.0 batches/s | ETA: 00:47\n",
      "  Train: [52800/57525] ( 91.8%) | Loss: 0.6574 | Speed: 101.0 batches/s | ETA: 00:46\n",
      "  Train: [52900/57525] ( 92.0%) | Loss: 0.6574 | Speed: 101.0 batches/s | ETA: 00:45\n",
      "  Train: [53000/57525] ( 92.1%) | Loss: 0.6574 | Speed: 101.0 batches/s | ETA: 00:44\n",
      "  Train: [53100/57525] ( 92.3%) | Loss: 0.6574 | Speed: 101.0 batches/s | ETA: 00:43\n",
      "  Train: [53200/57525] ( 92.5%) | Loss: 0.6574 | Speed: 101.0 batches/s | ETA: 00:42\n",
      "  Train: [53300/57525] ( 92.7%) | Loss: 0.6574 | Speed: 101.0 batches/s | ETA: 00:41\n",
      "  Train: [53400/57525] ( 92.8%) | Loss: 0.6574 | Speed: 101.0 batches/s | ETA: 00:40\n",
      "  Train: [53500/57525] ( 93.0%) | Loss: 0.6574 | Speed: 101.0 batches/s | ETA: 00:39\n",
      "  Train: [53600/57525] ( 93.2%) | Loss: 0.6574 | Speed: 100.9 batches/s | ETA: 00:38\n",
      "  Train: [53700/57525] ( 93.4%) | Loss: 0.6574 | Speed: 100.9 batches/s | ETA: 00:37\n",
      "  Train: [53800/57525] ( 93.5%) | Loss: 0.6574 | Speed: 100.9 batches/s | ETA: 00:36\n",
      "  Train: [53900/57525] ( 93.7%) | Loss: 0.6574 | Speed: 100.9 batches/s | ETA: 00:35\n",
      "  Train: [54000/57525] ( 93.9%) | Loss: 0.6573 | Speed: 100.9 batches/s | ETA: 00:34\n",
      "  Train: [54100/57525] ( 94.0%) | Loss: 0.6573 | Speed: 100.9 batches/s | ETA: 00:33\n",
      "  Train: [54200/57525] ( 94.2%) | Loss: 0.6573 | Speed: 100.9 batches/s | ETA: 00:32\n",
      "  Train: [54300/57525] ( 94.4%) | Loss: 0.6573 | Speed: 100.9 batches/s | ETA: 00:31\n",
      "  Train: [54400/57525] ( 94.6%) | Loss: 0.6573 | Speed: 100.9 batches/s | ETA: 00:30\n",
      "  Train: [54500/57525] ( 94.7%) | Loss: 0.6573 | Speed: 100.9 batches/s | ETA: 00:29\n",
      "  Train: [54600/57525] ( 94.9%) | Loss: 0.6572 | Speed: 100.9 batches/s | ETA: 00:28\n",
      "  Train: [54700/57525] ( 95.1%) | Loss: 0.6572 | Speed: 100.9 batches/s | ETA: 00:28\n",
      "  Train: [54800/57525] ( 95.3%) | Loss: 0.6572 | Speed: 100.9 batches/s | ETA: 00:27\n",
      "  Train: [54900/57525] ( 95.4%) | Loss: 0.6572 | Speed: 100.9 batches/s | ETA: 00:26\n",
      "  Train: [55000/57525] ( 95.6%) | Loss: 0.6572 | Speed: 100.9 batches/s | ETA: 00:25\n",
      "  Train: [55100/57525] ( 95.8%) | Loss: 0.6572 | Speed: 100.9 batches/s | ETA: 00:24\n",
      "  Train: [55200/57525] ( 96.0%) | Loss: 0.6572 | Speed: 100.9 batches/s | ETA: 00:23\n",
      "  Train: [55300/57525] ( 96.1%) | Loss: 0.6572 | Speed: 100.9 batches/s | ETA: 00:22\n",
      "  Train: [55400/57525] ( 96.3%) | Loss: 0.6572 | Speed: 100.9 batches/s | ETA: 00:21\n",
      "  Train: [55500/57525] ( 96.5%) | Loss: 0.6572 | Speed: 100.9 batches/s | ETA: 00:20\n",
      "  Train: [55600/57525] ( 96.7%) | Loss: 0.6571 | Speed: 100.9 batches/s | ETA: 00:19\n",
      "  Train: [55700/57525] ( 96.8%) | Loss: 0.6571 | Speed: 100.9 batches/s | ETA: 00:18\n",
      "  Train: [55800/57525] ( 97.0%) | Loss: 0.6571 | Speed: 100.9 batches/s | ETA: 00:17\n",
      "  Train: [55900/57525] ( 97.2%) | Loss: 0.6571 | Speed: 100.9 batches/s | ETA: 00:16\n",
      "  Train: [56000/57525] ( 97.3%) | Loss: 0.6571 | Speed: 100.9 batches/s | ETA: 00:15\n",
      "  Train: [56100/57525] ( 97.5%) | Loss: 0.6571 | Speed: 100.8 batches/s | ETA: 00:14\n",
      "  Train: [56200/57525] ( 97.7%) | Loss: 0.6571 | Speed: 100.8 batches/s | ETA: 00:13\n",
      "  Train: [56300/57525] ( 97.9%) | Loss: 0.6571 | Speed: 100.8 batches/s | ETA: 00:12\n",
      "  Train: [56400/57525] ( 98.0%) | Loss: 0.6570 | Speed: 100.8 batches/s | ETA: 00:11\n",
      "  Train: [56500/57525] ( 98.2%) | Loss: 0.6570 | Speed: 100.8 batches/s | ETA: 00:10\n",
      "  Train: [56600/57525] ( 98.4%) | Loss: 0.6570 | Speed: 100.8 batches/s | ETA: 00:09\n",
      "  Train: [56700/57525] ( 98.6%) | Loss: 0.6569 | Speed: 100.8 batches/s | ETA: 00:08\n",
      "  Train: [56800/57525] ( 98.7%) | Loss: 0.6569 | Speed: 100.8 batches/s | ETA: 00:07\n",
      "  Train: [56900/57525] ( 98.9%) | Loss: 0.6569 | Speed: 100.8 batches/s | ETA: 00:06\n",
      "  Train: [57000/57525] ( 99.1%) | Loss: 0.6569 | Speed: 100.8 batches/s | ETA: 00:05\n",
      "  Train: [57100/57525] ( 99.3%) | Loss: 0.6569 | Speed: 100.8 batches/s | ETA: 00:04\n",
      "  Train: [57200/57525] ( 99.4%) | Loss: 0.6569 | Speed: 100.8 batches/s | ETA: 00:03\n",
      "  Train: [57300/57525] ( 99.6%) | Loss: 0.6569 | Speed: 100.8 batches/s | ETA: 00:02\n",
      "  Train: [57400/57525] ( 99.8%) | Loss: 0.6569 | Speed: 100.8 batches/s | ETA: 00:01\n",
      "  Train: [57500/57525] (100.0%) | Loss: 0.6569 | Speed: 100.8 batches/s | ETA: 00:00\n",
      "  Running validation...\n",
      "    Val: [  50/7191] (  0.7%)\n",
      "    Val: [ 100/7191] (  1.4%)\n",
      "    Val: [ 150/7191] (  2.1%)\n",
      "    Val: [ 200/7191] (  2.8%)\n",
      "    Val: [ 250/7191] (  3.5%)\n",
      "    Val: [ 300/7191] (  4.2%)\n",
      "    Val: [ 350/7191] (  4.9%)\n",
      "    Val: [ 400/7191] (  5.6%)\n",
      "    Val: [ 450/7191] (  6.3%)\n",
      "    Val: [ 500/7191] (  7.0%)\n",
      "    Val: [ 550/7191] (  7.6%)\n",
      "    Val: [ 600/7191] (  8.3%)\n",
      "    Val: [ 650/7191] (  9.0%)\n",
      "    Val: [ 700/7191] (  9.7%)\n",
      "    Val: [ 750/7191] ( 10.4%)\n",
      "    Val: [ 800/7191] ( 11.1%)\n",
      "    Val: [ 850/7191] ( 11.8%)\n",
      "    Val: [ 900/7191] ( 12.5%)\n",
      "    Val: [ 950/7191] ( 13.2%)\n",
      "    Val: [1000/7191] ( 13.9%)\n",
      "    Val: [1050/7191] ( 14.6%)\n",
      "    Val: [1100/7191] ( 15.3%)\n",
      "    Val: [1150/7191] ( 16.0%)\n",
      "    Val: [1200/7191] ( 16.7%)\n",
      "    Val: [1250/7191] ( 17.4%)\n",
      "    Val: [1300/7191] ( 18.1%)\n",
      "    Val: [1350/7191] ( 18.8%)\n",
      "    Val: [1400/7191] ( 19.5%)\n",
      "    Val: [1450/7191] ( 20.2%)\n",
      "    Val: [1500/7191] ( 20.9%)\n",
      "    Val: [1550/7191] ( 21.6%)\n",
      "    Val: [1600/7191] ( 22.3%)\n",
      "    Val: [1650/7191] ( 22.9%)\n",
      "    Val: [1700/7191] ( 23.6%)\n",
      "    Val: [1750/7191] ( 24.3%)\n",
      "    Val: [1800/7191] ( 25.0%)\n",
      "    Val: [1850/7191] ( 25.7%)\n",
      "    Val: [1900/7191] ( 26.4%)\n",
      "    Val: [1950/7191] ( 27.1%)\n",
      "    Val: [2000/7191] ( 27.8%)\n",
      "    Val: [2050/7191] ( 28.5%)\n",
      "    Val: [2100/7191] ( 29.2%)\n",
      "    Val: [2150/7191] ( 29.9%)\n",
      "    Val: [2200/7191] ( 30.6%)\n",
      "    Val: [2250/7191] ( 31.3%)\n",
      "    Val: [2300/7191] ( 32.0%)\n",
      "    Val: [2350/7191] ( 32.7%)\n",
      "    Val: [2400/7191] ( 33.4%)\n",
      "    Val: [2450/7191] ( 34.1%)\n",
      "    Val: [2500/7191] ( 34.8%)\n",
      "    Val: [2550/7191] ( 35.5%)\n",
      "    Val: [2600/7191] ( 36.2%)\n",
      "    Val: [2650/7191] ( 36.9%)\n",
      "    Val: [2700/7191] ( 37.5%)\n",
      "    Val: [2750/7191] ( 38.2%)\n",
      "    Val: [2800/7191] ( 38.9%)\n",
      "    Val: [2850/7191] ( 39.6%)\n",
      "    Val: [2900/7191] ( 40.3%)\n",
      "    Val: [2950/7191] ( 41.0%)\n",
      "    Val: [3000/7191] ( 41.7%)\n",
      "    Val: [3050/7191] ( 42.4%)\n",
      "    Val: [3100/7191] ( 43.1%)\n",
      "    Val: [3150/7191] ( 43.8%)\n",
      "    Val: [3200/7191] ( 44.5%)\n",
      "    Val: [3250/7191] ( 45.2%)\n",
      "    Val: [3300/7191] ( 45.9%)\n",
      "    Val: [3350/7191] ( 46.6%)\n",
      "    Val: [3400/7191] ( 47.3%)\n",
      "    Val: [3450/7191] ( 48.0%)\n",
      "    Val: [3500/7191] ( 48.7%)\n",
      "    Val: [3550/7191] ( 49.4%)\n",
      "    Val: [3600/7191] ( 50.1%)\n",
      "    Val: [3650/7191] ( 50.8%)\n",
      "    Val: [3700/7191] ( 51.5%)\n",
      "    Val: [3750/7191] ( 52.1%)\n",
      "    Val: [3800/7191] ( 52.8%)\n",
      "    Val: [3850/7191] ( 53.5%)\n",
      "    Val: [3900/7191] ( 54.2%)\n",
      "    Val: [3950/7191] ( 54.9%)\n",
      "    Val: [4000/7191] ( 55.6%)\n",
      "    Val: [4050/7191] ( 56.3%)\n",
      "    Val: [4100/7191] ( 57.0%)\n",
      "    Val: [4150/7191] ( 57.7%)\n",
      "    Val: [4200/7191] ( 58.4%)\n",
      "    Val: [4250/7191] ( 59.1%)\n",
      "    Val: [4300/7191] ( 59.8%)\n",
      "    Val: [4350/7191] ( 60.5%)\n",
      "    Val: [4400/7191] ( 61.2%)\n",
      "    Val: [4450/7191] ( 61.9%)\n",
      "    Val: [4500/7191] ( 62.6%)\n",
      "    Val: [4550/7191] ( 63.3%)\n",
      "    Val: [4600/7191] ( 64.0%)\n",
      "    Val: [4650/7191] ( 64.7%)\n",
      "    Val: [4700/7191] ( 65.4%)\n",
      "    Val: [4750/7191] ( 66.1%)\n",
      "    Val: [4800/7191] ( 66.8%)\n",
      "    Val: [4850/7191] ( 67.4%)\n",
      "    Val: [4900/7191] ( 68.1%)\n",
      "    Val: [4950/7191] ( 68.8%)\n",
      "    Val: [5000/7191] ( 69.5%)\n",
      "    Val: [5050/7191] ( 70.2%)\n",
      "    Val: [5100/7191] ( 70.9%)\n",
      "    Val: [5150/7191] ( 71.6%)\n",
      "    Val: [5200/7191] ( 72.3%)\n",
      "    Val: [5250/7191] ( 73.0%)\n",
      "    Val: [5300/7191] ( 73.7%)\n",
      "    Val: [5350/7191] ( 74.4%)\n",
      "    Val: [5400/7191] ( 75.1%)\n",
      "    Val: [5450/7191] ( 75.8%)\n",
      "    Val: [5500/7191] ( 76.5%)\n",
      "    Val: [5550/7191] ( 77.2%)\n",
      "    Val: [5600/7191] ( 77.9%)\n",
      "    Val: [5650/7191] ( 78.6%)\n",
      "    Val: [5700/7191] ( 79.3%)\n",
      "    Val: [5750/7191] ( 80.0%)\n",
      "    Val: [5800/7191] ( 80.7%)\n",
      "    Val: [5850/7191] ( 81.4%)\n",
      "    Val: [5900/7191] ( 82.0%)\n",
      "    Val: [5950/7191] ( 82.7%)\n",
      "    Val: [6000/7191] ( 83.4%)\n",
      "    Val: [6050/7191] ( 84.1%)\n",
      "    Val: [6100/7191] ( 84.8%)\n",
      "    Val: [6150/7191] ( 85.5%)\n",
      "    Val: [6200/7191] ( 86.2%)\n",
      "    Val: [6250/7191] ( 86.9%)\n",
      "    Val: [6300/7191] ( 87.6%)\n",
      "    Val: [6350/7191] ( 88.3%)\n",
      "    Val: [6400/7191] ( 89.0%)\n",
      "    Val: [6450/7191] ( 89.7%)\n",
      "    Val: [6500/7191] ( 90.4%)\n",
      "    Val: [6550/7191] ( 91.1%)\n",
      "    Val: [6600/7191] ( 91.8%)\n",
      "    Val: [6650/7191] ( 92.5%)\n",
      "    Val: [6700/7191] ( 93.2%)\n",
      "    Val: [6750/7191] ( 93.9%)\n",
      "    Val: [6800/7191] ( 94.6%)\n",
      "    Val: [6850/7191] ( 95.3%)\n",
      "    Val: [6900/7191] ( 96.0%)\n",
      "    Val: [6950/7191] ( 96.6%)\n",
      "    Val: [7000/7191] ( 97.3%)\n",
      "    Val: [7050/7191] ( 98.0%)\n",
      "    Val: [7100/7191] ( 98.7%)\n",
      "    Val: [7150/7191] ( 99.4%)\n",
      "\n",
      "  Epoch 03 Summary:\n",
      "    Train MSE (norm): 0.6569 | Val MSE (norm): 0.6215\n",
      "    Time: Train=9.5min, Val=0.3min, Total=9.9min\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 5e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler: reduces LR when validation loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "EPOCHS = 3\n",
    "BATCH  = 512\n",
    "\n",
    "def iter_minibatches(indexes, batch_size=256, shuffle=True, epoch=None, use_normalized=True):\n",
    "    \"\"\"\n",
    "    Generate minibatches. If use_normalized=True, returns normalized targets.\n",
    "    \"\"\"\n",
    "    idx = np.asarray(indexes)\n",
    "    if shuffle:\n",
    "        if epoch is not None:\n",
    "            rng = np.random.default_rng(42 + epoch)\n",
    "        else:\n",
    "            rng = np.random.default_rng()\n",
    "        rng.shuffle(idx)\n",
    "    \n",
    "    target_array = y_norm if use_normalized else y\n",
    "    \n",
    "    for start in range(0, len(idx), batch_size):\n",
    "        mb = idx[start:start+batch_size]\n",
    "        yield (\n",
    "            torch.from_numpy(X_seq[mb]).to(device),           # (B, 4, 23) float32\n",
    "            torch.from_numpy(X_cell[mb]).long().to(device),   # (B,) int64\n",
    "            torch.from_numpy(X_ph[mb]).long().to(device),     # (B,) int64\n",
    "            torch.from_numpy(X_chr[mb]).long().to(device),    # (B,) int64\n",
    "            torch.from_numpy(X_strand[mb]).long().to(device), # (B,) int64\n",
    "            torch.from_numpy(target_array[mb]).to(device),    # (B,) float32\n",
    "        )\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model.train()\n",
    "    train_sum, n_train = 0.0, 0\n",
    "    batch_count = 0\n",
    "    total_train_batches = len(idx_train) // BATCH + (1 if len(idx_train) % BATCH != 0 else 0)\n",
    "    last_update_time = time.time()\n",
    "    \n",
    "    for seq, cl, ph, ch, st, tgt_norm in iter_minibatches(idx_train, batch_size=BATCH, shuffle=True, epoch=epoch, use_normalized=True):\n",
    "        pred_norm = model(seq, cl, ph, ch, st)\n",
    "        loss = criterion(pred_norm, tgt_norm)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_sum += loss.item() * tgt_norm.size(0)\n",
    "        n_train   += tgt_norm.size(0)\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Progress update every 100 batches or every 30 seconds\n",
    "        current_time = time.time()\n",
    "        if batch_count % 100 == 0 or (current_time - last_update_time) >= 30:\n",
    "            progress = (batch_count / total_train_batches) * 100\n",
    "            elapsed = current_time - epoch_start\n",
    "            batches_per_sec = batch_count / elapsed if elapsed > 0 else 0\n",
    "            eta_seconds = (total_train_batches - batch_count) / batches_per_sec if batches_per_sec > 0 else 0\n",
    "            eta_min = int(eta_seconds // 60)\n",
    "            eta_sec = int(eta_seconds % 60)\n",
    "            current_loss = train_sum / n_train if n_train > 0 else 0.0\n",
    "            \n",
    "            print(f\"  Train: [{batch_count:5d}/{total_train_batches}] ({progress:5.1f}%) | \"\n",
    "                  f\"Loss: {current_loss:.4f} | \"\n",
    "                  f\"Speed: {batches_per_sec:.1f} batches/s | \"\n",
    "                  f\"ETA: {eta_min:02d}:{eta_sec:02d}\", flush=True)\n",
    "            last_update_time = current_time\n",
    "\n",
    "    train_loss = train_sum / n_train if n_train > 0 else 0.0\n",
    "    train_time = time.time() - epoch_start\n",
    "    \n",
    "    # ---- val ----\n",
    "    print(\"  Running validation...\", flush=True)\n",
    "    val_start = time.time()\n",
    "    model.eval()\n",
    "    val_sum, n_val = 0.0, 0\n",
    "    val_batch_count = 0\n",
    "    total_val_batches = len(idx_val) // BATCH + (1 if len(idx_val) % BATCH != 0 else 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq, cl, ph, ch, st, tgt_norm in iter_minibatches(idx_val, batch_size=BATCH, shuffle=False, use_normalized=True):\n",
    "            pred_norm = model(seq, cl, ph, ch, st)\n",
    "            loss = criterion(pred_norm, tgt_norm)\n",
    "            val_sum += loss.item() * tgt_norm.size(0)\n",
    "            n_val   += tgt_norm.size(0)\n",
    "            val_batch_count += 1\n",
    "            \n",
    "            if val_batch_count % 50 == 0:\n",
    "                val_progress = (val_batch_count / total_val_batches) * 100\n",
    "                print(f\"    Val: [{val_batch_count:4d}/{total_val_batches}] ({val_progress:5.1f}%)\", flush=True)\n",
    "\n",
    "    val_loss = val_sum / n_val if n_val > 0 else 0.0\n",
    "    val_time = time.time() - val_start\n",
    "    total_time = time.time() - epoch_start\n",
    "    \n",
    "    # Update learning rate scheduler based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n  Epoch {epoch:02d} Summary:\")\n",
    "    print(f\"    Train MSE (norm): {train_loss:.4f} | Val MSE (norm): {val_loss:.4f}\")\n",
    "    print(f\"    Time: Train={train_time/60:.1f}min, Val={val_time/60:.1f}min, Total={total_time/60:.1f}min\")\n",
    "    print(f\"{'='*60}\\n\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8709bf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'MSE': 0.41258042353906366, 'Pearson': 0.6141498719101354, 'Spearman': 0.46539906806388454, 'Accuracy': 0.6684352085146421}\n",
      "Test: {'MSE': 0.4141201608814616, 'Pearson': 0.6140253682155203, 'Spearman': 0.4654716423409502, 'Accuracy': 0.6683227563068305}\n"
     ]
    }
   ],
   "source": [
    "def mse_doc(yhat, y):\n",
    "    yhat = np.asarray(yhat, dtype=np.float64)\n",
    "    y    = np.asarray(y,    dtype=np.float64)\n",
    "    n = y.size\n",
    "    return float(np.sum((y - yhat)**2) / n)\n",
    "\n",
    "def pearson_doc(x, y):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "    n      = x.size\n",
    "    sum_x  = np.sum(x)\n",
    "    sum_y  = np.sum(y)\n",
    "    sum_xy = np.sum(x * y)\n",
    "    sum_x2 = np.sum(x * x)\n",
    "    sum_y2 = np.sum(y * y)\n",
    "    denom = np.sqrt((n * sum_x2 - sum_x * sum_x) * (n * sum_y2 - sum_y * sum_y))\n",
    "    return float((n * sum_xy - sum_x * sum_y) / denom) if denom != 0.0 else 0.0\n",
    "\n",
    "def _ranks_avg(a):\n",
    "    a = np.asarray(a, dtype=np.float64)\n",
    "    order = np.argsort(a, kind=\"mergesort\")\n",
    "    ranks = np.empty_like(order, dtype=np.float64)\n",
    "    sa = a[order]\n",
    "    diff = np.concatenate(([True], sa[1:] != sa[:-1], [True]))\n",
    "    idx = np.flatnonzero(diff)\n",
    "    for s, e in zip(idx[:-1], idx[1:]):\n",
    "        ranks[order[s:e]] = 0.5 * (s + e - 1) + 1.0\n",
    "    return ranks\n",
    "\n",
    "def spearman_doc(x, y):\n",
    "    rx = _ranks_avg(x)\n",
    "    ry = _ranks_avg(y)\n",
    "    d  = rx - ry\n",
    "    n  = rx.size\n",
    "    denom = n * (n * n - 1.0)\n",
    "    return float(1.0 - (6.0 * np.sum(d * d)) / denom) if denom != 0.0 else 0.0\n",
    "\n",
    "def accuracy_direction(yhat, y):\n",
    "    yhat = np.asarray(yhat, dtype=np.float64)\n",
    "    y    = np.asarray(y,    dtype=np.float64)\n",
    "    return float(np.mean((yhat >= 0) == (y >= 0)))\n",
    "\n",
    "@torch.no_grad()\n",
    "def preds_and_trues(indexes, batch_size=256):\n",
    "    \"\"\"\n",
    "    Get predictions and true values. Predictions are de-normalized from normalized space.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ps_norm, ys_norm = [], []\n",
    "    for seq, cl, ph, ch, st, tgt_norm in iter_minibatches(indexes, batch_size=batch_size, shuffle=False, use_normalized=True):\n",
    "        out_norm = model(seq, cl, ph, ch, st)\n",
    "        ps_norm.append(out_norm.detach().cpu().numpy())\n",
    "        ys_norm.append(tgt_norm.detach().cpu().numpy())\n",
    "    \n",
    "    yhat_norm = np.concatenate(ps_norm)\n",
    "    y_norm = np.concatenate(ys_norm)\n",
    "\n",
    "    yhat_real = yhat_norm * sigma + mu\n",
    "    y_real = y_norm * sigma + mu\n",
    "    \n",
    "    return yhat_real, y_real\n",
    "\n",
    "def eval_split(indexes):\n",
    "    yhat, y = preds_and_trues(indexes, batch_size=256)\n",
    "    return {\n",
    "        \"MSE\": mse_doc(yhat, y),\n",
    "        \"Pearson\": pearson_doc(yhat, y),\n",
    "        \"Spearman\": spearman_doc(yhat, y),\n",
    "        \"Accuracy\": accuracy_direction(yhat, y),\n",
    "    }\n",
    "\n",
    "# --- print results ---\n",
    "print(\"Validation:\", eval_split(idx_val))\n",
    "print(\"Test:\",       eval_split(idx_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
