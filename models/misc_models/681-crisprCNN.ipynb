{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4172d63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2921b503c90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== Import libraries ====\n",
    "import math\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b1f4a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "CUDA version: 13.0\n",
      "GPU device: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "# ==== Check GPU/CUDA availability ====\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"Training will use CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "995c5deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train=29452509  val=3681564  test=3681564\n",
      "cells=420  phenotypes=34  chrs=279  strands=3\n",
      "Target stats: mu=-0.0878, sigma=0.8148\n"
     ]
    }
   ],
   "source": [
    "DATA_CSV = \"data/GenomeCRISPR_+_strands.csv\"\n",
    "SEQ_LEN  = 23\n",
    "VAL_FRAC = 0.10\n",
    "TEST_FRAC= 0.10\n",
    "\n",
    "seq_col   = \"sequence\"\n",
    "cell_col  = \"cellline\"\n",
    "phen_col  = \"condition\"\n",
    "chr_col   = \"chr\"\n",
    "strand_col= \"strand\"\n",
    "target_col= \"log2fc\"\n",
    "\n",
    "# Read & keep only what we need\n",
    "df = pd.read_csv(DATA_CSV, low_memory=False)\n",
    "required_cols = [seq_col, cell_col, phen_col, chr_col, target_col]\n",
    "optional_cols = [strand_col]\n",
    "\n",
    "# Check for required columns\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing expected columns: {missing}. Got: {list(df.columns)}\")\n",
    "\n",
    "# Check for optional columns (strand)\n",
    "has_strand = strand_col in df.columns\n",
    "cols_to_keep = required_cols + ([strand_col] if has_strand else [])\n",
    "\n",
    "df = df[cols_to_keep].copy()\n",
    "df = df.dropna(subset=required_cols)\n",
    "\n",
    "# Clean sequences: uppercase A/C/G/T and enforce length 23\n",
    "df[seq_col] = df[seq_col].astype(str).str.upper().str.strip()\n",
    "df = df[df[seq_col].str.len() == SEQ_LEN]\n",
    "df = df[df[seq_col].str.match(r\"^[ACGT]+$\")]\n",
    "\n",
    "# Factorize categoricals (one-liners)\n",
    "cell_codes, cell_uniques = pd.factorize(df[cell_col].astype(str).str.strip(), sort=True)\n",
    "phen_codes, phen_uniques = pd.factorize(df[phen_col].astype(str).str.strip(), sort=True)\n",
    "# cast chr to string so 10/11/X/Y are handled uniformly\n",
    "chr_codes,  chr_uniques  = pd.factorize(df[chr_col].astype(str).str.strip(),  sort=True)\n",
    "\n",
    "# Factorize strand if available, otherwise create dummy\n",
    "if has_strand:\n",
    "    strand_codes, strand_uniques = pd.factorize(df[strand_col].astype(str).str.strip(), sort=True)\n",
    "    n_strand = len(strand_uniques)\n",
    "else:\n",
    "    # Create dummy strand codes (all zeros) if not available\n",
    "    strand_codes = np.zeros(len(df), dtype=np.int64)\n",
    "    strand_uniques = np.array([\"+\"] if \"+\" in str(df.get(strand_col, \"+\").iloc[0] if len(df) > 0 else \"+\") else [\"+\"])\n",
    "    n_strand = 1\n",
    "\n",
    "n_cell, n_ph, n_chr = len(cell_uniques), len(phen_uniques), len(chr_uniques)\n",
    "\n",
    "# One-hot the 23-mer sequences\n",
    "BASE2IDX = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\n",
    "def onehot_batch(seqs, L=SEQ_LEN):\n",
    "    N = len(seqs)\n",
    "    X = np.zeros((N, 4, L), dtype=np.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        for j, ch in enumerate(s):\n",
    "            X[i, BASE2IDX[ch], j] = 1.0\n",
    "    return X\n",
    "\n",
    "X_seq = onehot_batch(df[seq_col].tolist())\n",
    "X_cell = cell_codes.astype(np.int64)\n",
    "X_ph   = phen_codes.astype(np.int64)\n",
    "X_chr  = chr_codes.astype(np.int64)\n",
    "X_strand = strand_codes.astype(np.int64)\n",
    "y      = df[target_col].astype(np.float32).to_numpy()\n",
    "\n",
    "# Simple random split\n",
    "idx_all = np.arange(len(df))\n",
    "idx_train, idx_test = train_test_split(idx_all, test_size=TEST_FRAC, random_state=42)\n",
    "idx_train, idx_val  = train_test_split(idx_train, test_size=VAL_FRAC/(1-TEST_FRAC), random_state=42)\n",
    "\n",
    "def take(a, idx): return a[idx]\n",
    "Xtr_seq, Xva_seq, Xte_seq = take(X_seq, idx_train), take(X_seq, idx_val), take(X_seq, idx_test)\n",
    "Xtr_cel, Xva_cel, Xte_cel = take(X_cell, idx_train), take(X_cell, idx_val), take(X_cell, idx_test)\n",
    "Xtr_ph,  Xva_ph,  Xte_ph  = take(X_ph,  idx_train), take(X_ph,  idx_val), take(X_ph,  idx_test)\n",
    "Xtr_chr, Xva_chr, Xte_chr = take(X_chr, idx_train), take(X_chr, idx_val), take(X_chr, idx_test)\n",
    "Xtr_str, Xva_str, Xte_str = take(X_strand, idx_train), take(X_strand, idx_val), take(X_strand, idx_test)\n",
    "y_tr,    y_va,    y_te    = take(y,     idx_train), take(y,     idx_val), take(y,     idx_test)\n",
    "\n",
    "# Standardize targets using training set statistics\n",
    "mu = y_tr.mean()\n",
    "sigma = y_tr.std()\n",
    "y_tr_norm = (y_tr - mu) / sigma\n",
    "y_va_norm = (y_va - mu) / sigma\n",
    "y_te_norm = (y_te - mu) / sigma\n",
    "\n",
    "# Create full normalized array for easy indexing\n",
    "y_norm = np.zeros_like(y)\n",
    "y_norm[idx_train] = y_tr_norm\n",
    "y_norm[idx_val] = y_va_norm\n",
    "y_norm[idx_test] = y_te_norm\n",
    "\n",
    "print(f\"train={len(idx_train)}  val={len(idx_val)}  test={len(idx_test)}\")\n",
    "print(f\"cells={n_cell}  phenotypes={n_ph}  chrs={n_chr}  strands={n_strand}\")\n",
    "print(f\"Target stats: mu={mu:.4f}, sigma={sigma:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "292c2f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "params: 1094977\n"
     ]
    }
   ],
   "source": [
    "class EnhancedCrisprCNN(nn.Module):\n",
    "    def __init__(self, base_channels=64, \n",
    "                 n_cell=420, n_phen=34, n_chr=301, n_strand=2,\n",
    "                 emb_dim=32, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Deep Conv2D layers with residual connections\n",
    "        self.conv2d_1 = nn.Sequential(\n",
    "            nn.Conv2d(1, base_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv2d_2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels * 2, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(base_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.2)\n",
    "        )\n",
    "        \n",
    "        # Residual block for conv2d_3\n",
    "        self.conv2d_3_1 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 2, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(base_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.2)\n",
    "        )\n",
    "        self.conv2d_3_2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 2, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(base_channels * 2)\n",
    "        )\n",
    "        \n",
    "        self.conv2d_4 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 2, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(base_channels * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Multi-scale feature extraction with more branches\n",
    "        self.conv_multi1 = nn.Conv2d(base_channels * 2, base_channels, kernel_size=(1, 1))\n",
    "        self.conv_multi2 = nn.Conv2d(base_channels * 2, base_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.conv_multi3 = nn.Conv2d(base_channels * 2, base_channels, kernel_size=(5, 5), padding=(2, 2))\n",
    "        \n",
    "        # After pooling (max + avg for 3 branches): base_channels * 6\n",
    "        seq_feat_dim = base_channels * 6\n",
    "        \n",
    "        # Larger embeddings for categorical features\n",
    "        self.cell_emb = nn.Embedding(n_cell, emb_dim)\n",
    "        self.phen_emb = nn.Embedding(n_phen, emb_dim)\n",
    "        self.chr_emb = nn.Embedding(n_chr, emb_dim)\n",
    "        self.strand_emb = nn.Embedding(n_strand, emb_dim)\n",
    "        \n",
    "        # More feature interaction layers\n",
    "        # Cell-Phenotype interaction\n",
    "        self.cell_phen_interaction = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, emb_dim),\n",
    "            nn.BatchNorm1d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.3)\n",
    "        )\n",
    "        \n",
    "        # Cell-Chromosome interaction\n",
    "        self.cell_chr_interaction = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, emb_dim),\n",
    "            nn.BatchNorm1d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.3)\n",
    "        )\n",
    "        \n",
    "        # Phenotype-Chromosome interaction\n",
    "        self.phen_chr_interaction = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, emb_dim),\n",
    "            nn.BatchNorm1d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.3)\n",
    "        )\n",
    "        \n",
    "        # Triple interaction: Cell-Phenotype-Chromosome\n",
    "        self.triple_interaction = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 3, emb_dim),\n",
    "            nn.BatchNorm1d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.3)\n",
    "        )\n",
    "        \n",
    "        # Dual sequence feature projections for richer representation\n",
    "        self.seq_proj1 = nn.Sequential(\n",
    "            nn.Linear(seq_feat_dim, emb_dim * 2),\n",
    "            nn.BatchNorm1d(emb_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.2)\n",
    "        )\n",
    "        \n",
    "        self.seq_proj2 = nn.Sequential(\n",
    "            nn.Linear(seq_feat_dim, emb_dim * 2),\n",
    "            nn.BatchNorm1d(emb_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.2)\n",
    "        )\n",
    "        \n",
    "        # Combined features: seq + embeddings + interactions\n",
    "        interaction_features = emb_dim * 4  # 4 interaction terms now\n",
    "        # Use dual projected sequence features\n",
    "        total_features = (emb_dim * 4) + (emb_dim * 4) + interaction_features\n",
    "        \n",
    "        # Deep feature fusion layers with residual connections\n",
    "        self.fusion1 = nn.Sequential(\n",
    "            nn.Linear(total_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.fusion2 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.8)\n",
    "        )\n",
    "        \n",
    "        self.fusion3 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.6)\n",
    "        )\n",
    "        \n",
    "        self.fusion4 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.4)\n",
    "        )\n",
    "        \n",
    "        # Final prediction head\n",
    "        self.head = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, seq4x23, cell_idx, phen_idx, chr_idx, strand_idx):\n",
    "        # Reshape for Conv2D: (B, 4, 23) -> (B, 1, 4, 23)\n",
    "        x = seq4x23.unsqueeze(1)  # (B, 1, 4, 23)\n",
    "        \n",
    "        # Deep Conv2D processing\n",
    "        x = self.conv2d_1(x)  # (B, base_channels, 4, 23)\n",
    "        x = self.conv2d_2(x)  # (B, base_channels*2, 4, 23)\n",
    "        \n",
    "        # Residual block for conv2d_3\n",
    "        residual = x\n",
    "        x = self.conv2d_3_1(x)\n",
    "        x = self.conv2d_3_2(x)\n",
    "        x = F.relu(x + residual)  # Residual connection\n",
    "        \n",
    "        x = self.conv2d_4(x)  # (B, base_channels*2, 4, 23)\n",
    "        \n",
    "        # Multi-scale feature extraction with 3 branches\n",
    "        x1 = self.conv_multi1(x)  # (B, base_channels, 4, 23)\n",
    "        x2 = self.conv_multi2(x)  # (B, base_channels, 4, 23)\n",
    "        x3 = self.conv_multi3(x)  # (B, base_channels, 4, 23)\n",
    "        \n",
    "        # Better pooling: both max and avg for all branches\n",
    "        x1_avg = F.adaptive_avg_pool2d(x1, 1).squeeze(-1).squeeze(-1)  # (B, base_channels)\n",
    "        x1_max = F.adaptive_max_pool2d(x1, 1).squeeze(-1).squeeze(-1)  # (B, base_channels)\n",
    "        x2_avg = F.adaptive_avg_pool2d(x2, 1).squeeze(-1).squeeze(-1)  # (B, base_channels)\n",
    "        x2_max = F.adaptive_max_pool2d(x2, 1).squeeze(-1).squeeze(-1)  # (B, base_channels)\n",
    "        x3_avg = F.adaptive_avg_pool2d(x3, 1).squeeze(-1).squeeze(-1)  # (B, base_channels)\n",
    "        x3_max = F.adaptive_max_pool2d(x3, 1).squeeze(-1).squeeze(-1)  # (B, base_channels)\n",
    "        \n",
    "        # Concatenate pooled features\n",
    "        x_seq = torch.cat([x1_avg, x1_max, x2_avg, x2_max, x3_avg, x3_max], dim=1)  # (B, base_channels * 6)\n",
    "        \n",
    "        # Dual sequence feature projections\n",
    "        x_seq_proj1 = self.seq_proj1(x_seq)  # (B, emb_dim * 2)\n",
    "        x_seq_proj2 = self.seq_proj2(x_seq)  # (B, emb_dim * 2)\n",
    "        x_seq_proj = torch.cat([x_seq_proj1, x_seq_proj2], dim=1)  # (B, emb_dim * 4)\n",
    "        \n",
    "        # Categorical embeddings\n",
    "        x_cell = self.cell_emb(cell_idx)    # (B, emb_dim)\n",
    "        x_phen = self.phen_emb(phen_idx)    # (B, emb_dim)\n",
    "        x_chr = self.chr_emb(chr_idx)       # (B, emb_dim)\n",
    "        x_strand = self.strand_emb(strand_idx)  # (B, emb_dim)\n",
    "        \n",
    "        # Feature interactions between categorical embeddings\n",
    "        cell_phen_inter = self.cell_phen_interaction(torch.cat([x_cell, x_phen], dim=1))  # (B, emb_dim)\n",
    "        cell_chr_inter = self.cell_chr_interaction(torch.cat([x_cell, x_chr], dim=1))  # (B, emb_dim)\n",
    "        phen_chr_inter = self.phen_chr_interaction(torch.cat([x_phen, x_chr], dim=1))  # (B, emb_dim)\n",
    "        triple_inter = self.triple_interaction(torch.cat([x_cell, x_phen, x_chr], dim=1))  # (B, emb_dim)\n",
    "        \n",
    "        # Combine all features: dual projected sequence + embeddings + interactions\n",
    "        x = torch.cat([\n",
    "            x_seq_proj,  # Dual projected sequence features\n",
    "            x_cell, x_phen, x_chr, x_strand,  # Original embeddings\n",
    "            cell_phen_inter, cell_chr_inter, phen_chr_inter, triple_inter  # Interaction features\n",
    "        ], dim=1)  # (B, total_features)\n",
    "        \n",
    "        # Feature fusion with residual connections\n",
    "        x = self.fusion1(x)  # (B, 256)\n",
    "        x = self.fusion2(x) + x  # Residual connection (B, 256)\n",
    "        x = self.fusion3(x)  # (B, 128)\n",
    "        x = self.fusion4(x)  # (B, 64)\n",
    "        \n",
    "        # Final prediction\n",
    "        return self.head(x).squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = EnhancedCrisprCNN(\n",
    "    base_channels=64,\n",
    "    n_cell=n_cell, n_phen=n_ph, n_chr=n_chr, n_strand=n_strand,\n",
    "    emb_dim=32,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "print(\"params:\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b9aecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 01/3\n",
      "============================================================\n",
      "  Train: [  100/57525] (  0.2%) | Loss: 1.0528 | Speed: 30.3 batches/s | ETA: 31:34\n",
      "  Train: [  200/57525] (  0.3%) | Loss: 1.0453 | Speed: 40.0 batches/s | ETA: 23:53\n",
      "  Train: [  300/57525] (  0.5%) | Loss: 1.0370 | Speed: 44.4 batches/s | ETA: 21:28\n",
      "  Train: [  400/57525] (  0.7%) | Loss: 1.0217 | Speed: 47.2 batches/s | ETA: 20:09\n",
      "  Train: [  500/57525] (  0.9%) | Loss: 1.0133 | Speed: 49.3 batches/s | ETA: 19:16\n",
      "  Train: [  600/57525] (  1.0%) | Loss: 1.0088 | Speed: 51.5 batches/s | ETA: 18:26\n",
      "  Train: [  700/57525] (  1.2%) | Loss: 1.0107 | Speed: 53.0 batches/s | ETA: 17:52\n",
      "  Train: [  800/57525] (  1.4%) | Loss: 1.0069 | Speed: 53.9 batches/s | ETA: 17:32\n",
      "  Train: [  900/57525] (  1.6%) | Loss: 1.0036 | Speed: 54.6 batches/s | ETA: 17:17\n",
      "  Train: [ 1000/57525] (  1.7%) | Loss: 1.0032 | Speed: 55.2 batches/s | ETA: 17:03\n",
      "  Train: [ 1100/57525] (  1.9%) | Loss: 0.9986 | Speed: 55.8 batches/s | ETA: 16:51\n",
      "  Train: [ 1200/57525] (  2.1%) | Loss: 0.9957 | Speed: 55.9 batches/s | ETA: 16:47\n",
      "  Train: [ 1300/57525] (  2.3%) | Loss: 0.9922 | Speed: 56.3 batches/s | ETA: 16:39\n",
      "  Train: [ 1400/57525] (  2.4%) | Loss: 0.9911 | Speed: 56.5 batches/s | ETA: 16:33\n",
      "  Train: [ 1500/57525] (  2.6%) | Loss: 0.9898 | Speed: 56.8 batches/s | ETA: 16:26\n",
      "  Train: [ 1600/57525] (  2.8%) | Loss: 0.9863 | Speed: 57.0 batches/s | ETA: 16:20\n",
      "  Train: [ 1700/57525] (  3.0%) | Loss: 0.9835 | Speed: 57.3 batches/s | ETA: 16:14\n",
      "  Train: [ 1800/57525] (  3.1%) | Loss: 0.9792 | Speed: 57.4 batches/s | ETA: 16:11\n",
      "  Train: [ 1900/57525] (  3.3%) | Loss: 0.9779 | Speed: 57.5 batches/s | ETA: 16:07\n",
      "  Train: [ 2000/57525] (  3.5%) | Loss: 0.9761 | Speed: 57.5 batches/s | ETA: 16:05\n",
      "  Train: [ 2100/57525] (  3.7%) | Loss: 0.9740 | Speed: 57.6 batches/s | ETA: 16:01\n",
      "  Train: [ 2200/57525] (  3.8%) | Loss: 0.9727 | Speed: 57.8 batches/s | ETA: 15:57\n",
      "  Train: [ 2300/57525] (  4.0%) | Loss: 0.9694 | Speed: 57.8 batches/s | ETA: 15:55\n",
      "  Train: [ 2400/57525] (  4.2%) | Loss: 0.9678 | Speed: 57.9 batches/s | ETA: 15:51\n",
      "  Train: [ 2500/57525] (  4.3%) | Loss: 0.9663 | Speed: 58.1 batches/s | ETA: 15:47\n",
      "  Train: [ 2600/57525] (  4.5%) | Loss: 0.9649 | Speed: 58.2 batches/s | ETA: 15:43\n",
      "  Train: [ 2700/57525] (  4.7%) | Loss: 0.9633 | Speed: 58.3 batches/s | ETA: 15:39\n",
      "  Train: [ 2800/57525] (  4.9%) | Loss: 0.9620 | Speed: 58.4 batches/s | ETA: 15:37\n",
      "  Train: [ 2900/57525] (  5.0%) | Loss: 0.9597 | Speed: 58.5 batches/s | ETA: 15:33\n",
      "  Train: [ 3000/57525] (  5.2%) | Loss: 0.9580 | Speed: 58.7 batches/s | ETA: 15:29\n",
      "  Train: [ 3100/57525] (  5.4%) | Loss: 0.9561 | Speed: 58.8 batches/s | ETA: 15:25\n",
      "  Train: [ 3200/57525] (  5.6%) | Loss: 0.9546 | Speed: 58.9 batches/s | ETA: 15:22\n",
      "  Train: [ 3300/57525] (  5.7%) | Loss: 0.9528 | Speed: 59.0 batches/s | ETA: 15:19\n",
      "  Train: [ 3400/57525] (  5.9%) | Loss: 0.9514 | Speed: 59.1 batches/s | ETA: 15:16\n",
      "  Train: [ 3500/57525] (  6.1%) | Loss: 0.9498 | Speed: 59.2 batches/s | ETA: 15:13\n",
      "  Train: [ 3600/57525] (  6.3%) | Loss: 0.9487 | Speed: 59.2 batches/s | ETA: 15:10\n",
      "  Train: [ 3700/57525] (  6.4%) | Loss: 0.9477 | Speed: 59.3 batches/s | ETA: 15:07\n",
      "  Train: [ 3800/57525] (  6.6%) | Loss: 0.9458 | Speed: 59.3 batches/s | ETA: 15:05\n",
      "  Train: [ 3900/57525] (  6.8%) | Loss: 0.9446 | Speed: 59.4 batches/s | ETA: 15:02\n",
      "  Train: [ 4000/57525] (  7.0%) | Loss: 0.9431 | Speed: 59.5 batches/s | ETA: 14:59\n",
      "  Train: [ 4100/57525] (  7.1%) | Loss: 0.9429 | Speed: 59.6 batches/s | ETA: 14:56\n",
      "  Train: [ 4200/57525] (  7.3%) | Loss: 0.9423 | Speed: 59.6 batches/s | ETA: 14:54\n",
      "  Train: [ 4300/57525] (  7.5%) | Loss: 0.9409 | Speed: 59.7 batches/s | ETA: 14:51\n",
      "  Train: [ 4400/57525] (  7.6%) | Loss: 0.9404 | Speed: 59.8 batches/s | ETA: 14:48\n",
      "  Train: [ 4500/57525] (  7.8%) | Loss: 0.9395 | Speed: 59.9 batches/s | ETA: 14:45\n",
      "  Train: [ 4600/57525] (  8.0%) | Loss: 0.9386 | Speed: 59.9 batches/s | ETA: 14:43\n",
      "  Train: [ 4700/57525] (  8.2%) | Loss: 0.9381 | Speed: 59.9 batches/s | ETA: 14:41\n",
      "  Train: [ 4800/57525] (  8.3%) | Loss: 0.9372 | Speed: 60.0 batches/s | ETA: 14:38\n",
      "  Train: [ 4900/57525] (  8.5%) | Loss: 0.9359 | Speed: 60.0 batches/s | ETA: 14:36\n",
      "  Train: [ 5000/57525] (  8.7%) | Loss: 0.9352 | Speed: 60.1 batches/s | ETA: 14:34\n",
      "  Train: [ 5100/57525] (  8.9%) | Loss: 0.9341 | Speed: 60.1 batches/s | ETA: 14:32\n",
      "  Train: [ 5200/57525] (  9.0%) | Loss: 0.9327 | Speed: 60.1 batches/s | ETA: 14:30\n",
      "  Train: [ 5300/57525] (  9.2%) | Loss: 0.9322 | Speed: 60.2 batches/s | ETA: 14:27\n",
      "  Train: [ 5400/57525] (  9.4%) | Loss: 0.9315 | Speed: 60.2 batches/s | ETA: 14:25\n",
      "  Train: [ 5500/57525] (  9.6%) | Loss: 0.9312 | Speed: 60.2 batches/s | ETA: 14:23\n",
      "  Train: [ 5600/57525] (  9.7%) | Loss: 0.9299 | Speed: 60.3 batches/s | ETA: 14:21\n",
      "  Train: [ 5700/57525] (  9.9%) | Loss: 0.9288 | Speed: 60.3 batches/s | ETA: 14:19\n",
      "  Train: [ 5800/57525] ( 10.1%) | Loss: 0.9278 | Speed: 60.3 batches/s | ETA: 14:17\n",
      "  Train: [ 5900/57525] ( 10.3%) | Loss: 0.9270 | Speed: 60.4 batches/s | ETA: 14:15\n",
      "  Train: [ 6000/57525] ( 10.4%) | Loss: 0.9259 | Speed: 60.4 batches/s | ETA: 14:13\n",
      "  Train: [ 6100/57525] ( 10.6%) | Loss: 0.9252 | Speed: 60.4 batches/s | ETA: 14:10\n",
      "  Train: [ 6200/57525] ( 10.8%) | Loss: 0.9240 | Speed: 60.5 batches/s | ETA: 14:08\n",
      "  Train: [ 6300/57525] ( 11.0%) | Loss: 0.9231 | Speed: 60.5 batches/s | ETA: 14:06\n",
      "  Train: [ 6400/57525] ( 11.1%) | Loss: 0.9223 | Speed: 60.5 batches/s | ETA: 14:04\n",
      "  Train: [ 6500/57525] ( 11.3%) | Loss: 0.9216 | Speed: 60.5 batches/s | ETA: 14:02\n",
      "  Train: [ 6600/57525] ( 11.5%) | Loss: 0.9209 | Speed: 60.6 batches/s | ETA: 14:00\n",
      "  Train: [ 6700/57525] ( 11.6%) | Loss: 0.9200 | Speed: 60.6 batches/s | ETA: 13:58\n",
      "  Train: [ 6800/57525] ( 11.8%) | Loss: 0.9193 | Speed: 60.6 batches/s | ETA: 13:56\n",
      "  Train: [ 6900/57525] ( 12.0%) | Loss: 0.9184 | Speed: 60.7 batches/s | ETA: 13:54\n",
      "  Train: [ 7000/57525] ( 12.2%) | Loss: 0.9178 | Speed: 60.7 batches/s | ETA: 13:52\n",
      "  Train: [ 7100/57525] ( 12.3%) | Loss: 0.9169 | Speed: 60.7 batches/s | ETA: 13:50\n",
      "  Train: [ 7200/57525] ( 12.5%) | Loss: 0.9164 | Speed: 60.8 batches/s | ETA: 13:48\n",
      "  Train: [ 7300/57525] ( 12.7%) | Loss: 0.9156 | Speed: 60.8 batches/s | ETA: 13:46\n",
      "  Train: [ 7400/57525] ( 12.9%) | Loss: 0.9151 | Speed: 60.8 batches/s | ETA: 13:44\n",
      "  Train: [ 7500/57525] ( 13.0%) | Loss: 0.9142 | Speed: 60.8 batches/s | ETA: 13:42\n",
      "  Train: [ 7600/57525] ( 13.2%) | Loss: 0.9133 | Speed: 60.8 batches/s | ETA: 13:40\n",
      "  Train: [ 7700/57525] ( 13.4%) | Loss: 0.9125 | Speed: 60.9 batches/s | ETA: 13:38\n",
      "  Train: [ 7800/57525] ( 13.6%) | Loss: 0.9118 | Speed: 60.9 batches/s | ETA: 13:36\n",
      "  Train: [ 7900/57525] ( 13.7%) | Loss: 0.9106 | Speed: 60.9 batches/s | ETA: 13:34\n",
      "  Train: [ 8000/57525] ( 13.9%) | Loss: 0.9097 | Speed: 60.9 batches/s | ETA: 13:32\n",
      "  Train: [ 8100/57525] ( 14.1%) | Loss: 0.9092 | Speed: 60.9 batches/s | ETA: 13:31\n",
      "  Train: [ 8200/57525] ( 14.3%) | Loss: 0.9086 | Speed: 60.9 batches/s | ETA: 13:29\n",
      "  Train: [ 8300/57525] ( 14.4%) | Loss: 0.9081 | Speed: 61.0 batches/s | ETA: 13:27\n",
      "  Train: [ 8400/57525] ( 14.6%) | Loss: 0.9075 | Speed: 61.0 batches/s | ETA: 13:25\n",
      "  Train: [ 8500/57525] ( 14.8%) | Loss: 0.9066 | Speed: 61.0 batches/s | ETA: 13:23\n",
      "  Train: [ 8600/57525] ( 15.0%) | Loss: 0.9057 | Speed: 61.0 batches/s | ETA: 13:21\n",
      "  Train: [ 8700/57525] ( 15.1%) | Loss: 0.9048 | Speed: 61.0 batches/s | ETA: 13:19\n",
      "  Train: [ 8800/57525] ( 15.3%) | Loss: 0.9042 | Speed: 61.1 batches/s | ETA: 13:17\n",
      "  Train: [ 8900/57525] ( 15.5%) | Loss: 0.9031 | Speed: 61.1 batches/s | ETA: 13:15\n",
      "  Train: [ 9000/57525] ( 15.6%) | Loss: 0.9024 | Speed: 61.1 batches/s | ETA: 13:14\n",
      "  Train: [ 9100/57525] ( 15.8%) | Loss: 0.9017 | Speed: 61.1 batches/s | ETA: 13:12\n",
      "  Train: [ 9200/57525] ( 16.0%) | Loss: 0.9007 | Speed: 61.1 batches/s | ETA: 13:10\n",
      "  Train: [ 9300/57525] ( 16.2%) | Loss: 0.9000 | Speed: 61.2 batches/s | ETA: 13:08\n",
      "  Train: [ 9400/57525] ( 16.3%) | Loss: 0.8993 | Speed: 61.2 batches/s | ETA: 13:06\n",
      "  Train: [ 9500/57525] ( 16.5%) | Loss: 0.8987 | Speed: 61.2 batches/s | ETA: 13:04\n",
      "  Train: [ 9600/57525] ( 16.7%) | Loss: 0.8978 | Speed: 61.2 batches/s | ETA: 13:03\n",
      "  Train: [ 9700/57525] ( 16.9%) | Loss: 0.8971 | Speed: 61.2 batches/s | ETA: 13:01\n",
      "  Train: [ 9800/57525] ( 17.0%) | Loss: 0.8965 | Speed: 61.2 batches/s | ETA: 12:59\n",
      "  Train: [ 9900/57525] ( 17.2%) | Loss: 0.8959 | Speed: 61.2 batches/s | ETA: 12:58\n",
      "  Train: [10000/57525] ( 17.4%) | Loss: 0.8951 | Speed: 61.2 batches/s | ETA: 12:56\n",
      "  Train: [10100/57525] ( 17.6%) | Loss: 0.8943 | Speed: 61.2 batches/s | ETA: 12:54\n",
      "  Train: [10200/57525] ( 17.7%) | Loss: 0.8937 | Speed: 61.2 batches/s | ETA: 12:52\n",
      "  Train: [10300/57525] ( 17.9%) | Loss: 0.8928 | Speed: 61.3 batches/s | ETA: 12:50\n",
      "  Train: [10400/57525] ( 18.1%) | Loss: 0.8924 | Speed: 61.3 batches/s | ETA: 12:49\n",
      "  Train: [10500/57525] ( 18.3%) | Loss: 0.8915 | Speed: 61.3 batches/s | ETA: 12:47\n",
      "  Train: [10600/57525] ( 18.4%) | Loss: 0.8909 | Speed: 61.3 batches/s | ETA: 12:45\n",
      "  Train: [10700/57525] ( 18.6%) | Loss: 0.8902 | Speed: 61.3 batches/s | ETA: 12:43\n",
      "  Train: [10800/57525] ( 18.8%) | Loss: 0.8894 | Speed: 61.4 batches/s | ETA: 12:41\n",
      "  Train: [10900/57525] ( 18.9%) | Loss: 0.8888 | Speed: 61.4 batches/s | ETA: 12:39\n",
      "  Train: [11000/57525] ( 19.1%) | Loss: 0.8880 | Speed: 61.3 batches/s | ETA: 12:38\n",
      "  Train: [11100/57525] ( 19.3%) | Loss: 0.8873 | Speed: 61.3 batches/s | ETA: 12:37\n",
      "  Train: [11200/57525] ( 19.5%) | Loss: 0.8866 | Speed: 61.3 batches/s | ETA: 12:36\n",
      "  Train: [11300/57525] ( 19.6%) | Loss: 0.8859 | Speed: 61.3 batches/s | ETA: 12:34\n",
      "  Train: [11400/57525] ( 19.8%) | Loss: 0.8852 | Speed: 61.3 batches/s | ETA: 12:32\n",
      "  Train: [11500/57525] ( 20.0%) | Loss: 0.8845 | Speed: 61.3 batches/s | ETA: 12:30\n",
      "  Train: [11600/57525] ( 20.2%) | Loss: 0.8836 | Speed: 61.3 batches/s | ETA: 12:29\n",
      "  Train: [11700/57525] ( 20.3%) | Loss: 0.8829 | Speed: 61.3 batches/s | ETA: 12:27\n",
      "  Train: [11800/57525] ( 20.5%) | Loss: 0.8820 | Speed: 61.4 batches/s | ETA: 12:25\n",
      "  Train: [11900/57525] ( 20.7%) | Loss: 0.8814 | Speed: 61.4 batches/s | ETA: 12:23\n",
      "  Train: [12000/57525] ( 20.9%) | Loss: 0.8807 | Speed: 61.4 batches/s | ETA: 12:21\n",
      "  Train: [12100/57525] ( 21.0%) | Loss: 0.8800 | Speed: 61.4 batches/s | ETA: 12:19\n",
      "  Train: [12200/57525] ( 21.2%) | Loss: 0.8793 | Speed: 61.4 batches/s | ETA: 12:18\n",
      "  Train: [12300/57525] ( 21.4%) | Loss: 0.8788 | Speed: 61.4 batches/s | ETA: 12:16\n",
      "  Train: [12400/57525] ( 21.6%) | Loss: 0.8782 | Speed: 61.4 batches/s | ETA: 12:14\n",
      "  Train: [12500/57525] ( 21.7%) | Loss: 0.8775 | Speed: 61.4 batches/s | ETA: 12:13\n",
      "  Train: [12600/57525] ( 21.9%) | Loss: 0.8769 | Speed: 61.4 batches/s | ETA: 12:11\n",
      "  Train: [12700/57525] ( 22.1%) | Loss: 0.8763 | Speed: 61.4 batches/s | ETA: 12:09\n",
      "  Train: [12800/57525] ( 22.3%) | Loss: 0.8754 | Speed: 61.4 batches/s | ETA: 12:08\n",
      "  Train: [12900/57525] ( 22.4%) | Loss: 0.8746 | Speed: 61.4 batches/s | ETA: 12:06\n",
      "  Train: [13000/57525] ( 22.6%) | Loss: 0.8741 | Speed: 61.4 batches/s | ETA: 12:05\n",
      "  Train: [13100/57525] ( 22.8%) | Loss: 0.8736 | Speed: 61.3 batches/s | ETA: 12:04\n",
      "  Train: [13200/57525] ( 22.9%) | Loss: 0.8728 | Speed: 61.3 batches/s | ETA: 12:03\n",
      "  Train: [13300/57525] ( 23.1%) | Loss: 0.8721 | Speed: 61.2 batches/s | ETA: 12:02\n",
      "  Train: [13400/57525] ( 23.3%) | Loss: 0.8713 | Speed: 61.2 batches/s | ETA: 12:01\n",
      "  Train: [13500/57525] ( 23.5%) | Loss: 0.8706 | Speed: 61.2 batches/s | ETA: 11:59\n",
      "  Train: [13600/57525] ( 23.6%) | Loss: 0.8700 | Speed: 61.1 batches/s | ETA: 11:58\n",
      "  Train: [13700/57525] ( 23.8%) | Loss: 0.8693 | Speed: 61.1 batches/s | ETA: 11:57\n",
      "  Train: [13800/57525] ( 24.0%) | Loss: 0.8689 | Speed: 61.1 batches/s | ETA: 11:56\n",
      "  Train: [13900/57525] ( 24.2%) | Loss: 0.8681 | Speed: 61.0 batches/s | ETA: 11:54\n",
      "  Train: [14000/57525] ( 24.3%) | Loss: 0.8675 | Speed: 61.0 batches/s | ETA: 11:53\n",
      "  Train: [14100/57525] ( 24.5%) | Loss: 0.8668 | Speed: 61.0 batches/s | ETA: 11:52\n",
      "  Train: [14200/57525] ( 24.7%) | Loss: 0.8662 | Speed: 61.0 batches/s | ETA: 11:50\n",
      "  Train: [14300/57525] ( 24.9%) | Loss: 0.8653 | Speed: 61.0 batches/s | ETA: 11:49\n",
      "  Train: [14400/57525] ( 25.0%) | Loss: 0.8648 | Speed: 61.0 batches/s | ETA: 11:47\n",
      "  Train: [14500/57525] ( 25.2%) | Loss: 0.8641 | Speed: 60.9 batches/s | ETA: 11:45\n",
      "  Train: [14600/57525] ( 25.4%) | Loss: 0.8635 | Speed: 60.9 batches/s | ETA: 11:44\n",
      "  Train: [14700/57525] ( 25.6%) | Loss: 0.8629 | Speed: 60.9 batches/s | ETA: 11:42\n",
      "  Train: [14800/57525] ( 25.7%) | Loss: 0.8622 | Speed: 60.9 batches/s | ETA: 11:41\n",
      "  Train: [14900/57525] ( 25.9%) | Loss: 0.8616 | Speed: 60.9 batches/s | ETA: 11:39\n",
      "  Train: [15000/57525] ( 26.1%) | Loss: 0.8611 | Speed: 60.9 batches/s | ETA: 11:38\n",
      "  Train: [15100/57525] ( 26.2%) | Loss: 0.8603 | Speed: 60.9 batches/s | ETA: 11:36\n",
      "  Train: [15200/57525] ( 26.4%) | Loss: 0.8597 | Speed: 60.9 batches/s | ETA: 11:35\n",
      "  Train: [15300/57525] ( 26.6%) | Loss: 0.8594 | Speed: 60.9 batches/s | ETA: 11:33\n",
      "  Train: [15400/57525] ( 26.8%) | Loss: 0.8588 | Speed: 60.9 batches/s | ETA: 11:31\n",
      "  Train: [15500/57525] ( 26.9%) | Loss: 0.8581 | Speed: 60.9 batches/s | ETA: 11:30\n",
      "  Train: [15600/57525] ( 27.1%) | Loss: 0.8577 | Speed: 60.9 batches/s | ETA: 11:28\n",
      "  Train: [15700/57525] ( 27.3%) | Loss: 0.8570 | Speed: 60.9 batches/s | ETA: 11:27\n",
      "  Train: [15800/57525] ( 27.5%) | Loss: 0.8565 | Speed: 60.8 batches/s | ETA: 11:25\n",
      "  Train: [15900/57525] ( 27.6%) | Loss: 0.8561 | Speed: 60.8 batches/s | ETA: 11:24\n",
      "  Train: [16000/57525] ( 27.8%) | Loss: 0.8555 | Speed: 60.8 batches/s | ETA: 11:22\n",
      "  Train: [16100/57525] ( 28.0%) | Loss: 0.8549 | Speed: 60.8 batches/s | ETA: 11:21\n",
      "  Train: [16200/57525] ( 28.2%) | Loss: 0.8542 | Speed: 60.8 batches/s | ETA: 11:19\n",
      "  Train: [16300/57525] ( 28.3%) | Loss: 0.8538 | Speed: 60.8 batches/s | ETA: 11:17\n",
      "  Train: [16400/57525] ( 28.5%) | Loss: 0.8532 | Speed: 60.8 batches/s | ETA: 11:16\n",
      "  Train: [16500/57525] ( 28.7%) | Loss: 0.8526 | Speed: 60.8 batches/s | ETA: 11:14\n",
      "  Train: [16600/57525] ( 28.9%) | Loss: 0.8518 | Speed: 60.8 batches/s | ETA: 11:13\n",
      "  Train: [16700/57525] ( 29.0%) | Loss: 0.8513 | Speed: 60.8 batches/s | ETA: 11:11\n",
      "  Train: [16800/57525] ( 29.2%) | Loss: 0.8506 | Speed: 60.8 batches/s | ETA: 11:10\n",
      "  Train: [16900/57525] ( 29.4%) | Loss: 0.8499 | Speed: 60.8 batches/s | ETA: 11:08\n",
      "  Train: [17000/57525] ( 29.6%) | Loss: 0.8493 | Speed: 60.7 batches/s | ETA: 11:07\n",
      "  Train: [17100/57525] ( 29.7%) | Loss: 0.8488 | Speed: 60.7 batches/s | ETA: 11:05\n",
      "  Train: [17200/57525] ( 29.9%) | Loss: 0.8482 | Speed: 60.7 batches/s | ETA: 11:04\n",
      "  Train: [17300/57525] ( 30.1%) | Loss: 0.8476 | Speed: 60.7 batches/s | ETA: 11:02\n",
      "  Train: [17400/57525] ( 30.2%) | Loss: 0.8471 | Speed: 60.7 batches/s | ETA: 11:00\n",
      "  Train: [17500/57525] ( 30.4%) | Loss: 0.8465 | Speed: 60.7 batches/s | ETA: 10:59\n",
      "  Train: [17600/57525] ( 30.6%) | Loss: 0.8461 | Speed: 60.7 batches/s | ETA: 10:57\n",
      "  Train: [17700/57525] ( 30.8%) | Loss: 0.8455 | Speed: 60.7 batches/s | ETA: 10:56\n",
      "  Train: [17800/57525] ( 30.9%) | Loss: 0.8451 | Speed: 60.7 batches/s | ETA: 10:54\n",
      "  Train: [17900/57525] ( 31.1%) | Loss: 0.8445 | Speed: 60.6 batches/s | ETA: 10:53\n",
      "  Train: [18000/57525] ( 31.3%) | Loss: 0.8440 | Speed: 60.6 batches/s | ETA: 10:52\n",
      "  Train: [18100/57525] ( 31.5%) | Loss: 0.8433 | Speed: 60.6 batches/s | ETA: 10:50\n",
      "  Train: [18200/57525] ( 31.6%) | Loss: 0.8428 | Speed: 60.5 batches/s | ETA: 10:49\n",
      "  Train: [18300/57525] ( 31.8%) | Loss: 0.8423 | Speed: 60.5 batches/s | ETA: 10:48\n",
      "  Train: [18400/57525] ( 32.0%) | Loss: 0.8418 | Speed: 60.5 batches/s | ETA: 10:46\n",
      "  Train: [18500/57525] ( 32.2%) | Loss: 0.8412 | Speed: 60.5 batches/s | ETA: 10:45\n",
      "  Train: [18600/57525] ( 32.3%) | Loss: 0.8407 | Speed: 60.5 batches/s | ETA: 10:43\n",
      "  Train: [18700/57525] ( 32.5%) | Loss: 0.8401 | Speed: 60.5 batches/s | ETA: 10:41\n",
      "  Train: [18800/57525] ( 32.7%) | Loss: 0.8396 | Speed: 60.5 batches/s | ETA: 10:40\n",
      "  Train: [18900/57525] ( 32.9%) | Loss: 0.8390 | Speed: 60.5 batches/s | ETA: 10:38\n",
      "  Train: [19000/57525] ( 33.0%) | Loss: 0.8385 | Speed: 60.5 batches/s | ETA: 10:37\n",
      "  Train: [19100/57525] ( 33.2%) | Loss: 0.8382 | Speed: 60.5 batches/s | ETA: 10:35\n",
      "  Train: [19200/57525] ( 33.4%) | Loss: 0.8377 | Speed: 60.4 batches/s | ETA: 10:34\n",
      "  Train: [19300/57525] ( 33.6%) | Loss: 0.8373 | Speed: 60.4 batches/s | ETA: 10:32\n",
      "  Train: [19400/57525] ( 33.7%) | Loss: 0.8368 | Speed: 60.4 batches/s | ETA: 10:30\n",
      "  Train: [19500/57525] ( 33.9%) | Loss: 0.8363 | Speed: 60.4 batches/s | ETA: 10:29\n",
      "  Train: [19600/57525] ( 34.1%) | Loss: 0.8359 | Speed: 60.4 batches/s | ETA: 10:27\n",
      "  Train: [19700/57525] ( 34.2%) | Loss: 0.8354 | Speed: 60.4 batches/s | ETA: 10:26\n",
      "  Train: [19800/57525] ( 34.4%) | Loss: 0.8348 | Speed: 60.4 batches/s | ETA: 10:24\n",
      "  Train: [19900/57525] ( 34.6%) | Loss: 0.8343 | Speed: 60.4 batches/s | ETA: 10:22\n",
      "  Train: [20000/57525] ( 34.8%) | Loss: 0.8338 | Speed: 60.4 batches/s | ETA: 10:21\n",
      "  Train: [20100/57525] ( 34.9%) | Loss: 0.8333 | Speed: 60.4 batches/s | ETA: 10:19\n",
      "  Train: [20200/57525] ( 35.1%) | Loss: 0.8329 | Speed: 60.4 batches/s | ETA: 10:18\n",
      "  Train: [20300/57525] ( 35.3%) | Loss: 0.8325 | Speed: 60.4 batches/s | ETA: 10:16\n",
      "  Train: [20400/57525] ( 35.5%) | Loss: 0.8320 | Speed: 60.4 batches/s | ETA: 10:14\n",
      "  Train: [20500/57525] ( 35.6%) | Loss: 0.8316 | Speed: 60.4 batches/s | ETA: 10:13\n",
      "  Train: [20600/57525] ( 35.8%) | Loss: 0.8313 | Speed: 60.4 batches/s | ETA: 10:11\n",
      "  Train: [20700/57525] ( 36.0%) | Loss: 0.8308 | Speed: 60.3 batches/s | ETA: 10:10\n",
      "  Train: [20800/57525] ( 36.2%) | Loss: 0.8303 | Speed: 60.3 batches/s | ETA: 10:08\n",
      "  Train: [20900/57525] ( 36.3%) | Loss: 0.8297 | Speed: 60.3 batches/s | ETA: 10:07\n",
      "  Train: [21000/57525] ( 36.5%) | Loss: 0.8292 | Speed: 60.3 batches/s | ETA: 10:05\n",
      "  Train: [21100/57525] ( 36.7%) | Loss: 0.8288 | Speed: 60.3 batches/s | ETA: 10:03\n",
      "  Train: [21200/57525] ( 36.9%) | Loss: 0.8284 | Speed: 60.3 batches/s | ETA: 10:02\n",
      "  Train: [21300/57525] ( 37.0%) | Loss: 0.8278 | Speed: 60.3 batches/s | ETA: 10:00\n",
      "  Train: [21400/57525] ( 37.2%) | Loss: 0.8273 | Speed: 60.3 batches/s | ETA: 09:59\n",
      "  Train: [21500/57525] ( 37.4%) | Loss: 0.8268 | Speed: 60.3 batches/s | ETA: 09:57\n",
      "  Train: [21600/57525] ( 37.5%) | Loss: 0.8264 | Speed: 60.3 batches/s | ETA: 09:56\n",
      "  Train: [21700/57525] ( 37.7%) | Loss: 0.8259 | Speed: 60.3 batches/s | ETA: 09:54\n",
      "  Train: [21800/57525] ( 37.9%) | Loss: 0.8255 | Speed: 60.3 batches/s | ETA: 09:52\n",
      "  Train: [21900/57525] ( 38.1%) | Loss: 0.8250 | Speed: 60.2 batches/s | ETA: 09:51\n",
      "  Train: [22000/57525] ( 38.2%) | Loss: 0.8246 | Speed: 60.2 batches/s | ETA: 09:49\n",
      "  Train: [22100/57525] ( 38.4%) | Loss: 0.8242 | Speed: 60.2 batches/s | ETA: 09:48\n",
      "  Train: [22200/57525] ( 38.6%) | Loss: 0.8238 | Speed: 60.2 batches/s | ETA: 09:46\n",
      "  Train: [22300/57525] ( 38.8%) | Loss: 0.8233 | Speed: 60.2 batches/s | ETA: 09:44\n",
      "  Train: [22400/57525] ( 38.9%) | Loss: 0.8229 | Speed: 60.2 batches/s | ETA: 09:43\n",
      "  Train: [22500/57525] ( 39.1%) | Loss: 0.8225 | Speed: 60.2 batches/s | ETA: 09:41\n",
      "  Train: [22600/57525] ( 39.3%) | Loss: 0.8222 | Speed: 60.2 batches/s | ETA: 09:40\n",
      "  Train: [22700/57525] ( 39.5%) | Loss: 0.8218 | Speed: 60.2 batches/s | ETA: 09:38\n",
      "  Train: [22800/57525] ( 39.6%) | Loss: 0.8212 | Speed: 60.2 batches/s | ETA: 09:37\n",
      "  Train: [22900/57525] ( 39.8%) | Loss: 0.8207 | Speed: 60.2 batches/s | ETA: 09:35\n",
      "  Train: [23000/57525] ( 40.0%) | Loss: 0.8203 | Speed: 60.2 batches/s | ETA: 09:33\n",
      "  Train: [23100/57525] ( 40.2%) | Loss: 0.8199 | Speed: 60.2 batches/s | ETA: 09:32\n",
      "  Train: [23200/57525] ( 40.3%) | Loss: 0.8195 | Speed: 60.2 batches/s | ETA: 09:30\n",
      "  Train: [23300/57525] ( 40.5%) | Loss: 0.8189 | Speed: 60.2 batches/s | ETA: 09:28\n",
      "  Train: [23400/57525] ( 40.7%) | Loss: 0.8185 | Speed: 60.2 batches/s | ETA: 09:27\n",
      "  Train: [23500/57525] ( 40.9%) | Loss: 0.8180 | Speed: 60.1 batches/s | ETA: 09:25\n",
      "  Train: [23600/57525] ( 41.0%) | Loss: 0.8176 | Speed: 60.1 batches/s | ETA: 09:24\n",
      "  Train: [23700/57525] ( 41.2%) | Loss: 0.8173 | Speed: 60.1 batches/s | ETA: 09:22\n",
      "  Train: [23800/57525] ( 41.4%) | Loss: 0.8170 | Speed: 60.1 batches/s | ETA: 09:20\n",
      "  Train: [23900/57525] ( 41.5%) | Loss: 0.8165 | Speed: 60.1 batches/s | ETA: 09:19\n",
      "  Train: [24000/57525] ( 41.7%) | Loss: 0.8161 | Speed: 60.1 batches/s | ETA: 09:17\n",
      "  Train: [24100/57525] ( 41.9%) | Loss: 0.8156 | Speed: 60.1 batches/s | ETA: 09:16\n",
      "  Train: [24200/57525] ( 42.1%) | Loss: 0.8152 | Speed: 60.1 batches/s | ETA: 09:14\n",
      "  Train: [24300/57525] ( 42.2%) | Loss: 0.8148 | Speed: 60.1 batches/s | ETA: 09:12\n",
      "  Train: [24400/57525] ( 42.4%) | Loss: 0.8144 | Speed: 60.1 batches/s | ETA: 09:11\n",
      "  Train: [24500/57525] ( 42.6%) | Loss: 0.8139 | Speed: 60.1 batches/s | ETA: 09:09\n",
      "  Train: [24600/57525] ( 42.8%) | Loss: 0.8135 | Speed: 60.1 batches/s | ETA: 09:07\n",
      "  Train: [24700/57525] ( 42.9%) | Loss: 0.8130 | Speed: 60.1 batches/s | ETA: 09:06\n",
      "  Train: [24800/57525] ( 43.1%) | Loss: 0.8127 | Speed: 60.1 batches/s | ETA: 09:04\n",
      "  Train: [24900/57525] ( 43.3%) | Loss: 0.8123 | Speed: 60.1 batches/s | ETA: 09:02\n",
      "  Train: [25000/57525] ( 43.5%) | Loss: 0.8119 | Speed: 60.1 batches/s | ETA: 09:01\n",
      "  Train: [25100/57525] ( 43.6%) | Loss: 0.8115 | Speed: 60.1 batches/s | ETA: 08:59\n",
      "  Train: [25200/57525] ( 43.8%) | Loss: 0.8112 | Speed: 60.1 batches/s | ETA: 08:58\n",
      "  Train: [25300/57525] ( 44.0%) | Loss: 0.8109 | Speed: 60.1 batches/s | ETA: 08:56\n",
      "  Train: [25400/57525] ( 44.2%) | Loss: 0.8104 | Speed: 60.0 batches/s | ETA: 08:55\n",
      "  Train: [25500/57525] ( 44.3%) | Loss: 0.8101 | Speed: 60.0 batches/s | ETA: 08:53\n",
      "  Train: [25600/57525] ( 44.5%) | Loss: 0.8098 | Speed: 60.0 batches/s | ETA: 08:51\n",
      "  Train: [25700/57525] ( 44.7%) | Loss: 0.8095 | Speed: 60.0 batches/s | ETA: 08:50\n",
      "  Train: [25800/57525] ( 44.9%) | Loss: 0.8091 | Speed: 60.0 batches/s | ETA: 08:48\n",
      "  Train: [25900/57525] ( 45.0%) | Loss: 0.8088 | Speed: 60.0 batches/s | ETA: 08:47\n",
      "  Train: [26000/57525] ( 45.2%) | Loss: 0.8084 | Speed: 60.0 batches/s | ETA: 08:45\n",
      "  Train: [26100/57525] ( 45.4%) | Loss: 0.8080 | Speed: 60.0 batches/s | ETA: 08:43\n",
      "  Train: [26200/57525] ( 45.5%) | Loss: 0.8077 | Speed: 60.0 batches/s | ETA: 08:42\n",
      "  Train: [26300/57525] ( 45.7%) | Loss: 0.8074 | Speed: 60.0 batches/s | ETA: 08:40\n",
      "  Train: [26400/57525] ( 45.9%) | Loss: 0.8071 | Speed: 60.0 batches/s | ETA: 08:38\n",
      "  Train: [26500/57525] ( 46.1%) | Loss: 0.8068 | Speed: 60.0 batches/s | ETA: 08:37\n",
      "  Train: [26600/57525] ( 46.2%) | Loss: 0.8064 | Speed: 60.0 batches/s | ETA: 08:35\n",
      "  Train: [26700/57525] ( 46.4%) | Loss: 0.8061 | Speed: 60.0 batches/s | ETA: 08:33\n",
      "  Train: [26800/57525] ( 46.6%) | Loss: 0.8058 | Speed: 60.0 batches/s | ETA: 08:32\n",
      "  Train: [26900/57525] ( 46.8%) | Loss: 0.8054 | Speed: 60.0 batches/s | ETA: 08:30\n",
      "  Train: [27000/57525] ( 46.9%) | Loss: 0.8051 | Speed: 60.0 batches/s | ETA: 08:29\n",
      "  Train: [27100/57525] ( 47.1%) | Loss: 0.8048 | Speed: 60.0 batches/s | ETA: 08:27\n",
      "  Train: [27200/57525] ( 47.3%) | Loss: 0.8044 | Speed: 60.0 batches/s | ETA: 08:25\n",
      "  Train: [27300/57525] ( 47.5%) | Loss: 0.8041 | Speed: 60.0 batches/s | ETA: 08:23\n",
      "  Train: [27400/57525] ( 47.6%) | Loss: 0.8037 | Speed: 60.0 batches/s | ETA: 08:22\n",
      "  Train: [27500/57525] ( 47.8%) | Loss: 0.8033 | Speed: 60.0 batches/s | ETA: 08:20\n",
      "  Train: [27600/57525] ( 48.0%) | Loss: 0.8030 | Speed: 60.0 batches/s | ETA: 08:19\n",
      "  Train: [27700/57525] ( 48.2%) | Loss: 0.8026 | Speed: 60.0 batches/s | ETA: 08:17\n",
      "  Train: [27800/57525] ( 48.3%) | Loss: 0.8022 | Speed: 60.0 batches/s | ETA: 08:15\n",
      "  Train: [27900/57525] ( 48.5%) | Loss: 0.8019 | Speed: 60.0 batches/s | ETA: 08:14\n",
      "  Train: [28000/57525] ( 48.7%) | Loss: 0.8016 | Speed: 60.0 batches/s | ETA: 08:12\n",
      "  Train: [28100/57525] ( 48.8%) | Loss: 0.8012 | Speed: 60.0 batches/s | ETA: 08:10\n",
      "  Train: [28200/57525] ( 49.0%) | Loss: 0.8009 | Speed: 60.0 batches/s | ETA: 08:09\n",
      "  Train: [28300/57525] ( 49.2%) | Loss: 0.8005 | Speed: 60.0 batches/s | ETA: 08:07\n",
      "  Train: [28400/57525] ( 49.4%) | Loss: 0.8001 | Speed: 60.0 batches/s | ETA: 08:05\n",
      "  Train: [28500/57525] ( 49.5%) | Loss: 0.7998 | Speed: 59.9 batches/s | ETA: 08:04\n",
      "  Train: [28600/57525] ( 49.7%) | Loss: 0.7996 | Speed: 59.9 batches/s | ETA: 08:02\n",
      "  Train: [28700/57525] ( 49.9%) | Loss: 0.7992 | Speed: 59.9 batches/s | ETA: 08:00\n",
      "  Train: [28800/57525] ( 50.1%) | Loss: 0.7989 | Speed: 59.9 batches/s | ETA: 07:59\n",
      "  Train: [28900/57525] ( 50.2%) | Loss: 0.7985 | Speed: 59.9 batches/s | ETA: 07:57\n",
      "  Train: [29000/57525] ( 50.4%) | Loss: 0.7982 | Speed: 59.9 batches/s | ETA: 07:56\n",
      "  Train: [29100/57525] ( 50.6%) | Loss: 0.7979 | Speed: 59.9 batches/s | ETA: 07:54\n",
      "  Train: [29200/57525] ( 50.8%) | Loss: 0.7976 | Speed: 59.9 batches/s | ETA: 07:52\n",
      "  Train: [29300/57525] ( 50.9%) | Loss: 0.7973 | Speed: 59.9 batches/s | ETA: 07:51\n",
      "  Train: [29400/57525] ( 51.1%) | Loss: 0.7971 | Speed: 59.9 batches/s | ETA: 07:49\n",
      "  Train: [29500/57525] ( 51.3%) | Loss: 0.7967 | Speed: 59.9 batches/s | ETA: 07:47\n",
      "  Train: [29600/57525] ( 51.5%) | Loss: 0.7964 | Speed: 59.9 batches/s | ETA: 07:46\n",
      "  Train: [29700/57525] ( 51.6%) | Loss: 0.7961 | Speed: 59.9 batches/s | ETA: 07:44\n",
      "  Train: [29800/57525] ( 51.8%) | Loss: 0.7959 | Speed: 59.9 batches/s | ETA: 07:42\n",
      "  Train: [29900/57525] ( 52.0%) | Loss: 0.7955 | Speed: 59.9 batches/s | ETA: 07:41\n",
      "  Train: [30000/57525] ( 52.2%) | Loss: 0.7952 | Speed: 59.9 batches/s | ETA: 07:39\n",
      "  Train: [30100/57525] ( 52.3%) | Loss: 0.7948 | Speed: 59.9 batches/s | ETA: 07:38\n",
      "  Train: [30200/57525] ( 52.5%) | Loss: 0.7944 | Speed: 59.9 batches/s | ETA: 07:36\n",
      "  Train: [30300/57525] ( 52.7%) | Loss: 0.7941 | Speed: 59.9 batches/s | ETA: 07:34\n",
      "  Train: [30400/57525] ( 52.8%) | Loss: 0.7937 | Speed: 59.9 batches/s | ETA: 07:33\n",
      "  Train: [30500/57525] ( 53.0%) | Loss: 0.7934 | Speed: 59.9 batches/s | ETA: 07:31\n",
      "  Train: [30600/57525] ( 53.2%) | Loss: 0.7932 | Speed: 59.9 batches/s | ETA: 07:29\n",
      "  Train: [30700/57525] ( 53.4%) | Loss: 0.7930 | Speed: 59.9 batches/s | ETA: 07:28\n",
      "  Train: [30800/57525] ( 53.5%) | Loss: 0.7927 | Speed: 59.9 batches/s | ETA: 07:26\n",
      "  Train: [30900/57525] ( 53.7%) | Loss: 0.7925 | Speed: 59.9 batches/s | ETA: 07:24\n",
      "  Train: [31000/57525] ( 53.9%) | Loss: 0.7922 | Speed: 59.9 batches/s | ETA: 07:23\n",
      "  Train: [31100/57525] ( 54.1%) | Loss: 0.7920 | Speed: 59.9 batches/s | ETA: 07:21\n",
      "  Train: [31200/57525] ( 54.2%) | Loss: 0.7916 | Speed: 59.9 batches/s | ETA: 07:19\n",
      "  Train: [31300/57525] ( 54.4%) | Loss: 0.7914 | Speed: 59.9 batches/s | ETA: 07:18\n",
      "  Train: [31400/57525] ( 54.6%) | Loss: 0.7911 | Speed: 59.9 batches/s | ETA: 07:16\n",
      "  Train: [31500/57525] ( 54.8%) | Loss: 0.7909 | Speed: 59.9 batches/s | ETA: 07:14\n",
      "  Train: [31600/57525] ( 54.9%) | Loss: 0.7906 | Speed: 59.8 batches/s | ETA: 07:13\n",
      "  Train: [31700/57525] ( 55.1%) | Loss: 0.7904 | Speed: 59.8 batches/s | ETA: 07:11\n",
      "  Train: [31800/57525] ( 55.3%) | Loss: 0.7901 | Speed: 59.8 batches/s | ETA: 07:09\n",
      "  Train: [31900/57525] ( 55.5%) | Loss: 0.7897 | Speed: 59.8 batches/s | ETA: 07:08\n",
      "  Train: [32000/57525] ( 55.6%) | Loss: 0.7894 | Speed: 59.8 batches/s | ETA: 07:06\n",
      "  Train: [32100/57525] ( 55.8%) | Loss: 0.7890 | Speed: 59.8 batches/s | ETA: 07:04\n",
      "  Train: [32200/57525] ( 56.0%) | Loss: 0.7887 | Speed: 59.8 batches/s | ETA: 07:03\n",
      "  Train: [32300/57525] ( 56.1%) | Loss: 0.7885 | Speed: 59.8 batches/s | ETA: 07:01\n",
      "  Train: [32400/57525] ( 56.3%) | Loss: 0.7882 | Speed: 59.8 batches/s | ETA: 06:59\n",
      "  Train: [32500/57525] ( 56.5%) | Loss: 0.7880 | Speed: 59.8 batches/s | ETA: 06:58\n",
      "  Train: [32600/57525] ( 56.7%) | Loss: 0.7878 | Speed: 59.8 batches/s | ETA: 06:56\n",
      "  Train: [32700/57525] ( 56.8%) | Loss: 0.7875 | Speed: 59.8 batches/s | ETA: 06:54\n",
      "  Train: [32800/57525] ( 57.0%) | Loss: 0.7873 | Speed: 59.8 batches/s | ETA: 06:53\n",
      "  Train: [32900/57525] ( 57.2%) | Loss: 0.7870 | Speed: 59.8 batches/s | ETA: 06:51\n",
      "  Train: [33000/57525] ( 57.4%) | Loss: 0.7867 | Speed: 59.8 batches/s | ETA: 06:49\n",
      "  Train: [33100/57525] ( 57.5%) | Loss: 0.7865 | Speed: 59.8 batches/s | ETA: 06:48\n",
      "  Train: [33200/57525] ( 57.7%) | Loss: 0.7862 | Speed: 59.8 batches/s | ETA: 06:46\n",
      "  Train: [33300/57525] ( 57.9%) | Loss: 0.7859 | Speed: 59.8 batches/s | ETA: 06:44\n",
      "  Train: [33400/57525] ( 58.1%) | Loss: 0.7856 | Speed: 59.8 batches/s | ETA: 06:43\n",
      "  Train: [33500/57525] ( 58.2%) | Loss: 0.7853 | Speed: 59.8 batches/s | ETA: 06:41\n",
      "  Train: [33600/57525] ( 58.4%) | Loss: 0.7850 | Speed: 59.8 batches/s | ETA: 06:40\n",
      "  Train: [33700/57525] ( 58.6%) | Loss: 0.7848 | Speed: 59.8 batches/s | ETA: 06:38\n",
      "  Train: [33800/57525] ( 58.8%) | Loss: 0.7845 | Speed: 59.8 batches/s | ETA: 06:36\n",
      "  Train: [33900/57525] ( 58.9%) | Loss: 0.7842 | Speed: 59.8 batches/s | ETA: 06:35\n",
      "  Train: [34000/57525] ( 59.1%) | Loss: 0.7840 | Speed: 59.8 batches/s | ETA: 06:33\n",
      "  Train: [34100/57525] ( 59.3%) | Loss: 0.7837 | Speed: 59.8 batches/s | ETA: 06:31\n",
      "  Train: [34200/57525] ( 59.5%) | Loss: 0.7834 | Speed: 59.8 batches/s | ETA: 06:30\n",
      "  Train: [34300/57525] ( 59.6%) | Loss: 0.7831 | Speed: 59.8 batches/s | ETA: 06:28\n",
      "  Train: [34400/57525] ( 59.8%) | Loss: 0.7828 | Speed: 59.8 batches/s | ETA: 06:26\n",
      "  Train: [34500/57525] ( 60.0%) | Loss: 0.7825 | Speed: 59.8 batches/s | ETA: 06:25\n",
      "  Train: [34600/57525] ( 60.1%) | Loss: 0.7823 | Speed: 59.8 batches/s | ETA: 06:23\n",
      "  Train: [34700/57525] ( 60.3%) | Loss: 0.7819 | Speed: 59.8 batches/s | ETA: 06:21\n",
      "  Train: [34800/57525] ( 60.5%) | Loss: 0.7817 | Speed: 59.8 batches/s | ETA: 06:20\n",
      "  Train: [34900/57525] ( 60.7%) | Loss: 0.7814 | Speed: 59.8 batches/s | ETA: 06:18\n",
      "  Train: [35000/57525] ( 60.8%) | Loss: 0.7812 | Speed: 59.8 batches/s | ETA: 06:16\n",
      "  Train: [35100/57525] ( 61.0%) | Loss: 0.7810 | Speed: 59.8 batches/s | ETA: 06:15\n",
      "  Train: [35200/57525] ( 61.2%) | Loss: 0.7807 | Speed: 59.8 batches/s | ETA: 06:13\n",
      "  Train: [35300/57525] ( 61.4%) | Loss: 0.7805 | Speed: 59.7 batches/s | ETA: 06:12\n",
      "  Train: [35400/57525] ( 61.5%) | Loss: 0.7802 | Speed: 59.7 batches/s | ETA: 06:10\n",
      "  Train: [35500/57525] ( 61.7%) | Loss: 0.7799 | Speed: 59.7 batches/s | ETA: 06:08\n",
      "  Train: [35600/57525] ( 61.9%) | Loss: 0.7797 | Speed: 59.7 batches/s | ETA: 06:07\n",
      "  Train: [35700/57525] ( 62.1%) | Loss: 0.7794 | Speed: 59.7 batches/s | ETA: 06:05\n",
      "  Train: [35800/57525] ( 62.2%) | Loss: 0.7793 | Speed: 59.7 batches/s | ETA: 06:03\n",
      "  Train: [35900/57525] ( 62.4%) | Loss: 0.7790 | Speed: 59.7 batches/s | ETA: 06:02\n",
      "  Train: [36000/57525] ( 62.6%) | Loss: 0.7788 | Speed: 59.7 batches/s | ETA: 06:00\n",
      "  Train: [36100/57525] ( 62.8%) | Loss: 0.7785 | Speed: 59.7 batches/s | ETA: 05:58\n",
      "  Train: [36200/57525] ( 62.9%) | Loss: 0.7782 | Speed: 59.7 batches/s | ETA: 05:57\n",
      "  Train: [36300/57525] ( 63.1%) | Loss: 0.7781 | Speed: 59.7 batches/s | ETA: 05:55\n",
      "  Train: [36400/57525] ( 63.3%) | Loss: 0.7778 | Speed: 59.7 batches/s | ETA: 05:53\n",
      "  Train: [36500/57525] ( 63.5%) | Loss: 0.7776 | Speed: 59.7 batches/s | ETA: 05:52\n",
      "  Train: [36600/57525] ( 63.6%) | Loss: 0.7774 | Speed: 59.7 batches/s | ETA: 05:50\n",
      "  Train: [36700/57525] ( 63.8%) | Loss: 0.7771 | Speed: 59.7 batches/s | ETA: 05:48\n",
      "  Train: [36800/57525] ( 64.0%) | Loss: 0.7769 | Speed: 59.7 batches/s | ETA: 05:47\n",
      "  Train: [36900/57525] ( 64.1%) | Loss: 0.7767 | Speed: 59.7 batches/s | ETA: 05:45\n",
      "  Train: [37000/57525] ( 64.3%) | Loss: 0.7764 | Speed: 59.7 batches/s | ETA: 05:44\n",
      "  Train: [37100/57525] ( 64.5%) | Loss: 0.7762 | Speed: 59.7 batches/s | ETA: 05:42\n",
      "  Train: [37200/57525] ( 64.7%) | Loss: 0.7760 | Speed: 59.7 batches/s | ETA: 05:40\n",
      "  Train: [37300/57525] ( 64.8%) | Loss: 0.7758 | Speed: 59.7 batches/s | ETA: 05:39\n",
      "  Train: [37400/57525] ( 65.0%) | Loss: 0.7755 | Speed: 59.7 batches/s | ETA: 05:37\n",
      "  Train: [37500/57525] ( 65.2%) | Loss: 0.7753 | Speed: 59.7 batches/s | ETA: 05:35\n",
      "  Train: [37600/57525] ( 65.4%) | Loss: 0.7750 | Speed: 59.7 batches/s | ETA: 05:34\n",
      "  Train: [37700/57525] ( 65.5%) | Loss: 0.7748 | Speed: 59.7 batches/s | ETA: 05:32\n",
      "  Train: [37800/57525] ( 65.7%) | Loss: 0.7745 | Speed: 59.6 batches/s | ETA: 05:30\n",
      "  Train: [37900/57525] ( 65.9%) | Loss: 0.7742 | Speed: 59.6 batches/s | ETA: 05:29\n",
      "  Train: [38000/57525] ( 66.1%) | Loss: 0.7740 | Speed: 59.6 batches/s | ETA: 05:27\n",
      "  Train: [38100/57525] ( 66.2%) | Loss: 0.7737 | Speed: 59.6 batches/s | ETA: 05:25\n",
      "  Train: [38200/57525] ( 66.4%) | Loss: 0.7735 | Speed: 59.6 batches/s | ETA: 05:24\n",
      "  Train: [38300/57525] ( 66.6%) | Loss: 0.7732 | Speed: 59.6 batches/s | ETA: 05:22\n",
      "  Train: [38400/57525] ( 66.8%) | Loss: 0.7730 | Speed: 59.6 batches/s | ETA: 05:20\n",
      "  Train: [38500/57525] ( 66.9%) | Loss: 0.7728 | Speed: 59.6 batches/s | ETA: 05:19\n",
      "  Train: [38600/57525] ( 67.1%) | Loss: 0.7725 | Speed: 59.6 batches/s | ETA: 05:17\n",
      "  Train: [38700/57525] ( 67.3%) | Loss: 0.7722 | Speed: 59.6 batches/s | ETA: 05:15\n",
      "  Train: [38800/57525] ( 67.4%) | Loss: 0.7720 | Speed: 59.6 batches/s | ETA: 05:14\n",
      "  Train: [38900/57525] ( 67.6%) | Loss: 0.7718 | Speed: 59.6 batches/s | ETA: 05:12\n",
      "  Train: [39000/57525] ( 67.8%) | Loss: 0.7716 | Speed: 59.6 batches/s | ETA: 05:10\n",
      "  Train: [39100/57525] ( 68.0%) | Loss: 0.7714 | Speed: 59.6 batches/s | ETA: 05:09\n",
      "  Train: [39200/57525] ( 68.1%) | Loss: 0.7712 | Speed: 59.6 batches/s | ETA: 05:07\n",
      "  Train: [39300/57525] ( 68.3%) | Loss: 0.7710 | Speed: 59.6 batches/s | ETA: 05:05\n",
      "  Train: [39400/57525] ( 68.5%) | Loss: 0.7708 | Speed: 59.6 batches/s | ETA: 05:03\n",
      "  Train: [39500/57525] ( 68.7%) | Loss: 0.7706 | Speed: 59.6 batches/s | ETA: 05:02\n",
      "  Train: [39600/57525] ( 68.8%) | Loss: 0.7704 | Speed: 59.6 batches/s | ETA: 05:00\n",
      "  Train: [39700/57525] ( 69.0%) | Loss: 0.7702 | Speed: 59.6 batches/s | ETA: 04:58\n",
      "  Train: [39800/57525] ( 69.2%) | Loss: 0.7699 | Speed: 59.6 batches/s | ETA: 04:57\n",
      "  Train: [39900/57525] ( 69.4%) | Loss: 0.7697 | Speed: 59.6 batches/s | ETA: 04:55\n",
      "  Train: [40000/57525] ( 69.5%) | Loss: 0.7695 | Speed: 59.6 batches/s | ETA: 04:53\n",
      "  Train: [40100/57525] ( 69.7%) | Loss: 0.7693 | Speed: 59.6 batches/s | ETA: 04:52\n",
      "  Train: [40200/57525] ( 69.9%) | Loss: 0.7690 | Speed: 59.6 batches/s | ETA: 04:50\n",
      "  Train: [40300/57525] ( 70.1%) | Loss: 0.7688 | Speed: 59.6 batches/s | ETA: 04:48\n",
      "  Train: [40400/57525] ( 70.2%) | Loss: 0.7686 | Speed: 59.6 batches/s | ETA: 04:47\n",
      "  Train: [40500/57525] ( 70.4%) | Loss: 0.7684 | Speed: 59.6 batches/s | ETA: 04:45\n",
      "  Train: [40600/57525] ( 70.6%) | Loss: 0.7682 | Speed: 59.6 batches/s | ETA: 04:43\n",
      "  Train: [40700/57525] ( 70.8%) | Loss: 0.7681 | Speed: 59.6 batches/s | ETA: 04:42\n",
      "  Train: [40800/57525] ( 70.9%) | Loss: 0.7679 | Speed: 59.6 batches/s | ETA: 04:40\n",
      "  Train: [40900/57525] ( 71.1%) | Loss: 0.7677 | Speed: 59.6 batches/s | ETA: 04:38\n",
      "  Train: [41000/57525] ( 71.3%) | Loss: 0.7675 | Speed: 59.6 batches/s | ETA: 04:37\n",
      "  Train: [41100/57525] ( 71.4%) | Loss: 0.7673 | Speed: 59.6 batches/s | ETA: 04:35\n",
      "  Train: [41200/57525] ( 71.6%) | Loss: 0.7670 | Speed: 59.6 batches/s | ETA: 04:33\n",
      "  Train: [41300/57525] ( 71.8%) | Loss: 0.7668 | Speed: 59.6 batches/s | ETA: 04:32\n",
      "  Train: [41400/57525] ( 72.0%) | Loss: 0.7666 | Speed: 59.6 batches/s | ETA: 04:30\n",
      "  Train: [41500/57525] ( 72.1%) | Loss: 0.7664 | Speed: 59.6 batches/s | ETA: 04:28\n",
      "  Train: [41600/57525] ( 72.3%) | Loss: 0.7662 | Speed: 59.6 batches/s | ETA: 04:27\n",
      "  Train: [41700/57525] ( 72.5%) | Loss: 0.7659 | Speed: 59.6 batches/s | ETA: 04:25\n",
      "  Train: [41800/57525] ( 72.7%) | Loss: 0.7657 | Speed: 59.6 batches/s | ETA: 04:23\n",
      "  Train: [41900/57525] ( 72.8%) | Loss: 0.7655 | Speed: 59.6 batches/s | ETA: 04:22\n",
      "  Train: [42000/57525] ( 73.0%) | Loss: 0.7653 | Speed: 59.6 batches/s | ETA: 04:20\n",
      "  Train: [42100/57525] ( 73.2%) | Loss: 0.7651 | Speed: 59.6 batches/s | ETA: 04:18\n",
      "  Train: [42200/57525] ( 73.4%) | Loss: 0.7650 | Speed: 59.6 batches/s | ETA: 04:17\n",
      "  Train: [42300/57525] ( 73.5%) | Loss: 0.7648 | Speed: 59.6 batches/s | ETA: 04:15\n",
      "  Train: [42400/57525] ( 73.7%) | Loss: 0.7646 | Speed: 59.6 batches/s | ETA: 04:13\n",
      "  Train: [42500/57525] ( 73.9%) | Loss: 0.7644 | Speed: 59.6 batches/s | ETA: 04:12\n",
      "  Train: [42600/57525] ( 74.1%) | Loss: 0.7642 | Speed: 59.6 batches/s | ETA: 04:10\n",
      "  Train: [42700/57525] ( 74.2%) | Loss: 0.7639 | Speed: 59.5 batches/s | ETA: 04:08\n",
      "  Train: [42800/57525] ( 74.4%) | Loss: 0.7638 | Speed: 59.5 batches/s | ETA: 04:07\n",
      "  Train: [42900/57525] ( 74.6%) | Loss: 0.7635 | Speed: 59.5 batches/s | ETA: 04:05\n",
      "  Train: [43000/57525] ( 74.8%) | Loss: 0.7634 | Speed: 59.5 batches/s | ETA: 04:03\n",
      "  Train: [43100/57525] ( 74.9%) | Loss: 0.7632 | Speed: 59.5 batches/s | ETA: 04:02\n",
      "  Train: [43200/57525] ( 75.1%) | Loss: 0.7630 | Speed: 59.5 batches/s | ETA: 04:00\n",
      "  Train: [43300/57525] ( 75.3%) | Loss: 0.7628 | Speed: 59.5 batches/s | ETA: 03:58\n",
      "  Train: [43400/57525] ( 75.4%) | Loss: 0.7626 | Speed: 59.5 batches/s | ETA: 03:57\n",
      "  Train: [43500/57525] ( 75.6%) | Loss: 0.7624 | Speed: 59.5 batches/s | ETA: 03:55\n",
      "  Train: [43600/57525] ( 75.8%) | Loss: 0.7622 | Speed: 59.5 batches/s | ETA: 03:53\n",
      "  Train: [43700/57525] ( 76.0%) | Loss: 0.7620 | Speed: 59.5 batches/s | ETA: 03:52\n",
      "  Train: [43800/57525] ( 76.1%) | Loss: 0.7618 | Speed: 59.5 batches/s | ETA: 03:50\n",
      "  Train: [43900/57525] ( 76.3%) | Loss: 0.7616 | Speed: 59.5 batches/s | ETA: 03:48\n",
      "  Train: [44000/57525] ( 76.5%) | Loss: 0.7615 | Speed: 59.5 batches/s | ETA: 03:47\n",
      "  Train: [44100/57525] ( 76.7%) | Loss: 0.7613 | Speed: 59.5 batches/s | ETA: 03:45\n",
      "  Train: [44200/57525] ( 76.8%) | Loss: 0.7612 | Speed: 59.5 batches/s | ETA: 03:43\n",
      "  Train: [44300/57525] ( 77.0%) | Loss: 0.7610 | Speed: 59.5 batches/s | ETA: 03:42\n",
      "  Train: [44400/57525] ( 77.2%) | Loss: 0.7609 | Speed: 59.5 batches/s | ETA: 03:40\n",
      "  Train: [44500/57525] ( 77.4%) | Loss: 0.7607 | Speed: 59.5 batches/s | ETA: 03:38\n",
      "  Train: [44600/57525] ( 77.5%) | Loss: 0.7605 | Speed: 59.5 batches/s | ETA: 03:37\n",
      "  Train: [44700/57525] ( 77.7%) | Loss: 0.7603 | Speed: 59.5 batches/s | ETA: 03:35\n",
      "  Train: [44800/57525] ( 77.9%) | Loss: 0.7600 | Speed: 59.5 batches/s | ETA: 03:33\n",
      "  Train: [44900/57525] ( 78.1%) | Loss: 0.7598 | Speed: 59.5 batches/s | ETA: 03:32\n",
      "  Train: [45000/57525] ( 78.2%) | Loss: 0.7597 | Speed: 59.5 batches/s | ETA: 03:30\n",
      "  Train: [45100/57525] ( 78.4%) | Loss: 0.7595 | Speed: 59.5 batches/s | ETA: 03:28\n",
      "  Train: [45200/57525] ( 78.6%) | Loss: 0.7592 | Speed: 59.5 batches/s | ETA: 03:27\n",
      "  Train: [45300/57525] ( 78.7%) | Loss: 0.7591 | Speed: 59.5 batches/s | ETA: 03:25\n",
      "  Train: [45400/57525] ( 78.9%) | Loss: 0.7589 | Speed: 59.5 batches/s | ETA: 03:23\n",
      "  Train: [45500/57525] ( 79.1%) | Loss: 0.7587 | Speed: 59.5 batches/s | ETA: 03:22\n",
      "  Train: [45600/57525] ( 79.3%) | Loss: 0.7585 | Speed: 59.5 batches/s | ETA: 03:20\n",
      "  Train: [45700/57525] ( 79.4%) | Loss: 0.7583 | Speed: 59.5 batches/s | ETA: 03:18\n",
      "  Train: [45800/57525] ( 79.6%) | Loss: 0.7581 | Speed: 59.5 batches/s | ETA: 03:17\n",
      "  Train: [45900/57525] ( 79.8%) | Loss: 0.7579 | Speed: 59.5 batches/s | ETA: 03:15\n",
      "  Train: [46000/57525] ( 80.0%) | Loss: 0.7577 | Speed: 59.5 batches/s | ETA: 03:13\n",
      "  Train: [46100/57525] ( 80.1%) | Loss: 0.7575 | Speed: 59.5 batches/s | ETA: 03:12\n",
      "  Train: [46200/57525] ( 80.3%) | Loss: 0.7574 | Speed: 59.4 batches/s | ETA: 03:10\n",
      "  Train: [46300/57525] ( 80.5%) | Loss: 0.7572 | Speed: 59.4 batches/s | ETA: 03:08\n",
      "  Train: [46400/57525] ( 80.7%) | Loss: 0.7570 | Speed: 59.4 batches/s | ETA: 03:07\n",
      "  Train: [46500/57525] ( 80.8%) | Loss: 0.7568 | Speed: 59.4 batches/s | ETA: 03:05\n",
      "  Train: [46600/57525] ( 81.0%) | Loss: 0.7567 | Speed: 59.4 batches/s | ETA: 03:03\n",
      "  Train: [46700/57525] ( 81.2%) | Loss: 0.7565 | Speed: 59.4 batches/s | ETA: 03:02\n",
      "  Train: [46800/57525] ( 81.4%) | Loss: 0.7563 | Speed: 59.4 batches/s | ETA: 03:00\n",
      "  Train: [46900/57525] ( 81.5%) | Loss: 0.7562 | Speed: 59.4 batches/s | ETA: 02:58\n",
      "  Train: [47000/57525] ( 81.7%) | Loss: 0.7560 | Speed: 59.4 batches/s | ETA: 02:57\n",
      "  Train: [47100/57525] ( 81.9%) | Loss: 0.7558 | Speed: 59.4 batches/s | ETA: 02:55\n",
      "  Train: [47200/57525] ( 82.1%) | Loss: 0.7557 | Speed: 59.4 batches/s | ETA: 02:53\n",
      "  Train: [47300/57525] ( 82.2%) | Loss: 0.7554 | Speed: 59.4 batches/s | ETA: 02:52\n",
      "  Train: [47400/57525] ( 82.4%) | Loss: 0.7552 | Speed: 59.4 batches/s | ETA: 02:50\n",
      "  Train: [47500/57525] ( 82.6%) | Loss: 0.7550 | Speed: 59.4 batches/s | ETA: 02:48\n",
      "  Train: [47600/57525] ( 82.7%) | Loss: 0.7548 | Speed: 59.4 batches/s | ETA: 02:47\n",
      "  Train: [47700/57525] ( 82.9%) | Loss: 0.7547 | Speed: 59.4 batches/s | ETA: 02:45\n",
      "  Train: [47800/57525] ( 83.1%) | Loss: 0.7545 | Speed: 59.4 batches/s | ETA: 02:43\n",
      "  Train: [47900/57525] ( 83.3%) | Loss: 0.7544 | Speed: 59.4 batches/s | ETA: 02:41\n",
      "  Train: [48000/57525] ( 83.4%) | Loss: 0.7542 | Speed: 59.4 batches/s | ETA: 02:40\n",
      "  Train: [48100/57525] ( 83.6%) | Loss: 0.7540 | Speed: 59.4 batches/s | ETA: 02:38\n",
      "  Train: [48200/57525] ( 83.8%) | Loss: 0.7539 | Speed: 59.4 batches/s | ETA: 02:36\n",
      "  Train: [48300/57525] ( 84.0%) | Loss: 0.7537 | Speed: 59.4 batches/s | ETA: 02:35\n",
      "  Train: [48400/57525] ( 84.1%) | Loss: 0.7536 | Speed: 59.4 batches/s | ETA: 02:33\n",
      "  Train: [48500/57525] ( 84.3%) | Loss: 0.7534 | Speed: 59.4 batches/s | ETA: 02:31\n",
      "  Train: [48600/57525] ( 84.5%) | Loss: 0.7532 | Speed: 59.4 batches/s | ETA: 02:30\n",
      "  Train: [48700/57525] ( 84.7%) | Loss: 0.7530 | Speed: 59.4 batches/s | ETA: 02:28\n",
      "  Train: [48800/57525] ( 84.8%) | Loss: 0.7529 | Speed: 59.4 batches/s | ETA: 02:26\n",
      "  Train: [48900/57525] ( 85.0%) | Loss: 0.7526 | Speed: 59.4 batches/s | ETA: 02:25\n",
      "  Train: [49000/57525] ( 85.2%) | Loss: 0.7525 | Speed: 59.4 batches/s | ETA: 02:23\n",
      "  Train: [49100/57525] ( 85.4%) | Loss: 0.7523 | Speed: 59.4 batches/s | ETA: 02:21\n",
      "  Train: [49200/57525] ( 85.5%) | Loss: 0.7521 | Speed: 59.4 batches/s | ETA: 02:20\n",
      "  Train: [49300/57525] ( 85.7%) | Loss: 0.7519 | Speed: 59.4 batches/s | ETA: 02:18\n",
      "  Train: [49400/57525] ( 85.9%) | Loss: 0.7517 | Speed: 59.4 batches/s | ETA: 02:16\n",
      "  Train: [49500/57525] ( 86.0%) | Loss: 0.7516 | Speed: 59.4 batches/s | ETA: 02:15\n",
      "  Train: [49600/57525] ( 86.2%) | Loss: 0.7514 | Speed: 59.4 batches/s | ETA: 02:13\n",
      "  Train: [49700/57525] ( 86.4%) | Loss: 0.7513 | Speed: 59.4 batches/s | ETA: 02:11\n",
      "  Train: [49800/57525] ( 86.6%) | Loss: 0.7512 | Speed: 59.4 batches/s | ETA: 02:10\n",
      "  Train: [49900/57525] ( 86.7%) | Loss: 0.7510 | Speed: 59.4 batches/s | ETA: 02:08\n",
      "  Train: [50000/57525] ( 86.9%) | Loss: 0.7509 | Speed: 59.4 batches/s | ETA: 02:06\n",
      "  Train: [50100/57525] ( 87.1%) | Loss: 0.7507 | Speed: 59.4 batches/s | ETA: 02:04\n",
      "  Train: [50200/57525] ( 87.3%) | Loss: 0.7505 | Speed: 59.4 batches/s | ETA: 02:03\n",
      "  Train: [50300/57525] ( 87.4%) | Loss: 0.7503 | Speed: 59.4 batches/s | ETA: 02:01\n",
      "  Train: [50400/57525] ( 87.6%) | Loss: 0.7501 | Speed: 59.4 batches/s | ETA: 01:59\n",
      "  Train: [50500/57525] ( 87.8%) | Loss: 0.7499 | Speed: 59.4 batches/s | ETA: 01:58\n",
      "  Train: [50600/57525] ( 88.0%) | Loss: 0.7498 | Speed: 59.4 batches/s | ETA: 01:56\n",
      "  Train: [50700/57525] ( 88.1%) | Loss: 0.7496 | Speed: 59.4 batches/s | ETA: 01:54\n",
      "  Train: [50800/57525] ( 88.3%) | Loss: 0.7495 | Speed: 59.4 batches/s | ETA: 01:53\n",
      "  Train: [50900/57525] ( 88.5%) | Loss: 0.7493 | Speed: 59.4 batches/s | ETA: 01:51\n",
      "  Train: [51000/57525] ( 88.7%) | Loss: 0.7492 | Speed: 59.4 batches/s | ETA: 01:49\n",
      "  Train: [51100/57525] ( 88.8%) | Loss: 0.7490 | Speed: 59.4 batches/s | ETA: 01:48\n",
      "  Train: [51200/57525] ( 89.0%) | Loss: 0.7489 | Speed: 59.4 batches/s | ETA: 01:46\n",
      "  Train: [51300/57525] ( 89.2%) | Loss: 0.7487 | Speed: 59.4 batches/s | ETA: 01:44\n",
      "  Train: [51400/57525] ( 89.4%) | Loss: 0.7486 | Speed: 59.3 batches/s | ETA: 01:43\n",
      "  Train: [51500/57525] ( 89.5%) | Loss: 0.7484 | Speed: 59.3 batches/s | ETA: 01:41\n",
      "  Train: [51600/57525] ( 89.7%) | Loss: 0.7482 | Speed: 59.3 batches/s | ETA: 01:39\n",
      "  Train: [51700/57525] ( 89.9%) | Loss: 0.7481 | Speed: 59.3 batches/s | ETA: 01:38\n",
      "  Train: [51800/57525] ( 90.0%) | Loss: 0.7479 | Speed: 59.3 batches/s | ETA: 01:36\n",
      "  Train: [51900/57525] ( 90.2%) | Loss: 0.7477 | Speed: 59.3 batches/s | ETA: 01:34\n",
      "  Train: [52000/57525] ( 90.4%) | Loss: 0.7476 | Speed: 59.3 batches/s | ETA: 01:33\n",
      "  Train: [52100/57525] ( 90.6%) | Loss: 0.7474 | Speed: 59.3 batches/s | ETA: 01:31\n",
      "  Train: [52200/57525] ( 90.7%) | Loss: 0.7473 | Speed: 59.3 batches/s | ETA: 01:29\n",
      "  Train: [52300/57525] ( 90.9%) | Loss: 0.7471 | Speed: 59.3 batches/s | ETA: 01:28\n",
      "  Train: [52400/57525] ( 91.1%) | Loss: 0.7470 | Speed: 59.3 batches/s | ETA: 01:26\n",
      "  Train: [52500/57525] ( 91.3%) | Loss: 0.7468 | Speed: 59.3 batches/s | ETA: 01:24\n",
      "  Train: [52600/57525] ( 91.4%) | Loss: 0.7467 | Speed: 59.3 batches/s | ETA: 01:23\n",
      "  Train: [52700/57525] ( 91.6%) | Loss: 0.7465 | Speed: 59.3 batches/s | ETA: 01:21\n",
      "  Train: [52800/57525] ( 91.8%) | Loss: 0.7463 | Speed: 59.3 batches/s | ETA: 01:19\n",
      "  Train: [52900/57525] ( 92.0%) | Loss: 0.7462 | Speed: 59.3 batches/s | ETA: 01:18\n",
      "  Train: [53000/57525] ( 92.1%) | Loss: 0.7460 | Speed: 59.3 batches/s | ETA: 01:16\n",
      "  Train: [53100/57525] ( 92.3%) | Loss: 0.7459 | Speed: 59.3 batches/s | ETA: 01:14\n",
      "  Train: [53200/57525] ( 92.5%) | Loss: 0.7457 | Speed: 59.3 batches/s | ETA: 01:12\n",
      "  Train: [53300/57525] ( 92.7%) | Loss: 0.7456 | Speed: 59.3 batches/s | ETA: 01:11\n",
      "  Train: [53400/57525] ( 92.8%) | Loss: 0.7454 | Speed: 59.3 batches/s | ETA: 01:09\n",
      "  Train: [53500/57525] ( 93.0%) | Loss: 0.7453 | Speed: 59.3 batches/s | ETA: 01:07\n",
      "  Train: [53600/57525] ( 93.2%) | Loss: 0.7451 | Speed: 59.3 batches/s | ETA: 01:06\n",
      "  Train: [53700/57525] ( 93.4%) | Loss: 0.7450 | Speed: 59.3 batches/s | ETA: 01:04\n",
      "  Train: [53800/57525] ( 93.5%) | Loss: 0.7448 | Speed: 59.3 batches/s | ETA: 01:02\n",
      "  Train: [53900/57525] ( 93.7%) | Loss: 0.7447 | Speed: 59.3 batches/s | ETA: 01:01\n",
      "  Train: [54000/57525] ( 93.9%) | Loss: 0.7445 | Speed: 59.3 batches/s | ETA: 00:59\n",
      "  Train: [54100/57525] ( 94.0%) | Loss: 0.7444 | Speed: 59.3 batches/s | ETA: 00:57\n",
      "  Train: [54200/57525] ( 94.2%) | Loss: 0.7442 | Speed: 59.3 batches/s | ETA: 00:56\n",
      "  Train: [54300/57525] ( 94.4%) | Loss: 0.7441 | Speed: 59.3 batches/s | ETA: 00:54\n",
      "  Train: [54400/57525] ( 94.6%) | Loss: 0.7439 | Speed: 59.3 batches/s | ETA: 00:52\n",
      "  Train: [54500/57525] ( 94.7%) | Loss: 0.7438 | Speed: 59.3 batches/s | ETA: 00:51\n",
      "  Train: [54600/57525] ( 94.9%) | Loss: 0.7436 | Speed: 59.3 batches/s | ETA: 00:49\n",
      "  Train: [54700/57525] ( 95.1%) | Loss: 0.7435 | Speed: 59.3 batches/s | ETA: 00:47\n",
      "  Train: [54800/57525] ( 95.3%) | Loss: 0.7433 | Speed: 59.3 batches/s | ETA: 00:45\n",
      "  Train: [54900/57525] ( 95.4%) | Loss: 0.7431 | Speed: 59.3 batches/s | ETA: 00:44\n",
      "  Train: [55000/57525] ( 95.6%) | Loss: 0.7430 | Speed: 59.3 batches/s | ETA: 00:42\n",
      "  Train: [55100/57525] ( 95.8%) | Loss: 0.7428 | Speed: 59.3 batches/s | ETA: 00:40\n",
      "  Train: [55200/57525] ( 96.0%) | Loss: 0.7427 | Speed: 59.3 batches/s | ETA: 00:39\n",
      "  Train: [55300/57525] ( 96.1%) | Loss: 0.7426 | Speed: 59.3 batches/s | ETA: 00:37\n",
      "  Train: [55400/57525] ( 96.3%) | Loss: 0.7424 | Speed: 59.3 batches/s | ETA: 00:35\n",
      "  Train: [55500/57525] ( 96.5%) | Loss: 0.7423 | Speed: 59.3 batches/s | ETA: 00:34\n",
      "  Train: [55600/57525] ( 96.7%) | Loss: 0.7421 | Speed: 59.3 batches/s | ETA: 00:32\n",
      "  Train: [55700/57525] ( 96.8%) | Loss: 0.7420 | Speed: 59.3 batches/s | ETA: 00:30\n",
      "  Train: [55800/57525] ( 97.0%) | Loss: 0.7418 | Speed: 59.3 batches/s | ETA: 00:29\n",
      "  Train: [55900/57525] ( 97.2%) | Loss: 0.7417 | Speed: 59.3 batches/s | ETA: 00:27\n",
      "  Train: [56000/57525] ( 97.3%) | Loss: 0.7416 | Speed: 59.3 batches/s | ETA: 00:25\n",
      "  Train: [56100/57525] ( 97.5%) | Loss: 0.7415 | Speed: 59.3 batches/s | ETA: 00:24\n",
      "  Train: [56200/57525] ( 97.7%) | Loss: 0.7413 | Speed: 59.3 batches/s | ETA: 00:22\n",
      "  Train: [56300/57525] ( 97.9%) | Loss: 0.7411 | Speed: 59.3 batches/s | ETA: 00:20\n",
      "  Train: [56400/57525] ( 98.0%) | Loss: 0.7410 | Speed: 59.3 batches/s | ETA: 00:18\n",
      "  Train: [56500/57525] ( 98.2%) | Loss: 0.7408 | Speed: 59.3 batches/s | ETA: 00:17\n",
      "  Train: [56600/57525] ( 98.4%) | Loss: 0.7407 | Speed: 59.3 batches/s | ETA: 00:15\n",
      "  Train: [56700/57525] ( 98.6%) | Loss: 0.7405 | Speed: 59.3 batches/s | ETA: 00:13\n",
      "  Train: [56800/57525] ( 98.7%) | Loss: 0.7404 | Speed: 59.3 batches/s | ETA: 00:12\n",
      "  Train: [56900/57525] ( 98.9%) | Loss: 0.7403 | Speed: 59.3 batches/s | ETA: 00:10\n",
      "  Train: [57000/57525] ( 99.1%) | Loss: 0.7401 | Speed: 59.3 batches/s | ETA: 00:08\n",
      "  Train: [57100/57525] ( 99.3%) | Loss: 0.7400 | Speed: 59.3 batches/s | ETA: 00:07\n",
      "  Train: [57200/57525] ( 99.4%) | Loss: 0.7399 | Speed: 59.3 batches/s | ETA: 00:05\n",
      "  Train: [57300/57525] ( 99.6%) | Loss: 0.7398 | Speed: 59.3 batches/s | ETA: 00:03\n",
      "  Train: [57400/57525] ( 99.8%) | Loss: 0.7396 | Speed: 59.3 batches/s | ETA: 00:02\n",
      "  Train: [57500/57525] (100.0%) | Loss: 0.7395 | Speed: 59.3 batches/s | ETA: 00:00\n",
      "  Running validation...\n",
      "    Val: [  50/7191] (  0.7%)\n",
      "    Val: [ 100/7191] (  1.4%)\n",
      "    Val: [ 150/7191] (  2.1%)\n",
      "    Val: [ 200/7191] (  2.8%)\n",
      "    Val: [ 250/7191] (  3.5%)\n",
      "    Val: [ 300/7191] (  4.2%)\n",
      "    Val: [ 350/7191] (  4.9%)\n",
      "    Val: [ 400/7191] (  5.6%)\n",
      "    Val: [ 450/7191] (  6.3%)\n",
      "    Val: [ 500/7191] (  7.0%)\n",
      "    Val: [ 550/7191] (  7.6%)\n",
      "    Val: [ 600/7191] (  8.3%)\n",
      "    Val: [ 650/7191] (  9.0%)\n",
      "    Val: [ 700/7191] (  9.7%)\n",
      "    Val: [ 750/7191] ( 10.4%)\n",
      "    Val: [ 800/7191] ( 11.1%)\n",
      "    Val: [ 850/7191] ( 11.8%)\n",
      "    Val: [ 900/7191] ( 12.5%)\n",
      "    Val: [ 950/7191] ( 13.2%)\n",
      "    Val: [1000/7191] ( 13.9%)\n",
      "    Val: [1050/7191] ( 14.6%)\n",
      "    Val: [1100/7191] ( 15.3%)\n",
      "    Val: [1150/7191] ( 16.0%)\n",
      "    Val: [1200/7191] ( 16.7%)\n",
      "    Val: [1250/7191] ( 17.4%)\n",
      "    Val: [1300/7191] ( 18.1%)\n",
      "    Val: [1350/7191] ( 18.8%)\n",
      "    Val: [1400/7191] ( 19.5%)\n",
      "    Val: [1450/7191] ( 20.2%)\n",
      "    Val: [1500/7191] ( 20.9%)\n",
      "    Val: [1550/7191] ( 21.6%)\n",
      "    Val: [1600/7191] ( 22.3%)\n",
      "    Val: [1650/7191] ( 22.9%)\n",
      "    Val: [1700/7191] ( 23.6%)\n",
      "    Val: [1750/7191] ( 24.3%)\n",
      "    Val: [1800/7191] ( 25.0%)\n",
      "    Val: [1850/7191] ( 25.7%)\n",
      "    Val: [1900/7191] ( 26.4%)\n",
      "    Val: [1950/7191] ( 27.1%)\n",
      "    Val: [2000/7191] ( 27.8%)\n",
      "    Val: [2050/7191] ( 28.5%)\n",
      "    Val: [2100/7191] ( 29.2%)\n",
      "    Val: [2150/7191] ( 29.9%)\n",
      "    Val: [2200/7191] ( 30.6%)\n",
      "    Val: [2250/7191] ( 31.3%)\n",
      "    Val: [2300/7191] ( 32.0%)\n",
      "    Val: [2350/7191] ( 32.7%)\n",
      "    Val: [2400/7191] ( 33.4%)\n",
      "    Val: [2450/7191] ( 34.1%)\n",
      "    Val: [2500/7191] ( 34.8%)\n",
      "    Val: [2550/7191] ( 35.5%)\n",
      "    Val: [2600/7191] ( 36.2%)\n",
      "    Val: [2650/7191] ( 36.9%)\n",
      "    Val: [2700/7191] ( 37.5%)\n",
      "    Val: [2750/7191] ( 38.2%)\n",
      "    Val: [2800/7191] ( 38.9%)\n",
      "    Val: [2850/7191] ( 39.6%)\n",
      "    Val: [2900/7191] ( 40.3%)\n",
      "    Val: [2950/7191] ( 41.0%)\n",
      "    Val: [3000/7191] ( 41.7%)\n",
      "    Val: [3050/7191] ( 42.4%)\n",
      "    Val: [3100/7191] ( 43.1%)\n",
      "    Val: [3150/7191] ( 43.8%)\n",
      "    Val: [3200/7191] ( 44.5%)\n",
      "    Val: [3250/7191] ( 45.2%)\n",
      "    Val: [3300/7191] ( 45.9%)\n",
      "    Val: [3350/7191] ( 46.6%)\n",
      "    Val: [3400/7191] ( 47.3%)\n",
      "    Val: [3450/7191] ( 48.0%)\n",
      "    Val: [3500/7191] ( 48.7%)\n",
      "    Val: [3550/7191] ( 49.4%)\n",
      "    Val: [3600/7191] ( 50.1%)\n",
      "    Val: [3650/7191] ( 50.8%)\n",
      "    Val: [3700/7191] ( 51.5%)\n",
      "    Val: [3750/7191] ( 52.1%)\n",
      "    Val: [3800/7191] ( 52.8%)\n",
      "    Val: [3850/7191] ( 53.5%)\n",
      "    Val: [3900/7191] ( 54.2%)\n",
      "    Val: [3950/7191] ( 54.9%)\n",
      "    Val: [4000/7191] ( 55.6%)\n",
      "    Val: [4050/7191] ( 56.3%)\n",
      "    Val: [4100/7191] ( 57.0%)\n",
      "    Val: [4150/7191] ( 57.7%)\n",
      "    Val: [4200/7191] ( 58.4%)\n",
      "    Val: [4250/7191] ( 59.1%)\n",
      "    Val: [4300/7191] ( 59.8%)\n",
      "    Val: [4350/7191] ( 60.5%)\n",
      "    Val: [4400/7191] ( 61.2%)\n",
      "    Val: [4450/7191] ( 61.9%)\n",
      "    Val: [4500/7191] ( 62.6%)\n",
      "    Val: [4550/7191] ( 63.3%)\n",
      "    Val: [4600/7191] ( 64.0%)\n",
      "    Val: [4650/7191] ( 64.7%)\n",
      "    Val: [4700/7191] ( 65.4%)\n",
      "    Val: [4750/7191] ( 66.1%)\n",
      "    Val: [4800/7191] ( 66.8%)\n",
      "    Val: [4850/7191] ( 67.4%)\n",
      "    Val: [4900/7191] ( 68.1%)\n",
      "    Val: [4950/7191] ( 68.8%)\n",
      "    Val: [5000/7191] ( 69.5%)\n",
      "    Val: [5050/7191] ( 70.2%)\n",
      "    Val: [5100/7191] ( 70.9%)\n",
      "    Val: [5150/7191] ( 71.6%)\n",
      "    Val: [5200/7191] ( 72.3%)\n",
      "    Val: [5250/7191] ( 73.0%)\n",
      "    Val: [5300/7191] ( 73.7%)\n",
      "    Val: [5350/7191] ( 74.4%)\n",
      "    Val: [5400/7191] ( 75.1%)\n",
      "    Val: [5450/7191] ( 75.8%)\n",
      "    Val: [5500/7191] ( 76.5%)\n",
      "    Val: [5550/7191] ( 77.2%)\n",
      "    Val: [5600/7191] ( 77.9%)\n",
      "    Val: [5650/7191] ( 78.6%)\n",
      "    Val: [5700/7191] ( 79.3%)\n",
      "    Val: [5750/7191] ( 80.0%)\n",
      "    Val: [5800/7191] ( 80.7%)\n",
      "    Val: [5850/7191] ( 81.4%)\n",
      "    Val: [5900/7191] ( 82.0%)\n",
      "    Val: [5950/7191] ( 82.7%)\n",
      "    Val: [6000/7191] ( 83.4%)\n",
      "    Val: [6050/7191] ( 84.1%)\n",
      "    Val: [6100/7191] ( 84.8%)\n",
      "    Val: [6150/7191] ( 85.5%)\n",
      "    Val: [6200/7191] ( 86.2%)\n",
      "    Val: [6250/7191] ( 86.9%)\n",
      "    Val: [6300/7191] ( 87.6%)\n",
      "    Val: [6350/7191] ( 88.3%)\n",
      "    Val: [6400/7191] ( 89.0%)\n",
      "    Val: [6450/7191] ( 89.7%)\n",
      "    Val: [6500/7191] ( 90.4%)\n",
      "    Val: [6550/7191] ( 91.1%)\n",
      "    Val: [6600/7191] ( 91.8%)\n",
      "    Val: [6650/7191] ( 92.5%)\n",
      "    Val: [6700/7191] ( 93.2%)\n",
      "    Val: [6750/7191] ( 93.9%)\n",
      "    Val: [6800/7191] ( 94.6%)\n",
      "    Val: [6850/7191] ( 95.3%)\n",
      "    Val: [6900/7191] ( 96.0%)\n",
      "    Val: [6950/7191] ( 96.6%)\n",
      "    Val: [7000/7191] ( 97.3%)\n",
      "    Val: [7050/7191] ( 98.0%)\n",
      "    Val: [7100/7191] ( 98.7%)\n",
      "    Val: [7150/7191] ( 99.4%)\n",
      "\n",
      "  Epoch 01 Summary:\n",
      "    Train MSE (norm): 0.7394 | Val MSE (norm): 0.6394\n",
      "    Time: Train=16.2min, Val=0.6min, Total=16.7min\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Epoch 02/3\n",
      "============================================================\n",
      "  Train: [  100/57525] (  0.2%) | Loss: 0.6560 | Speed: 35.1 batches/s | ETA: 27:15\n",
      "  Train: [  200/57525] (  0.3%) | Loss: 0.6482 | Speed: 43.1 batches/s | ETA: 22:09\n",
      "  Train: [  300/57525] (  0.5%) | Loss: 0.6491 | Speed: 46.8 batches/s | ETA: 20:22\n",
      "  Train: [  400/57525] (  0.7%) | Loss: 0.6524 | Speed: 49.3 batches/s | ETA: 19:17\n",
      "  Train: [  500/57525] (  0.9%) | Loss: 0.6544 | Speed: 51.0 batches/s | ETA: 18:39\n",
      "  Train: [  600/57525] (  1.0%) | Loss: 0.6504 | Speed: 52.2 batches/s | ETA: 18:10\n",
      "  Train: [  700/57525] (  1.2%) | Loss: 0.6535 | Speed: 52.8 batches/s | ETA: 17:55\n",
      "  Train: [  800/57525] (  1.4%) | Loss: 0.6533 | Speed: 53.5 batches/s | ETA: 17:41\n",
      "  Train: [  900/57525] (  1.6%) | Loss: 0.6505 | Speed: 53.9 batches/s | ETA: 17:29\n",
      "  Train: [ 1000/57525] (  1.7%) | Loss: 0.6513 | Speed: 54.2 batches/s | ETA: 17:22\n",
      "  Train: [ 1100/57525] (  1.9%) | Loss: 0.6504 | Speed: 54.5 batches/s | ETA: 17:16\n",
      "  Train: [ 1200/57525] (  2.1%) | Loss: 0.6500 | Speed: 54.7 batches/s | ETA: 17:10\n",
      "  Train: [ 1300/57525] (  2.3%) | Loss: 0.6521 | Speed: 54.9 batches/s | ETA: 17:04\n",
      "  Train: [ 1400/57525] (  2.4%) | Loss: 0.6509 | Speed: 55.1 batches/s | ETA: 16:58\n",
      "  Train: [ 1500/57525] (  2.6%) | Loss: 0.6522 | Speed: 55.4 batches/s | ETA: 16:51\n",
      "  Train: [ 1600/57525] (  2.8%) | Loss: 0.6514 | Speed: 55.6 batches/s | ETA: 16:45\n",
      "  Train: [ 1700/57525] (  3.0%) | Loss: 0.6531 | Speed: 55.8 batches/s | ETA: 16:40\n",
      "  Train: [ 1800/57525] (  3.1%) | Loss: 0.6525 | Speed: 56.0 batches/s | ETA: 16:35\n",
      "  Train: [ 1900/57525] (  3.3%) | Loss: 0.6532 | Speed: 56.1 batches/s | ETA: 16:30\n",
      "  Train: [ 2000/57525] (  3.5%) | Loss: 0.6542 | Speed: 56.3 batches/s | ETA: 16:26\n",
      "  Train: [ 2100/57525] (  3.7%) | Loss: 0.6545 | Speed: 56.5 batches/s | ETA: 16:21\n",
      "  Train: [ 2200/57525] (  3.8%) | Loss: 0.6543 | Speed: 56.6 batches/s | ETA: 16:16\n",
      "  Train: [ 2300/57525] (  4.0%) | Loss: 0.6543 | Speed: 56.8 batches/s | ETA: 16:12\n",
      "  Train: [ 2400/57525] (  4.2%) | Loss: 0.6554 | Speed: 56.9 batches/s | ETA: 16:09\n",
      "  Train: [ 2500/57525] (  4.3%) | Loss: 0.6558 | Speed: 57.0 batches/s | ETA: 16:05\n",
      "  Train: [ 2600/57525] (  4.5%) | Loss: 0.6562 | Speed: 57.1 batches/s | ETA: 16:02\n",
      "  Train: [ 2700/57525] (  4.7%) | Loss: 0.6558 | Speed: 57.1 batches/s | ETA: 15:59\n",
      "  Train: [ 2800/57525] (  4.9%) | Loss: 0.6565 | Speed: 57.2 batches/s | ETA: 15:56\n",
      "  Train: [ 2900/57525] (  5.0%) | Loss: 0.6561 | Speed: 57.2 batches/s | ETA: 15:54\n",
      "  Train: [ 3000/57525] (  5.2%) | Loss: 0.6558 | Speed: 57.3 batches/s | ETA: 15:51\n",
      "  Train: [ 3100/57525] (  5.4%) | Loss: 0.6559 | Speed: 57.3 batches/s | ETA: 15:49\n",
      "  Train: [ 3200/57525] (  5.6%) | Loss: 0.6562 | Speed: 57.4 batches/s | ETA: 15:46\n",
      "  Train: [ 3300/57525] (  5.7%) | Loss: 0.6567 | Speed: 57.5 batches/s | ETA: 15:43\n",
      "  Train: [ 3400/57525] (  5.9%) | Loss: 0.6566 | Speed: 57.5 batches/s | ETA: 15:40\n",
      "  Train: [ 3500/57525] (  6.1%) | Loss: 0.6562 | Speed: 57.6 batches/s | ETA: 15:38\n",
      "  Train: [ 3600/57525] (  6.3%) | Loss: 0.6553 | Speed: 57.6 batches/s | ETA: 15:35\n",
      "  Train: [ 3700/57525] (  6.4%) | Loss: 0.6549 | Speed: 57.7 batches/s | ETA: 15:32\n",
      "  Train: [ 3800/57525] (  6.6%) | Loss: 0.6545 | Speed: 57.7 batches/s | ETA: 15:31\n",
      "  Train: [ 3900/57525] (  6.8%) | Loss: 0.6548 | Speed: 57.7 batches/s | ETA: 15:29\n",
      "  Train: [ 4000/57525] (  7.0%) | Loss: 0.6549 | Speed: 57.7 batches/s | ETA: 15:27\n",
      "  Train: [ 4100/57525] (  7.1%) | Loss: 0.6544 | Speed: 57.7 batches/s | ETA: 15:25\n",
      "  Train: [ 4200/57525] (  7.3%) | Loss: 0.6544 | Speed: 57.7 batches/s | ETA: 15:23\n",
      "  Train: [ 4300/57525] (  7.5%) | Loss: 0.6545 | Speed: 57.8 batches/s | ETA: 15:21\n",
      "  Train: [ 4400/57525] (  7.6%) | Loss: 0.6545 | Speed: 57.8 batches/s | ETA: 15:19\n",
      "  Train: [ 4500/57525] (  7.8%) | Loss: 0.6546 | Speed: 57.8 batches/s | ETA: 15:17\n",
      "  Train: [ 4600/57525] (  8.0%) | Loss: 0.6549 | Speed: 57.8 batches/s | ETA: 15:14\n",
      "  Train: [ 4700/57525] (  8.2%) | Loss: 0.6546 | Speed: 57.8 batches/s | ETA: 15:13\n",
      "  Train: [ 4800/57525] (  8.3%) | Loss: 0.6541 | Speed: 57.9 batches/s | ETA: 15:10\n",
      "  Train: [ 4900/57525] (  8.5%) | Loss: 0.6541 | Speed: 57.9 batches/s | ETA: 15:08\n",
      "  Train: [ 5000/57525] (  8.7%) | Loss: 0.6539 | Speed: 58.0 batches/s | ETA: 15:06\n",
      "  Train: [ 5100/57525] (  8.9%) | Loss: 0.6535 | Speed: 58.0 batches/s | ETA: 15:04\n",
      "  Train: [ 5200/57525] (  9.0%) | Loss: 0.6538 | Speed: 58.0 batches/s | ETA: 15:02\n",
      "  Train: [ 5300/57525] (  9.2%) | Loss: 0.6536 | Speed: 58.0 batches/s | ETA: 15:00\n",
      "  Train: [ 5400/57525] (  9.4%) | Loss: 0.6533 | Speed: 58.0 batches/s | ETA: 14:58\n",
      "  Train: [ 5500/57525] (  9.6%) | Loss: 0.6529 | Speed: 58.0 batches/s | ETA: 14:56\n",
      "  Train: [ 5600/57525] (  9.7%) | Loss: 0.6532 | Speed: 58.0 batches/s | ETA: 14:54\n",
      "  Train: [ 5700/57525] (  9.9%) | Loss: 0.6533 | Speed: 58.0 batches/s | ETA: 14:52\n",
      "  Train: [ 5800/57525] ( 10.1%) | Loss: 0.6534 | Speed: 58.1 batches/s | ETA: 14:50\n",
      "  Train: [ 5900/57525] ( 10.3%) | Loss: 0.6537 | Speed: 58.1 batches/s | ETA: 14:48\n",
      "  Train: [ 6000/57525] ( 10.4%) | Loss: 0.6535 | Speed: 58.1 batches/s | ETA: 14:47\n",
      "  Train: [ 6100/57525] ( 10.6%) | Loss: 0.6538 | Speed: 58.1 batches/s | ETA: 14:45\n",
      "  Train: [ 6200/57525] ( 10.8%) | Loss: 0.6536 | Speed: 58.1 batches/s | ETA: 14:43\n",
      "  Train: [ 6300/57525] ( 11.0%) | Loss: 0.6534 | Speed: 58.1 batches/s | ETA: 14:42\n",
      "  Train: [ 6400/57525] ( 11.1%) | Loss: 0.6532 | Speed: 58.1 batches/s | ETA: 14:40\n",
      "  Train: [ 6500/57525] ( 11.3%) | Loss: 0.6533 | Speed: 58.1 batches/s | ETA: 14:38\n",
      "  Train: [ 6600/57525] ( 11.5%) | Loss: 0.6534 | Speed: 58.0 batches/s | ETA: 14:37\n",
      "  Train: [ 6700/57525] ( 11.6%) | Loss: 0.6535 | Speed: 58.1 batches/s | ETA: 14:35\n",
      "  Train: [ 6800/57525] ( 11.8%) | Loss: 0.6534 | Speed: 58.1 batches/s | ETA: 14:33\n",
      "  Train: [ 6900/57525] ( 12.0%) | Loss: 0.6533 | Speed: 58.1 batches/s | ETA: 14:31\n",
      "  Train: [ 7000/57525] ( 12.2%) | Loss: 0.6534 | Speed: 58.1 batches/s | ETA: 14:29\n",
      "  Train: [ 7100/57525] ( 12.3%) | Loss: 0.6533 | Speed: 58.1 batches/s | ETA: 14:28\n",
      "  Train: [ 7200/57525] ( 12.5%) | Loss: 0.6533 | Speed: 58.1 batches/s | ETA: 14:26\n",
      "  Train: [ 7300/57525] ( 12.7%) | Loss: 0.6530 | Speed: 58.1 batches/s | ETA: 14:24\n",
      "  Train: [ 7400/57525] ( 12.9%) | Loss: 0.6529 | Speed: 58.1 batches/s | ETA: 14:22\n",
      "  Train: [ 7500/57525] ( 13.0%) | Loss: 0.6529 | Speed: 58.1 batches/s | ETA: 14:20\n",
      "  Train: [ 7600/57525] ( 13.2%) | Loss: 0.6527 | Speed: 58.1 batches/s | ETA: 14:18\n",
      "  Train: [ 7700/57525] ( 13.4%) | Loss: 0.6528 | Speed: 58.1 batches/s | ETA: 14:16\n",
      "  Train: [ 7800/57525] ( 13.6%) | Loss: 0.6533 | Speed: 58.1 batches/s | ETA: 14:15\n",
      "  Train: [ 7900/57525] ( 13.7%) | Loss: 0.6534 | Speed: 58.1 batches/s | ETA: 14:13\n",
      "  Train: [ 8000/57525] ( 13.9%) | Loss: 0.6535 | Speed: 58.1 batches/s | ETA: 14:11\n",
      "  Train: [ 8100/57525] ( 14.1%) | Loss: 0.6536 | Speed: 58.1 batches/s | ETA: 14:10\n",
      "  Train: [ 8200/57525] ( 14.3%) | Loss: 0.6538 | Speed: 58.1 batches/s | ETA: 14:08\n",
      "  Train: [ 8300/57525] ( 14.4%) | Loss: 0.6540 | Speed: 58.1 batches/s | ETA: 14:06\n",
      "  Train: [ 8400/57525] ( 14.6%) | Loss: 0.6541 | Speed: 58.1 batches/s | ETA: 14:05\n",
      "  Train: [ 8500/57525] ( 14.8%) | Loss: 0.6540 | Speed: 58.1 batches/s | ETA: 14:03\n",
      "  Train: [ 8600/57525] ( 15.0%) | Loss: 0.6540 | Speed: 58.1 batches/s | ETA: 14:01\n",
      "  Train: [ 8700/57525] ( 15.1%) | Loss: 0.6539 | Speed: 58.1 batches/s | ETA: 14:00\n",
      "  Train: [ 8800/57525] ( 15.3%) | Loss: 0.6540 | Speed: 58.1 batches/s | ETA: 13:58\n",
      "  Train: [ 8900/57525] ( 15.5%) | Loss: 0.6540 | Speed: 58.1 batches/s | ETA: 13:56\n",
      "  Train: [ 9000/57525] ( 15.6%) | Loss: 0.6540 | Speed: 58.1 batches/s | ETA: 13:54\n",
      "  Train: [ 9100/57525] ( 15.8%) | Loss: 0.6538 | Speed: 58.1 batches/s | ETA: 13:53\n",
      "  Train: [ 9200/57525] ( 16.0%) | Loss: 0.6539 | Speed: 58.1 batches/s | ETA: 13:51\n",
      "  Train: [ 9300/57525] ( 16.2%) | Loss: 0.6542 | Speed: 58.1 batches/s | ETA: 13:50\n",
      "  Train: [ 9400/57525] ( 16.3%) | Loss: 0.6541 | Speed: 58.1 batches/s | ETA: 13:48\n",
      "  Train: [ 9500/57525] ( 16.5%) | Loss: 0.6540 | Speed: 58.0 batches/s | ETA: 13:47\n",
      "  Train: [ 9600/57525] ( 16.7%) | Loss: 0.6538 | Speed: 58.0 batches/s | ETA: 13:45\n",
      "  Train: [ 9700/57525] ( 16.9%) | Loss: 0.6536 | Speed: 58.0 batches/s | ETA: 13:44\n",
      "  Train: [ 9800/57525] ( 17.0%) | Loss: 0.6538 | Speed: 57.9 batches/s | ETA: 13:43\n",
      "  Train: [ 9900/57525] ( 17.2%) | Loss: 0.6536 | Speed: 57.9 batches/s | ETA: 13:42\n",
      "  Train: [10000/57525] ( 17.4%) | Loss: 0.6535 | Speed: 57.9 batches/s | ETA: 13:40\n",
      "  Train: [10100/57525] ( 17.6%) | Loss: 0.6533 | Speed: 57.9 batches/s | ETA: 13:39\n",
      "  Train: [10200/57525] ( 17.7%) | Loss: 0.6534 | Speed: 57.9 batches/s | ETA: 13:37\n",
      "  Train: [10300/57525] ( 17.9%) | Loss: 0.6535 | Speed: 57.8 batches/s | ETA: 13:36\n",
      "  Train: [10400/57525] ( 18.1%) | Loss: 0.6535 | Speed: 57.8 batches/s | ETA: 13:35\n",
      "  Train: [10500/57525] ( 18.3%) | Loss: 0.6535 | Speed: 57.8 batches/s | ETA: 13:33\n",
      "  Train: [10600/57525] ( 18.4%) | Loss: 0.6536 | Speed: 57.8 batches/s | ETA: 13:32\n",
      "  Train: [10700/57525] ( 18.6%) | Loss: 0.6535 | Speed: 57.8 batches/s | ETA: 13:30\n",
      "  Train: [10800/57525] ( 18.8%) | Loss: 0.6535 | Speed: 57.8 batches/s | ETA: 13:28\n",
      "  Train: [10900/57525] ( 18.9%) | Loss: 0.6535 | Speed: 57.8 batches/s | ETA: 13:26\n",
      "  Train: [11000/57525] ( 19.1%) | Loss: 0.6534 | Speed: 57.8 batches/s | ETA: 13:25\n",
      "  Train: [11100/57525] ( 19.3%) | Loss: 0.6534 | Speed: 57.8 batches/s | ETA: 13:23\n",
      "  Train: [11200/57525] ( 19.5%) | Loss: 0.6534 | Speed: 57.8 batches/s | ETA: 13:21\n",
      "  Train: [11300/57525] ( 19.6%) | Loss: 0.6533 | Speed: 57.8 batches/s | ETA: 13:19\n",
      "  Train: [11400/57525] ( 19.8%) | Loss: 0.6531 | Speed: 57.8 batches/s | ETA: 13:17\n",
      "  Train: [11500/57525] ( 20.0%) | Loss: 0.6532 | Speed: 57.8 batches/s | ETA: 13:15\n",
      "  Train: [11600/57525] ( 20.2%) | Loss: 0.6534 | Speed: 57.8 batches/s | ETA: 13:14\n",
      "  Train: [11700/57525] ( 20.3%) | Loss: 0.6531 | Speed: 57.8 batches/s | ETA: 13:12\n",
      "  Train: [11800/57525] ( 20.5%) | Loss: 0.6530 | Speed: 57.8 batches/s | ETA: 13:10\n",
      "  Train: [11900/57525] ( 20.7%) | Loss: 0.6529 | Speed: 57.8 batches/s | ETA: 13:09\n",
      "  Train: [12000/57525] ( 20.9%) | Loss: 0.6530 | Speed: 57.8 batches/s | ETA: 13:07\n",
      "  Train: [12100/57525] ( 21.0%) | Loss: 0.6532 | Speed: 57.8 batches/s | ETA: 13:05\n",
      "  Train: [12200/57525] ( 21.2%) | Loss: 0.6532 | Speed: 57.9 batches/s | ETA: 13:03\n",
      "  Train: [12300/57525] ( 21.4%) | Loss: 0.6532 | Speed: 57.9 batches/s | ETA: 13:01\n",
      "  Train: [12400/57525] ( 21.6%) | Loss: 0.6533 | Speed: 57.9 batches/s | ETA: 12:59\n",
      "  Train: [12500/57525] ( 21.7%) | Loss: 0.6532 | Speed: 57.9 batches/s | ETA: 12:57\n",
      "  Train: [12600/57525] ( 21.9%) | Loss: 0.6531 | Speed: 57.9 batches/s | ETA: 12:55\n",
      "  Train: [12700/57525] ( 22.1%) | Loss: 0.6531 | Speed: 57.9 batches/s | ETA: 12:54\n",
      "  Train: [12800/57525] ( 22.3%) | Loss: 0.6531 | Speed: 57.9 batches/s | ETA: 12:52\n",
      "  Train: [12900/57525] ( 22.4%) | Loss: 0.6531 | Speed: 57.9 batches/s | ETA: 12:50\n",
      "  Train: [13000/57525] ( 22.6%) | Loss: 0.6529 | Speed: 57.9 batches/s | ETA: 12:48\n",
      "  Train: [13100/57525] ( 22.8%) | Loss: 0.6528 | Speed: 57.9 batches/s | ETA: 12:46\n",
      "  Train: [13200/57525] ( 22.9%) | Loss: 0.6528 | Speed: 58.0 batches/s | ETA: 12:44\n",
      "  Train: [13300/57525] ( 23.1%) | Loss: 0.6529 | Speed: 58.0 batches/s | ETA: 12:42\n",
      "  Train: [13400/57525] ( 23.3%) | Loss: 0.6530 | Speed: 58.0 batches/s | ETA: 12:41\n",
      "  Train: [13500/57525] ( 23.5%) | Loss: 0.6531 | Speed: 58.0 batches/s | ETA: 12:39\n",
      "  Train: [13600/57525] ( 23.6%) | Loss: 0.6532 | Speed: 58.0 batches/s | ETA: 12:37\n",
      "  Train: [13700/57525] ( 23.8%) | Loss: 0.6533 | Speed: 58.0 batches/s | ETA: 12:35\n",
      "  Train: [13800/57525] ( 24.0%) | Loss: 0.6533 | Speed: 58.0 batches/s | ETA: 12:33\n",
      "  Train: [13900/57525] ( 24.2%) | Loss: 0.6533 | Speed: 58.0 batches/s | ETA: 12:32\n",
      "  Train: [14000/57525] ( 24.3%) | Loss: 0.6532 | Speed: 58.0 batches/s | ETA: 12:30\n",
      "  Train: [14100/57525] ( 24.5%) | Loss: 0.6531 | Speed: 58.0 batches/s | ETA: 12:28\n",
      "  Train: [14200/57525] ( 24.7%) | Loss: 0.6532 | Speed: 58.0 batches/s | ETA: 12:26\n",
      "  Train: [14300/57525] ( 24.9%) | Loss: 0.6532 | Speed: 58.0 batches/s | ETA: 12:24\n",
      "  Train: [14400/57525] ( 25.0%) | Loss: 0.6532 | Speed: 58.0 batches/s | ETA: 12:23\n",
      "  Train: [14500/57525] ( 25.2%) | Loss: 0.6531 | Speed: 58.0 batches/s | ETA: 12:21\n",
      "  Train: [14600/57525] ( 25.4%) | Loss: 0.6530 | Speed: 58.0 batches/s | ETA: 12:19\n",
      "  Train: [14700/57525] ( 25.6%) | Loss: 0.6529 | Speed: 58.0 batches/s | ETA: 12:17\n",
      "  Train: [14800/57525] ( 25.7%) | Loss: 0.6528 | Speed: 58.0 batches/s | ETA: 12:16\n",
      "  Train: [14900/57525] ( 25.9%) | Loss: 0.6528 | Speed: 58.0 batches/s | ETA: 12:14\n",
      "  Train: [15000/57525] ( 26.1%) | Loss: 0.6527 | Speed: 58.0 batches/s | ETA: 12:13\n",
      "  Train: [15100/57525] ( 26.2%) | Loss: 0.6526 | Speed: 58.0 batches/s | ETA: 12:11\n",
      "  Train: [15200/57525] ( 26.4%) | Loss: 0.6524 | Speed: 58.0 batches/s | ETA: 12:10\n",
      "  Train: [15300/57525] ( 26.6%) | Loss: 0.6524 | Speed: 58.0 batches/s | ETA: 12:08\n",
      "  Train: [15400/57525] ( 26.8%) | Loss: 0.6523 | Speed: 58.0 batches/s | ETA: 12:06\n",
      "  Train: [15500/57525] ( 26.9%) | Loss: 0.6523 | Speed: 58.0 batches/s | ETA: 12:04\n",
      "  Train: [15600/57525] ( 27.1%) | Loss: 0.6524 | Speed: 58.0 batches/s | ETA: 12:03\n",
      "  Train: [15700/57525] ( 27.3%) | Loss: 0.6525 | Speed: 58.0 batches/s | ETA: 12:01\n",
      "  Train: [15800/57525] ( 27.5%) | Loss: 0.6525 | Speed: 58.0 batches/s | ETA: 11:59\n",
      "  Train: [15900/57525] ( 27.6%) | Loss: 0.6525 | Speed: 58.0 batches/s | ETA: 11:57\n",
      "  Train: [16000/57525] ( 27.8%) | Loss: 0.6524 | Speed: 58.0 batches/s | ETA: 11:56\n",
      "  Train: [16100/57525] ( 28.0%) | Loss: 0.6525 | Speed: 58.0 batches/s | ETA: 11:54\n",
      "  Train: [16200/57525] ( 28.2%) | Loss: 0.6523 | Speed: 58.0 batches/s | ETA: 11:52\n",
      "  Train: [16300/57525] ( 28.3%) | Loss: 0.6524 | Speed: 58.0 batches/s | ETA: 11:50\n",
      "  Train: [16400/57525] ( 28.5%) | Loss: 0.6524 | Speed: 58.0 batches/s | ETA: 11:49\n",
      "  Train: [16500/57525] ( 28.7%) | Loss: 0.6522 | Speed: 58.0 batches/s | ETA: 11:47\n",
      "  Train: [16600/57525] ( 28.9%) | Loss: 0.6522 | Speed: 58.0 batches/s | ETA: 11:45\n",
      "  Train: [16700/57525] ( 29.0%) | Loss: 0.6522 | Speed: 58.0 batches/s | ETA: 11:43\n",
      "  Train: [16800/57525] ( 29.2%) | Loss: 0.6521 | Speed: 58.0 batches/s | ETA: 11:41\n",
      "  Train: [16900/57525] ( 29.4%) | Loss: 0.6520 | Speed: 58.0 batches/s | ETA: 11:40\n",
      "  Train: [17000/57525] ( 29.6%) | Loss: 0.6520 | Speed: 58.0 batches/s | ETA: 11:38\n",
      "  Train: [17100/57525] ( 29.7%) | Loss: 0.6519 | Speed: 58.0 batches/s | ETA: 11:36\n",
      "  Train: [17200/57525] ( 29.9%) | Loss: 0.6519 | Speed: 58.0 batches/s | ETA: 11:34\n",
      "  Train: [17300/57525] ( 30.1%) | Loss: 0.6518 | Speed: 58.0 batches/s | ETA: 11:32\n",
      "  Train: [17400/57525] ( 30.2%) | Loss: 0.6516 | Speed: 58.0 batches/s | ETA: 11:31\n",
      "  Train: [17500/57525] ( 30.4%) | Loss: 0.6516 | Speed: 58.0 batches/s | ETA: 11:29\n",
      "  Train: [17600/57525] ( 30.6%) | Loss: 0.6517 | Speed: 58.1 batches/s | ETA: 11:27\n",
      "  Train: [17700/57525] ( 30.8%) | Loss: 0.6516 | Speed: 58.1 batches/s | ETA: 11:25\n",
      "  Train: [17800/57525] ( 30.9%) | Loss: 0.6515 | Speed: 58.1 batches/s | ETA: 11:24\n",
      "  Train: [17900/57525] ( 31.1%) | Loss: 0.6514 | Speed: 58.1 batches/s | ETA: 11:22\n",
      "  Train: [18000/57525] ( 31.3%) | Loss: 0.6514 | Speed: 58.1 batches/s | ETA: 11:20\n",
      "  Train: [18100/57525] ( 31.5%) | Loss: 0.6515 | Speed: 58.1 batches/s | ETA: 11:18\n",
      "  Train: [18200/57525] ( 31.6%) | Loss: 0.6512 | Speed: 58.1 batches/s | ETA: 11:17\n",
      "  Train: [18300/57525] ( 31.8%) | Loss: 0.6512 | Speed: 58.1 batches/s | ETA: 11:15\n",
      "  Train: [18400/57525] ( 32.0%) | Loss: 0.6511 | Speed: 58.1 batches/s | ETA: 11:13\n",
      "  Train: [18500/57525] ( 32.2%) | Loss: 0.6511 | Speed: 58.1 batches/s | ETA: 11:12\n",
      "  Train: [18600/57525] ( 32.3%) | Loss: 0.6511 | Speed: 58.1 batches/s | ETA: 11:10\n",
      "  Train: [18700/57525] ( 32.5%) | Loss: 0.6512 | Speed: 58.1 batches/s | ETA: 11:08\n",
      "  Train: [18800/57525] ( 32.7%) | Loss: 0.6512 | Speed: 58.1 batches/s | ETA: 11:07\n",
      "  Train: [18900/57525] ( 32.9%) | Loss: 0.6511 | Speed: 58.0 batches/s | ETA: 11:05\n",
      "  Train: [19000/57525] ( 33.0%) | Loss: 0.6511 | Speed: 58.1 batches/s | ETA: 11:03\n",
      "  Train: [19100/57525] ( 33.2%) | Loss: 0.6510 | Speed: 58.1 batches/s | ETA: 11:01\n",
      "  Train: [19200/57525] ( 33.4%) | Loss: 0.6509 | Speed: 58.1 batches/s | ETA: 11:00\n",
      "  Train: [19300/57525] ( 33.6%) | Loss: 0.6508 | Speed: 58.1 batches/s | ETA: 10:58\n",
      "  Train: [19400/57525] ( 33.7%) | Loss: 0.6507 | Speed: 58.1 batches/s | ETA: 10:56\n",
      "  Train: [19500/57525] ( 33.9%) | Loss: 0.6506 | Speed: 58.1 batches/s | ETA: 10:54\n",
      "  Train: [19600/57525] ( 34.1%) | Loss: 0.6506 | Speed: 58.1 batches/s | ETA: 10:52\n",
      "  Train: [19700/57525] ( 34.2%) | Loss: 0.6506 | Speed: 58.1 batches/s | ETA: 10:51\n",
      "  Train: [19800/57525] ( 34.4%) | Loss: 0.6507 | Speed: 58.1 batches/s | ETA: 10:49\n",
      "  Train: [19900/57525] ( 34.6%) | Loss: 0.6505 | Speed: 58.1 batches/s | ETA: 10:47\n",
      "  Train: [20000/57525] ( 34.8%) | Loss: 0.6505 | Speed: 58.1 batches/s | ETA: 10:46\n",
      "  Train: [20100/57525] ( 34.9%) | Loss: 0.6504 | Speed: 58.1 batches/s | ETA: 10:44\n",
      "  Train: [20200/57525] ( 35.1%) | Loss: 0.6504 | Speed: 58.1 batches/s | ETA: 10:42\n",
      "  Train: [20300/57525] ( 35.3%) | Loss: 0.6504 | Speed: 58.1 batches/s | ETA: 10:41\n",
      "  Train: [20400/57525] ( 35.5%) | Loss: 0.6504 | Speed: 58.1 batches/s | ETA: 10:39\n",
      "  Train: [20500/57525] ( 35.6%) | Loss: 0.6503 | Speed: 58.1 batches/s | ETA: 10:37\n",
      "  Train: [20600/57525] ( 35.8%) | Loss: 0.6503 | Speed: 58.1 batches/s | ETA: 10:35\n",
      "  Train: [20700/57525] ( 36.0%) | Loss: 0.6503 | Speed: 58.1 batches/s | ETA: 10:34\n",
      "  Train: [20800/57525] ( 36.2%) | Loss: 0.6503 | Speed: 58.1 batches/s | ETA: 10:32\n",
      "  Train: [20900/57525] ( 36.3%) | Loss: 0.6503 | Speed: 58.0 batches/s | ETA: 10:30\n",
      "  Train: [21000/57525] ( 36.5%) | Loss: 0.6503 | Speed: 58.1 batches/s | ETA: 10:29\n",
      "  Train: [21100/57525] ( 36.7%) | Loss: 0.6502 | Speed: 58.1 batches/s | ETA: 10:27\n",
      "  Train: [21200/57525] ( 36.9%) | Loss: 0.6501 | Speed: 58.1 batches/s | ETA: 10:25\n",
      "  Train: [21300/57525] ( 37.0%) | Loss: 0.6501 | Speed: 58.1 batches/s | ETA: 10:23\n",
      "  Train: [21400/57525] ( 37.2%) | Loss: 0.6501 | Speed: 58.1 batches/s | ETA: 10:22\n",
      "  Train: [21500/57525] ( 37.4%) | Loss: 0.6500 | Speed: 58.1 batches/s | ETA: 10:20\n",
      "  Train: [21600/57525] ( 37.5%) | Loss: 0.6499 | Speed: 58.1 batches/s | ETA: 10:18\n",
      "  Train: [21700/57525] ( 37.7%) | Loss: 0.6498 | Speed: 58.1 batches/s | ETA: 10:16\n",
      "  Train: [21800/57525] ( 37.9%) | Loss: 0.6498 | Speed: 58.1 batches/s | ETA: 10:15\n",
      "  Train: [21900/57525] ( 38.1%) | Loss: 0.6497 | Speed: 58.1 batches/s | ETA: 10:13\n",
      "  Train: [22000/57525] ( 38.2%) | Loss: 0.6496 | Speed: 58.1 batches/s | ETA: 10:11\n",
      "  Train: [22100/57525] ( 38.4%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 10:09\n",
      "  Train: [22200/57525] ( 38.6%) | Loss: 0.6496 | Speed: 58.1 batches/s | ETA: 10:08\n",
      "  Train: [22300/57525] ( 38.8%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 10:06\n",
      "  Train: [22400/57525] ( 38.9%) | Loss: 0.6496 | Speed: 58.1 batches/s | ETA: 10:04\n",
      "  Train: [22500/57525] ( 39.1%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 10:02\n",
      "  Train: [22600/57525] ( 39.3%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 10:01\n",
      "  Train: [22700/57525] ( 39.5%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:59\n",
      "  Train: [22800/57525] ( 39.6%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:57\n",
      "  Train: [22900/57525] ( 39.8%) | Loss: 0.6494 | Speed: 58.1 batches/s | ETA: 09:55\n",
      "  Train: [23000/57525] ( 40.0%) | Loss: 0.6494 | Speed: 58.1 batches/s | ETA: 09:53\n",
      "  Train: [23100/57525] ( 40.2%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:52\n",
      "  Train: [23200/57525] ( 40.3%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:50\n",
      "  Train: [23300/57525] ( 40.5%) | Loss: 0.6494 | Speed: 58.1 batches/s | ETA: 09:48\n",
      "  Train: [23400/57525] ( 40.7%) | Loss: 0.6494 | Speed: 58.1 batches/s | ETA: 09:47\n",
      "  Train: [23500/57525] ( 40.9%) | Loss: 0.6494 | Speed: 58.1 batches/s | ETA: 09:45\n",
      "  Train: [23600/57525] ( 41.0%) | Loss: 0.6494 | Speed: 58.1 batches/s | ETA: 09:43\n",
      "  Train: [23700/57525] ( 41.2%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:41\n",
      "  Train: [23800/57525] ( 41.4%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:40\n",
      "  Train: [23900/57525] ( 41.5%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:38\n",
      "  Train: [24000/57525] ( 41.7%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:36\n",
      "  Train: [24100/57525] ( 41.9%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:35\n",
      "  Train: [24200/57525] ( 42.1%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:33\n",
      "  Train: [24300/57525] ( 42.2%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:31\n",
      "  Train: [24400/57525] ( 42.4%) | Loss: 0.6496 | Speed: 58.1 batches/s | ETA: 09:29\n",
      "  Train: [24500/57525] ( 42.6%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:28\n",
      "  Train: [24600/57525] ( 42.8%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:26\n",
      "  Train: [24700/57525] ( 42.9%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:24\n",
      "  Train: [24800/57525] ( 43.1%) | Loss: 0.6494 | Speed: 58.1 batches/s | ETA: 09:23\n",
      "  Train: [24900/57525] ( 43.3%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:21\n",
      "  Train: [25000/57525] ( 43.5%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:19\n",
      "  Train: [25100/57525] ( 43.6%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:18\n",
      "  Train: [25200/57525] ( 43.8%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:16\n",
      "  Train: [25300/57525] ( 44.0%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:14\n",
      "  Train: [25400/57525] ( 44.2%) | Loss: 0.6495 | Speed: 58.1 batches/s | ETA: 09:13\n",
      "  Train: [25500/57525] ( 44.3%) | Loss: 0.6494 | Speed: 58.1 batches/s | ETA: 09:11\n",
      "  Train: [25600/57525] ( 44.5%) | Loss: 0.6494 | Speed: 58.1 batches/s | ETA: 09:09\n",
      "  Train: [25700/57525] ( 44.7%) | Loss: 0.6494 | Speed: 58.1 batches/s | ETA: 09:07\n",
      "  Train: [25800/57525] ( 44.9%) | Loss: 0.6493 | Speed: 58.1 batches/s | ETA: 09:06\n",
      "  Train: [25900/57525] ( 45.0%) | Loss: 0.6493 | Speed: 58.1 batches/s | ETA: 09:04\n",
      "  Train: [26000/57525] ( 45.2%) | Loss: 0.6493 | Speed: 58.1 batches/s | ETA: 09:02\n",
      "  Train: [26100/57525] ( 45.4%) | Loss: 0.6491 | Speed: 58.1 batches/s | ETA: 09:01\n",
      "  Train: [26200/57525] ( 45.5%) | Loss: 0.6491 | Speed: 58.1 batches/s | ETA: 08:59\n",
      "  Train: [26300/57525] ( 45.7%) | Loss: 0.6491 | Speed: 58.0 batches/s | ETA: 08:57\n",
      "  Train: [26400/57525] ( 45.9%) | Loss: 0.6491 | Speed: 58.0 batches/s | ETA: 08:56\n",
      "  Train: [26500/57525] ( 46.1%) | Loss: 0.6490 | Speed: 58.0 batches/s | ETA: 08:54\n",
      "  Train: [26600/57525] ( 46.2%) | Loss: 0.6489 | Speed: 58.0 batches/s | ETA: 08:52\n",
      "  Train: [26700/57525] ( 46.4%) | Loss: 0.6489 | Speed: 58.0 batches/s | ETA: 08:51\n",
      "  Train: [26800/57525] ( 46.6%) | Loss: 0.6488 | Speed: 58.0 batches/s | ETA: 08:49\n",
      "  Train: [26900/57525] ( 46.8%) | Loss: 0.6488 | Speed: 58.0 batches/s | ETA: 08:48\n",
      "  Train: [27000/57525] ( 46.9%) | Loss: 0.6488 | Speed: 58.0 batches/s | ETA: 08:46\n",
      "  Train: [27100/57525] ( 47.1%) | Loss: 0.6487 | Speed: 58.0 batches/s | ETA: 08:44\n",
      "  Train: [27200/57525] ( 47.3%) | Loss: 0.6486 | Speed: 58.0 batches/s | ETA: 08:43\n",
      "  Train: [27300/57525] ( 47.5%) | Loss: 0.6487 | Speed: 57.9 batches/s | ETA: 08:41\n",
      "  Train: [27400/57525] ( 47.6%) | Loss: 0.6487 | Speed: 57.9 batches/s | ETA: 08:40\n",
      "  Train: [27500/57525] ( 47.8%) | Loss: 0.6486 | Speed: 57.9 batches/s | ETA: 08:38\n",
      "  Train: [27600/57525] ( 48.0%) | Loss: 0.6486 | Speed: 57.9 batches/s | ETA: 08:36\n",
      "  Train: [27700/57525] ( 48.2%) | Loss: 0.6485 | Speed: 57.9 batches/s | ETA: 08:35\n",
      "  Train: [27800/57525] ( 48.3%) | Loss: 0.6486 | Speed: 57.9 batches/s | ETA: 08:33\n",
      "  Train: [27900/57525] ( 48.5%) | Loss: 0.6485 | Speed: 57.9 batches/s | ETA: 08:31\n",
      "  Train: [28000/57525] ( 48.7%) | Loss: 0.6484 | Speed: 57.9 batches/s | ETA: 08:29\n",
      "  Train: [28100/57525] ( 48.8%) | Loss: 0.6483 | Speed: 57.9 batches/s | ETA: 08:28\n",
      "  Train: [28200/57525] ( 49.0%) | Loss: 0.6482 | Speed: 57.9 batches/s | ETA: 08:26\n",
      "  Train: [28300/57525] ( 49.2%) | Loss: 0.6482 | Speed: 57.9 batches/s | ETA: 08:24\n",
      "  Train: [28400/57525] ( 49.4%) | Loss: 0.6481 | Speed: 57.9 batches/s | ETA: 08:22\n",
      "  Train: [28500/57525] ( 49.5%) | Loss: 0.6481 | Speed: 57.9 batches/s | ETA: 08:21\n",
      "  Train: [28600/57525] ( 49.7%) | Loss: 0.6480 | Speed: 57.9 batches/s | ETA: 08:19\n",
      "  Train: [28700/57525] ( 49.9%) | Loss: 0.6479 | Speed: 57.9 batches/s | ETA: 08:17\n",
      "  Train: [28800/57525] ( 50.1%) | Loss: 0.6480 | Speed: 57.9 batches/s | ETA: 08:15\n",
      "  Train: [28900/57525] ( 50.2%) | Loss: 0.6479 | Speed: 57.9 batches/s | ETA: 08:14\n",
      "  Train: [29000/57525] ( 50.4%) | Loss: 0.6479 | Speed: 57.9 batches/s | ETA: 08:12\n",
      "  Train: [29100/57525] ( 50.6%) | Loss: 0.6478 | Speed: 57.9 batches/s | ETA: 08:10\n",
      "  Train: [29200/57525] ( 50.8%) | Loss: 0.6478 | Speed: 57.9 batches/s | ETA: 08:08\n",
      "  Train: [29300/57525] ( 50.9%) | Loss: 0.6477 | Speed: 57.9 batches/s | ETA: 08:07\n",
      "  Train: [29400/57525] ( 51.1%) | Loss: 0.6477 | Speed: 57.9 batches/s | ETA: 08:05\n",
      "  Train: [29500/57525] ( 51.3%) | Loss: 0.6477 | Speed: 57.9 batches/s | ETA: 08:03\n",
      "  Train: [29600/57525] ( 51.5%) | Loss: 0.6477 | Speed: 58.0 batches/s | ETA: 08:01\n",
      "  Train: [29700/57525] ( 51.6%) | Loss: 0.6476 | Speed: 58.0 batches/s | ETA: 08:00\n",
      "  Train: [29800/57525] ( 51.8%) | Loss: 0.6477 | Speed: 58.0 batches/s | ETA: 07:58\n",
      "  Train: [29900/57525] ( 52.0%) | Loss: 0.6476 | Speed: 58.0 batches/s | ETA: 07:56\n",
      "  Train: [30000/57525] ( 52.2%) | Loss: 0.6477 | Speed: 58.0 batches/s | ETA: 07:54\n",
      "  Train: [30100/57525] ( 52.3%) | Loss: 0.6476 | Speed: 58.0 batches/s | ETA: 07:53\n",
      "  Train: [30200/57525] ( 52.5%) | Loss: 0.6476 | Speed: 58.0 batches/s | ETA: 07:51\n",
      "  Train: [30300/57525] ( 52.7%) | Loss: 0.6475 | Speed: 58.0 batches/s | ETA: 07:49\n",
      "  Train: [30400/57525] ( 52.8%) | Loss: 0.6475 | Speed: 58.0 batches/s | ETA: 07:47\n",
      "  Train: [30500/57525] ( 53.0%) | Loss: 0.6475 | Speed: 58.0 batches/s | ETA: 07:46\n",
      "  Train: [30600/57525] ( 53.2%) | Loss: 0.6475 | Speed: 58.0 batches/s | ETA: 07:44\n",
      "  Train: [30700/57525] ( 53.4%) | Loss: 0.6475 | Speed: 58.0 batches/s | ETA: 07:42\n",
      "  Train: [30800/57525] ( 53.5%) | Loss: 0.6475 | Speed: 58.0 batches/s | ETA: 07:40\n",
      "  Train: [30900/57525] ( 53.7%) | Loss: 0.6474 | Speed: 58.0 batches/s | ETA: 07:38\n",
      "  Train: [31000/57525] ( 53.9%) | Loss: 0.6474 | Speed: 58.0 batches/s | ETA: 07:37\n",
      "  Train: [31100/57525] ( 54.1%) | Loss: 0.6474 | Speed: 58.0 batches/s | ETA: 07:35\n",
      "  Train: [31200/57525] ( 54.2%) | Loss: 0.6473 | Speed: 58.0 batches/s | ETA: 07:33\n",
      "  Train: [31300/57525] ( 54.4%) | Loss: 0.6473 | Speed: 58.0 batches/s | ETA: 07:31\n",
      "  Train: [31400/57525] ( 54.6%) | Loss: 0.6473 | Speed: 58.0 batches/s | ETA: 07:30\n",
      "  Train: [31500/57525] ( 54.8%) | Loss: 0.6472 | Speed: 58.0 batches/s | ETA: 07:28\n",
      "  Train: [31600/57525] ( 54.9%) | Loss: 0.6472 | Speed: 58.0 batches/s | ETA: 07:26\n",
      "  Train: [31700/57525] ( 55.1%) | Loss: 0.6473 | Speed: 58.0 batches/s | ETA: 07:25\n",
      "  Train: [31800/57525] ( 55.3%) | Loss: 0.6472 | Speed: 58.0 batches/s | ETA: 07:23\n",
      "  Train: [31900/57525] ( 55.5%) | Loss: 0.6472 | Speed: 58.0 batches/s | ETA: 07:21\n",
      "  Train: [32000/57525] ( 55.6%) | Loss: 0.6472 | Speed: 58.0 batches/s | ETA: 07:19\n",
      "  Train: [32100/57525] ( 55.8%) | Loss: 0.6471 | Speed: 58.0 batches/s | ETA: 07:18\n",
      "  Train: [32200/57525] ( 56.0%) | Loss: 0.6471 | Speed: 58.0 batches/s | ETA: 07:16\n",
      "  Train: [32300/57525] ( 56.1%) | Loss: 0.6471 | Speed: 58.0 batches/s | ETA: 07:14\n",
      "  Train: [32400/57525] ( 56.3%) | Loss: 0.6471 | Speed: 58.0 batches/s | ETA: 07:12\n",
      "  Train: [32500/57525] ( 56.5%) | Loss: 0.6471 | Speed: 58.0 batches/s | ETA: 07:11\n",
      "  Train: [32600/57525] ( 56.7%) | Loss: 0.6470 | Speed: 58.0 batches/s | ETA: 07:09\n",
      "  Train: [32700/57525] ( 56.8%) | Loss: 0.6470 | Speed: 58.0 batches/s | ETA: 07:07\n",
      "  Train: [32800/57525] ( 57.0%) | Loss: 0.6470 | Speed: 58.0 batches/s | ETA: 07:05\n",
      "  Train: [32900/57525] ( 57.2%) | Loss: 0.6470 | Speed: 58.1 batches/s | ETA: 07:04\n",
      "  Train: [33000/57525] ( 57.4%) | Loss: 0.6470 | Speed: 58.1 batches/s | ETA: 07:02\n",
      "  Train: [33100/57525] ( 57.5%) | Loss: 0.6470 | Speed: 58.1 batches/s | ETA: 07:00\n",
      "  Train: [33200/57525] ( 57.7%) | Loss: 0.6470 | Speed: 58.1 batches/s | ETA: 06:59\n",
      "  Train: [33300/57525] ( 57.9%) | Loss: 0.6470 | Speed: 58.1 batches/s | ETA: 06:57\n",
      "  Train: [33400/57525] ( 58.1%) | Loss: 0.6470 | Speed: 58.0 batches/s | ETA: 06:55\n",
      "  Train: [33500/57525] ( 58.2%) | Loss: 0.6470 | Speed: 58.0 batches/s | ETA: 06:53\n",
      "  Train: [33600/57525] ( 58.4%) | Loss: 0.6470 | Speed: 58.0 batches/s | ETA: 06:52\n",
      "  Train: [33700/57525] ( 58.6%) | Loss: 0.6470 | Speed: 58.0 batches/s | ETA: 06:50\n",
      "  Train: [33800/57525] ( 58.8%) | Loss: 0.6469 | Speed: 58.0 batches/s | ETA: 06:48\n",
      "  Train: [33900/57525] ( 58.9%) | Loss: 0.6468 | Speed: 58.0 batches/s | ETA: 06:47\n",
      "  Train: [34000/57525] ( 59.1%) | Loss: 0.6468 | Speed: 58.0 batches/s | ETA: 06:45\n",
      "  Train: [34100/57525] ( 59.3%) | Loss: 0.6467 | Speed: 58.0 batches/s | ETA: 06:43\n",
      "  Train: [34200/57525] ( 59.5%) | Loss: 0.6467 | Speed: 58.0 batches/s | ETA: 06:41\n",
      "  Train: [34300/57525] ( 59.6%) | Loss: 0.6467 | Speed: 58.0 batches/s | ETA: 06:40\n",
      "  Train: [34400/57525] ( 59.8%) | Loss: 0.6466 | Speed: 58.0 batches/s | ETA: 06:38\n",
      "  Train: [34500/57525] ( 60.0%) | Loss: 0.6466 | Speed: 58.0 batches/s | ETA: 06:37\n",
      "  Train: [34600/57525] ( 60.1%) | Loss: 0.6465 | Speed: 58.0 batches/s | ETA: 06:35\n",
      "  Train: [34700/57525] ( 60.3%) | Loss: 0.6465 | Speed: 58.0 batches/s | ETA: 06:33\n",
      "  Train: [34800/57525] ( 60.5%) | Loss: 0.6464 | Speed: 58.0 batches/s | ETA: 06:31\n",
      "  Train: [34900/57525] ( 60.7%) | Loss: 0.6463 | Speed: 58.0 batches/s | ETA: 06:30\n",
      "  Train: [35000/57525] ( 60.8%) | Loss: 0.6464 | Speed: 58.0 batches/s | ETA: 06:28\n",
      "  Train: [35100/57525] ( 61.0%) | Loss: 0.6463 | Speed: 58.0 batches/s | ETA: 06:26\n",
      "  Train: [35200/57525] ( 61.2%) | Loss: 0.6462 | Speed: 58.0 batches/s | ETA: 06:24\n",
      "  Train: [35300/57525] ( 61.4%) | Loss: 0.6462 | Speed: 58.0 batches/s | ETA: 06:23\n",
      "  Train: [35400/57525] ( 61.5%) | Loss: 0.6462 | Speed: 58.0 batches/s | ETA: 06:21\n",
      "  Train: [35500/57525] ( 61.7%) | Loss: 0.6462 | Speed: 58.0 batches/s | ETA: 06:19\n",
      "  Train: [35600/57525] ( 61.9%) | Loss: 0.6461 | Speed: 58.0 batches/s | ETA: 06:17\n",
      "  Train: [35700/57525] ( 62.1%) | Loss: 0.6461 | Speed: 58.0 batches/s | ETA: 06:16\n",
      "  Train: [35800/57525] ( 62.2%) | Loss: 0.6460 | Speed: 58.0 batches/s | ETA: 06:14\n",
      "  Train: [35900/57525] ( 62.4%) | Loss: 0.6459 | Speed: 58.0 batches/s | ETA: 06:12\n",
      "  Train: [36000/57525] ( 62.6%) | Loss: 0.6459 | Speed: 58.0 batches/s | ETA: 06:10\n",
      "  Train: [36100/57525] ( 62.8%) | Loss: 0.6459 | Speed: 58.0 batches/s | ETA: 06:09\n",
      "  Train: [36200/57525] ( 62.9%) | Loss: 0.6458 | Speed: 58.0 batches/s | ETA: 06:07\n",
      "  Train: [36300/57525] ( 63.1%) | Loss: 0.6458 | Speed: 58.0 batches/s | ETA: 06:05\n",
      "  Train: [36400/57525] ( 63.3%) | Loss: 0.6457 | Speed: 58.0 batches/s | ETA: 06:03\n",
      "  Train: [36500/57525] ( 63.5%) | Loss: 0.6456 | Speed: 58.0 batches/s | ETA: 06:02\n",
      "  Train: [36600/57525] ( 63.6%) | Loss: 0.6456 | Speed: 58.0 batches/s | ETA: 06:00\n",
      "  Train: [36700/57525] ( 63.8%) | Loss: 0.6456 | Speed: 58.1 batches/s | ETA: 05:58\n",
      "  Train: [36800/57525] ( 64.0%) | Loss: 0.6456 | Speed: 58.1 batches/s | ETA: 05:56\n",
      "  Train: [36900/57525] ( 64.1%) | Loss: 0.6455 | Speed: 58.1 batches/s | ETA: 05:55\n",
      "  Train: [37000/57525] ( 64.3%) | Loss: 0.6455 | Speed: 58.1 batches/s | ETA: 05:53\n",
      "  Train: [37100/57525] ( 64.5%) | Loss: 0.6454 | Speed: 58.1 batches/s | ETA: 05:51\n",
      "  Train: [37200/57525] ( 64.7%) | Loss: 0.6454 | Speed: 58.1 batches/s | ETA: 05:50\n",
      "  Train: [37300/57525] ( 64.8%) | Loss: 0.6454 | Speed: 58.1 batches/s | ETA: 05:48\n",
      "  Train: [37400/57525] ( 65.0%) | Loss: 0.6453 | Speed: 58.1 batches/s | ETA: 05:46\n",
      "  Train: [37500/57525] ( 65.2%) | Loss: 0.6453 | Speed: 58.1 batches/s | ETA: 05:44\n",
      "  Train: [37600/57525] ( 65.4%) | Loss: 0.6453 | Speed: 58.1 batches/s | ETA: 05:43\n",
      "  Train: [37700/57525] ( 65.5%) | Loss: 0.6453 | Speed: 58.1 batches/s | ETA: 05:41\n",
      "  Train: [37800/57525] ( 65.7%) | Loss: 0.6452 | Speed: 58.1 batches/s | ETA: 05:39\n",
      "  Train: [37900/57525] ( 65.9%) | Loss: 0.6452 | Speed: 58.1 batches/s | ETA: 05:37\n",
      "  Train: [38000/57525] ( 66.1%) | Loss: 0.6452 | Speed: 58.1 batches/s | ETA: 05:36\n",
      "  Train: [38100/57525] ( 66.2%) | Loss: 0.6452 | Speed: 58.1 batches/s | ETA: 05:34\n",
      "  Train: [38200/57525] ( 66.4%) | Loss: 0.6451 | Speed: 58.1 batches/s | ETA: 05:32\n",
      "  Train: [38300/57525] ( 66.6%) | Loss: 0.6451 | Speed: 58.1 batches/s | ETA: 05:30\n",
      "  Train: [38400/57525] ( 66.8%) | Loss: 0.6451 | Speed: 58.1 batches/s | ETA: 05:29\n",
      "  Train: [38500/57525] ( 66.9%) | Loss: 0.6451 | Speed: 58.1 batches/s | ETA: 05:27\n",
      "  Train: [38600/57525] ( 67.1%) | Loss: 0.6450 | Speed: 58.1 batches/s | ETA: 05:25\n",
      "  Train: [38700/57525] ( 67.3%) | Loss: 0.6450 | Speed: 58.1 batches/s | ETA: 05:24\n",
      "  Train: [38800/57525] ( 67.4%) | Loss: 0.6450 | Speed: 58.1 batches/s | ETA: 05:22\n",
      "  Train: [38900/57525] ( 67.6%) | Loss: 0.6450 | Speed: 58.1 batches/s | ETA: 05:20\n",
      "  Train: [39000/57525] ( 67.8%) | Loss: 0.6450 | Speed: 58.1 batches/s | ETA: 05:18\n",
      "  Train: [39100/57525] ( 68.0%) | Loss: 0.6450 | Speed: 58.1 batches/s | ETA: 05:17\n",
      "  Train: [39200/57525] ( 68.1%) | Loss: 0.6449 | Speed: 58.1 batches/s | ETA: 05:15\n",
      "  Train: [39300/57525] ( 68.3%) | Loss: 0.6449 | Speed: 58.0 batches/s | ETA: 05:13\n",
      "  Train: [39400/57525] ( 68.5%) | Loss: 0.6449 | Speed: 58.0 batches/s | ETA: 05:12\n",
      "  Train: [39500/57525] ( 68.7%) | Loss: 0.6448 | Speed: 58.0 batches/s | ETA: 05:10\n",
      "  Train: [39600/57525] ( 68.8%) | Loss: 0.6449 | Speed: 58.0 batches/s | ETA: 05:08\n",
      "  Train: [39700/57525] ( 69.0%) | Loss: 0.6449 | Speed: 58.0 batches/s | ETA: 05:07\n",
      "  Train: [39800/57525] ( 69.2%) | Loss: 0.6449 | Speed: 58.0 batches/s | ETA: 05:05\n",
      "  Train: [39900/57525] ( 69.4%) | Loss: 0.6448 | Speed: 58.1 batches/s | ETA: 05:03\n",
      "  Train: [40000/57525] ( 69.5%) | Loss: 0.6448 | Speed: 58.1 batches/s | ETA: 05:01\n",
      "  Train: [40100/57525] ( 69.7%) | Loss: 0.6448 | Speed: 58.1 batches/s | ETA: 05:00\n",
      "  Train: [40200/57525] ( 69.9%) | Loss: 0.6447 | Speed: 58.1 batches/s | ETA: 04:58\n",
      "  Train: [40300/57525] ( 70.1%) | Loss: 0.6447 | Speed: 58.1 batches/s | ETA: 04:56\n",
      "  Train: [40400/57525] ( 70.2%) | Loss: 0.6447 | Speed: 58.1 batches/s | ETA: 04:54\n",
      "  Train: [40500/57525] ( 70.4%) | Loss: 0.6447 | Speed: 58.1 batches/s | ETA: 04:53\n",
      "  Train: [40600/57525] ( 70.6%) | Loss: 0.6446 | Speed: 58.1 batches/s | ETA: 04:51\n",
      "  Train: [40700/57525] ( 70.8%) | Loss: 0.6446 | Speed: 58.1 batches/s | ETA: 04:49\n",
      "  Train: [40800/57525] ( 70.9%) | Loss: 0.6446 | Speed: 58.1 batches/s | ETA: 04:48\n",
      "  Train: [40900/57525] ( 71.1%) | Loss: 0.6445 | Speed: 58.1 batches/s | ETA: 04:46\n",
      "  Train: [41000/57525] ( 71.3%) | Loss: 0.6445 | Speed: 58.1 batches/s | ETA: 04:44\n",
      "  Train: [41100/57525] ( 71.4%) | Loss: 0.6445 | Speed: 58.1 batches/s | ETA: 04:42\n",
      "  Train: [41200/57525] ( 71.6%) | Loss: 0.6445 | Speed: 58.1 batches/s | ETA: 04:41\n",
      "  Train: [41300/57525] ( 71.8%) | Loss: 0.6444 | Speed: 58.1 batches/s | ETA: 04:39\n",
      "  Train: [41400/57525] ( 72.0%) | Loss: 0.6444 | Speed: 58.1 batches/s | ETA: 04:37\n",
      "  Train: [41500/57525] ( 72.1%) | Loss: 0.6443 | Speed: 58.1 batches/s | ETA: 04:35\n",
      "  Train: [41600/57525] ( 72.3%) | Loss: 0.6443 | Speed: 58.1 batches/s | ETA: 04:34\n",
      "  Train: [41700/57525] ( 72.5%) | Loss: 0.6443 | Speed: 58.1 batches/s | ETA: 04:32\n",
      "  Train: [41800/57525] ( 72.7%) | Loss: 0.6443 | Speed: 58.1 batches/s | ETA: 04:30\n",
      "  Train: [41900/57525] ( 72.8%) | Loss: 0.6443 | Speed: 58.1 batches/s | ETA: 04:28\n",
      "  Train: [42000/57525] ( 73.0%) | Loss: 0.6443 | Speed: 58.1 batches/s | ETA: 04:27\n",
      "  Train: [42100/57525] ( 73.2%) | Loss: 0.6443 | Speed: 58.1 batches/s | ETA: 04:25\n",
      "  Train: [42200/57525] ( 73.4%) | Loss: 0.6443 | Speed: 58.1 batches/s | ETA: 04:23\n",
      "  Train: [42300/57525] ( 73.5%) | Loss: 0.6442 | Speed: 58.1 batches/s | ETA: 04:22\n",
      "  Train: [42400/57525] ( 73.7%) | Loss: 0.6442 | Speed: 58.1 batches/s | ETA: 04:20\n",
      "  Train: [42500/57525] ( 73.9%) | Loss: 0.6442 | Speed: 58.1 batches/s | ETA: 04:18\n",
      "  Train: [42600/57525] ( 74.1%) | Loss: 0.6442 | Speed: 58.1 batches/s | ETA: 04:16\n",
      "  Train: [42700/57525] ( 74.2%) | Loss: 0.6442 | Speed: 58.1 batches/s | ETA: 04:15\n",
      "  Train: [42800/57525] ( 74.4%) | Loss: 0.6442 | Speed: 58.1 batches/s | ETA: 04:13\n",
      "  Train: [42900/57525] ( 74.6%) | Loss: 0.6442 | Speed: 58.1 batches/s | ETA: 04:11\n",
      "  Train: [43000/57525] ( 74.8%) | Loss: 0.6441 | Speed: 58.1 batches/s | ETA: 04:10\n",
      "  Train: [43100/57525] ( 74.9%) | Loss: 0.6441 | Speed: 58.0 batches/s | ETA: 04:08\n",
      "  Train: [43200/57525] ( 75.1%) | Loss: 0.6441 | Speed: 58.0 batches/s | ETA: 04:06\n",
      "  Train: [43300/57525] ( 75.3%) | Loss: 0.6441 | Speed: 58.0 batches/s | ETA: 04:05\n",
      "  Train: [43400/57525] ( 75.4%) | Loss: 0.6440 | Speed: 58.0 batches/s | ETA: 04:03\n",
      "  Train: [43500/57525] ( 75.6%) | Loss: 0.6440 | Speed: 58.0 batches/s | ETA: 04:01\n",
      "  Train: [43600/57525] ( 75.8%) | Loss: 0.6440 | Speed: 58.0 batches/s | ETA: 03:59\n",
      "  Train: [43700/57525] ( 76.0%) | Loss: 0.6439 | Speed: 58.0 batches/s | ETA: 03:58\n",
      "  Train: [43800/57525] ( 76.1%) | Loss: 0.6438 | Speed: 58.0 batches/s | ETA: 03:56\n",
      "  Train: [43900/57525] ( 76.3%) | Loss: 0.6438 | Speed: 58.0 batches/s | ETA: 03:54\n",
      "  Train: [44000/57525] ( 76.5%) | Loss: 0.6437 | Speed: 58.0 batches/s | ETA: 03:53\n",
      "  Train: [44100/57525] ( 76.7%) | Loss: 0.6437 | Speed: 58.0 batches/s | ETA: 03:51\n",
      "  Train: [44200/57525] ( 76.8%) | Loss: 0.6436 | Speed: 58.0 batches/s | ETA: 03:49\n",
      "  Train: [44300/57525] ( 77.0%) | Loss: 0.6435 | Speed: 58.0 batches/s | ETA: 03:48\n",
      "  Train: [44400/57525] ( 77.2%) | Loss: 0.6435 | Speed: 58.0 batches/s | ETA: 03:46\n",
      "  Train: [44500/57525] ( 77.4%) | Loss: 0.6435 | Speed: 58.0 batches/s | ETA: 03:44\n",
      "  Train: [44600/57525] ( 77.5%) | Loss: 0.6435 | Speed: 58.0 batches/s | ETA: 03:42\n",
      "  Train: [44700/57525] ( 77.7%) | Loss: 0.6434 | Speed: 58.0 batches/s | ETA: 03:41\n",
      "  Train: [44800/57525] ( 77.9%) | Loss: 0.6434 | Speed: 58.0 batches/s | ETA: 03:39\n",
      "  Train: [44900/57525] ( 78.1%) | Loss: 0.6434 | Speed: 58.0 batches/s | ETA: 03:37\n",
      "  Train: [45000/57525] ( 78.2%) | Loss: 0.6434 | Speed: 58.0 batches/s | ETA: 03:35\n",
      "  Train: [45100/57525] ( 78.4%) | Loss: 0.6434 | Speed: 58.0 batches/s | ETA: 03:34\n",
      "  Train: [45200/57525] ( 78.6%) | Loss: 0.6434 | Speed: 58.0 batches/s | ETA: 03:32\n",
      "  Train: [45300/57525] ( 78.7%) | Loss: 0.6433 | Speed: 58.0 batches/s | ETA: 03:30\n",
      "  Train: [45400/57525] ( 78.9%) | Loss: 0.6433 | Speed: 58.0 batches/s | ETA: 03:29\n",
      "  Train: [45500/57525] ( 79.1%) | Loss: 0.6433 | Speed: 58.0 batches/s | ETA: 03:27\n",
      "  Train: [45600/57525] ( 79.3%) | Loss: 0.6432 | Speed: 58.0 batches/s | ETA: 03:25\n",
      "  Train: [45700/57525] ( 79.4%) | Loss: 0.6432 | Speed: 58.0 batches/s | ETA: 03:23\n",
      "  Train: [45800/57525] ( 79.6%) | Loss: 0.6431 | Speed: 58.0 batches/s | ETA: 03:22\n",
      "  Train: [45900/57525] ( 79.8%) | Loss: 0.6431 | Speed: 58.0 batches/s | ETA: 03:20\n",
      "  Train: [46000/57525] ( 80.0%) | Loss: 0.6431 | Speed: 58.0 batches/s | ETA: 03:18\n",
      "  Train: [46100/57525] ( 80.1%) | Loss: 0.6432 | Speed: 58.0 batches/s | ETA: 03:16\n",
      "  Train: [46200/57525] ( 80.3%) | Loss: 0.6431 | Speed: 58.0 batches/s | ETA: 03:15\n",
      "  Train: [46300/57525] ( 80.5%) | Loss: 0.6431 | Speed: 58.0 batches/s | ETA: 03:13\n",
      "  Train: [46400/57525] ( 80.7%) | Loss: 0.6430 | Speed: 58.0 batches/s | ETA: 03:11\n",
      "  Train: [46500/57525] ( 80.8%) | Loss: 0.6430 | Speed: 58.0 batches/s | ETA: 03:10\n",
      "  Train: [46600/57525] ( 81.0%) | Loss: 0.6430 | Speed: 58.0 batches/s | ETA: 03:08\n",
      "  Train: [46700/57525] ( 81.2%) | Loss: 0.6430 | Speed: 58.0 batches/s | ETA: 03:06\n",
      "  Train: [46800/57525] ( 81.4%) | Loss: 0.6430 | Speed: 58.0 batches/s | ETA: 03:04\n",
      "  Train: [46900/57525] ( 81.5%) | Loss: 0.6430 | Speed: 58.0 batches/s | ETA: 03:03\n",
      "  Train: [47000/57525] ( 81.7%) | Loss: 0.6429 | Speed: 58.0 batches/s | ETA: 03:01\n",
      "  Train: [47100/57525] ( 81.9%) | Loss: 0.6429 | Speed: 58.0 batches/s | ETA: 02:59\n",
      "  Train: [47200/57525] ( 82.1%) | Loss: 0.6428 | Speed: 58.0 batches/s | ETA: 02:57\n",
      "  Train: [47300/57525] ( 82.2%) | Loss: 0.6428 | Speed: 58.0 batches/s | ETA: 02:56\n",
      "  Train: [47400/57525] ( 82.4%) | Loss: 0.6427 | Speed: 58.0 batches/s | ETA: 02:54\n",
      "  Train: [47500/57525] ( 82.6%) | Loss: 0.6427 | Speed: 58.0 batches/s | ETA: 02:52\n",
      "  Train: [47600/57525] ( 82.7%) | Loss: 0.6426 | Speed: 58.0 batches/s | ETA: 02:51\n",
      "  Train: [47700/57525] ( 82.9%) | Loss: 0.6426 | Speed: 58.0 batches/s | ETA: 02:49\n",
      "  Train: [47800/57525] ( 83.1%) | Loss: 0.6425 | Speed: 58.0 batches/s | ETA: 02:47\n",
      "  Train: [47900/57525] ( 83.3%) | Loss: 0.6424 | Speed: 58.0 batches/s | ETA: 02:45\n",
      "  Train: [48000/57525] ( 83.4%) | Loss: 0.6424 | Speed: 58.0 batches/s | ETA: 02:44\n",
      "  Train: [48100/57525] ( 83.6%) | Loss: 0.6423 | Speed: 58.0 batches/s | ETA: 02:42\n",
      "  Train: [48200/57525] ( 83.8%) | Loss: 0.6423 | Speed: 58.0 batches/s | ETA: 02:40\n",
      "  Train: [48300/57525] ( 84.0%) | Loss: 0.6423 | Speed: 58.0 batches/s | ETA: 02:38\n",
      "  Train: [48400/57525] ( 84.1%) | Loss: 0.6422 | Speed: 58.0 batches/s | ETA: 02:37\n",
      "  Train: [48500/57525] ( 84.3%) | Loss: 0.6422 | Speed: 58.0 batches/s | ETA: 02:35\n",
      "  Train: [48600/57525] ( 84.5%) | Loss: 0.6422 | Speed: 58.0 batches/s | ETA: 02:33\n",
      "  Train: [48700/57525] ( 84.7%) | Loss: 0.6421 | Speed: 58.0 batches/s | ETA: 02:32\n",
      "  Train: [48800/57525] ( 84.8%) | Loss: 0.6421 | Speed: 58.0 batches/s | ETA: 02:30\n",
      "  Train: [48900/57525] ( 85.0%) | Loss: 0.6421 | Speed: 58.0 batches/s | ETA: 02:28\n",
      "  Train: [49000/57525] ( 85.2%) | Loss: 0.6421 | Speed: 58.0 batches/s | ETA: 02:26\n",
      "  Train: [49100/57525] ( 85.4%) | Loss: 0.6420 | Speed: 58.0 batches/s | ETA: 02:25\n",
      "  Train: [49200/57525] ( 85.5%) | Loss: 0.6420 | Speed: 58.0 batches/s | ETA: 02:23\n",
      "  Train: [49300/57525] ( 85.7%) | Loss: 0.6420 | Speed: 58.0 batches/s | ETA: 02:21\n",
      "  Train: [49400/57525] ( 85.9%) | Loss: 0.6420 | Speed: 58.0 batches/s | ETA: 02:20\n",
      "  Train: [49500/57525] ( 86.0%) | Loss: 0.6420 | Speed: 58.0 batches/s | ETA: 02:18\n",
      "  Train: [49600/57525] ( 86.2%) | Loss: 0.6419 | Speed: 58.0 batches/s | ETA: 02:16\n",
      "  Train: [49700/57525] ( 86.4%) | Loss: 0.6419 | Speed: 58.0 batches/s | ETA: 02:14\n",
      "  Train: [49800/57525] ( 86.6%) | Loss: 0.6418 | Speed: 58.0 batches/s | ETA: 02:13\n",
      "  Train: [49900/57525] ( 86.7%) | Loss: 0.6419 | Speed: 58.0 batches/s | ETA: 02:11\n",
      "  Train: [50000/57525] ( 86.9%) | Loss: 0.6418 | Speed: 58.0 batches/s | ETA: 02:09\n",
      "  Train: [50100/57525] ( 87.1%) | Loss: 0.6418 | Speed: 58.0 batches/s | ETA: 02:07\n",
      "  Train: [50200/57525] ( 87.3%) | Loss: 0.6418 | Speed: 58.0 batches/s | ETA: 02:06\n",
      "  Train: [50300/57525] ( 87.4%) | Loss: 0.6417 | Speed: 58.0 batches/s | ETA: 02:04\n",
      "  Train: [50400/57525] ( 87.6%) | Loss: 0.6417 | Speed: 58.0 batches/s | ETA: 02:02\n",
      "  Train: [50500/57525] ( 87.8%) | Loss: 0.6416 | Speed: 58.0 batches/s | ETA: 02:01\n",
      "  Train: [50600/57525] ( 88.0%) | Loss: 0.6416 | Speed: 58.0 batches/s | ETA: 01:59\n",
      "  Train: [50700/57525] ( 88.1%) | Loss: 0.6416 | Speed: 58.0 batches/s | ETA: 01:57\n",
      "  Train: [50800/57525] ( 88.3%) | Loss: 0.6415 | Speed: 58.0 batches/s | ETA: 01:55\n",
      "  Train: [50900/57525] ( 88.5%) | Loss: 0.6415 | Speed: 58.0 batches/s | ETA: 01:54\n",
      "  Train: [51000/57525] ( 88.7%) | Loss: 0.6415 | Speed: 58.0 batches/s | ETA: 01:52\n",
      "  Train: [51100/57525] ( 88.8%) | Loss: 0.6415 | Speed: 58.0 batches/s | ETA: 01:50\n",
      "  Train: [51200/57525] ( 89.0%) | Loss: 0.6414 | Speed: 58.0 batches/s | ETA: 01:49\n",
      "  Train: [51300/57525] ( 89.2%) | Loss: 0.6414 | Speed: 58.0 batches/s | ETA: 01:47\n",
      "  Train: [51400/57525] ( 89.4%) | Loss: 0.6413 | Speed: 58.0 batches/s | ETA: 01:45\n",
      "  Train: [51500/57525] ( 89.5%) | Loss: 0.6414 | Speed: 58.0 batches/s | ETA: 01:43\n",
      "  Train: [51600/57525] ( 89.7%) | Loss: 0.6414 | Speed: 58.0 batches/s | ETA: 01:42\n",
      "  Train: [51700/57525] ( 89.9%) | Loss: 0.6414 | Speed: 58.0 batches/s | ETA: 01:40\n",
      "  Train: [51800/57525] ( 90.0%) | Loss: 0.6413 | Speed: 58.0 batches/s | ETA: 01:38\n",
      "  Train: [51900/57525] ( 90.2%) | Loss: 0.6413 | Speed: 58.0 batches/s | ETA: 01:37\n",
      "  Train: [52000/57525] ( 90.4%) | Loss: 0.6412 | Speed: 58.0 batches/s | ETA: 01:35\n",
      "  Train: [52100/57525] ( 90.6%) | Loss: 0.6412 | Speed: 58.0 batches/s | ETA: 01:33\n",
      "  Train: [52200/57525] ( 90.7%) | Loss: 0.6411 | Speed: 58.0 batches/s | ETA: 01:31\n",
      "  Train: [52300/57525] ( 90.9%) | Loss: 0.6411 | Speed: 58.0 batches/s | ETA: 01:30\n",
      "  Train: [52400/57525] ( 91.1%) | Loss: 0.6411 | Speed: 58.0 batches/s | ETA: 01:28\n",
      "  Train: [52500/57525] ( 91.3%) | Loss: 0.6411 | Speed: 58.0 batches/s | ETA: 01:26\n",
      "  Train: [52600/57525] ( 91.4%) | Loss: 0.6410 | Speed: 58.0 batches/s | ETA: 01:24\n",
      "  Train: [52700/57525] ( 91.6%) | Loss: 0.6410 | Speed: 58.0 batches/s | ETA: 01:23\n",
      "  Train: [52800/57525] ( 91.8%) | Loss: 0.6410 | Speed: 58.0 batches/s | ETA: 01:21\n",
      "  Train: [52900/57525] ( 92.0%) | Loss: 0.6410 | Speed: 58.0 batches/s | ETA: 01:19\n",
      "  Train: [53000/57525] ( 92.1%) | Loss: 0.6409 | Speed: 57.9 batches/s | ETA: 01:18\n",
      "  Train: [53100/57525] ( 92.3%) | Loss: 0.6409 | Speed: 57.9 batches/s | ETA: 01:16\n",
      "  Train: [53200/57525] ( 92.5%) | Loss: 0.6409 | Speed: 57.9 batches/s | ETA: 01:14\n",
      "  Train: [53300/57525] ( 92.7%) | Loss: 0.6409 | Speed: 57.9 batches/s | ETA: 01:12\n",
      "  Train: [53400/57525] ( 92.8%) | Loss: 0.6408 | Speed: 57.9 batches/s | ETA: 01:11\n",
      "  Train: [53500/57525] ( 93.0%) | Loss: 0.6408 | Speed: 57.9 batches/s | ETA: 01:09\n",
      "  Train: [53600/57525] ( 93.2%) | Loss: 0.6408 | Speed: 57.9 batches/s | ETA: 01:07\n",
      "  Train: [53700/57525] ( 93.4%) | Loss: 0.6407 | Speed: 57.9 batches/s | ETA: 01:06\n",
      "  Train: [53800/57525] ( 93.5%) | Loss: 0.6407 | Speed: 57.9 batches/s | ETA: 01:04\n",
      "  Train: [53900/57525] ( 93.7%) | Loss: 0.6407 | Speed: 57.9 batches/s | ETA: 01:02\n",
      "  Train: [54000/57525] ( 93.9%) | Loss: 0.6407 | Speed: 57.9 batches/s | ETA: 01:00\n",
      "  Train: [54100/57525] ( 94.0%) | Loss: 0.6407 | Speed: 57.9 batches/s | ETA: 00:59\n",
      "  Train: [54200/57525] ( 94.2%) | Loss: 0.6407 | Speed: 57.9 batches/s | ETA: 00:57\n",
      "  Train: [54300/57525] ( 94.4%) | Loss: 0.6406 | Speed: 57.9 batches/s | ETA: 00:55\n",
      "  Train: [54400/57525] ( 94.6%) | Loss: 0.6407 | Speed: 57.9 batches/s | ETA: 00:53\n",
      "  Train: [54500/57525] ( 94.7%) | Loss: 0.6406 | Speed: 57.9 batches/s | ETA: 00:52\n",
      "  Train: [54600/57525] ( 94.9%) | Loss: 0.6406 | Speed: 57.9 batches/s | ETA: 00:50\n",
      "  Train: [54700/57525] ( 95.1%) | Loss: 0.6406 | Speed: 57.9 batches/s | ETA: 00:48\n",
      "  Train: [54800/57525] ( 95.3%) | Loss: 0.6405 | Speed: 57.9 batches/s | ETA: 00:47\n",
      "  Train: [54900/57525] ( 95.4%) | Loss: 0.6405 | Speed: 57.9 batches/s | ETA: 00:45\n",
      "  Train: [55000/57525] ( 95.6%) | Loss: 0.6405 | Speed: 57.9 batches/s | ETA: 00:43\n",
      "  Train: [55100/57525] ( 95.8%) | Loss: 0.6404 | Speed: 57.9 batches/s | ETA: 00:41\n",
      "  Train: [55200/57525] ( 96.0%) | Loss: 0.6404 | Speed: 58.0 batches/s | ETA: 00:40\n",
      "  Train: [55300/57525] ( 96.1%) | Loss: 0.6404 | Speed: 58.0 batches/s | ETA: 00:38\n",
      "  Train: [55400/57525] ( 96.3%) | Loss: 0.6404 | Speed: 58.0 batches/s | ETA: 00:36\n",
      "  Train: [55500/57525] ( 96.5%) | Loss: 0.6404 | Speed: 58.0 batches/s | ETA: 00:34\n",
      "  Train: [55600/57525] ( 96.7%) | Loss: 0.6404 | Speed: 58.0 batches/s | ETA: 00:33\n",
      "  Train: [55700/57525] ( 96.8%) | Loss: 0.6403 | Speed: 58.0 batches/s | ETA: 00:31\n",
      "  Train: [55800/57525] ( 97.0%) | Loss: 0.6403 | Speed: 58.0 batches/s | ETA: 00:29\n",
      "  Train: [55900/57525] ( 97.2%) | Loss: 0.6402 | Speed: 58.0 batches/s | ETA: 00:28\n",
      "  Train: [56000/57525] ( 97.3%) | Loss: 0.6402 | Speed: 58.0 batches/s | ETA: 00:26\n",
      "  Train: [56100/57525] ( 97.5%) | Loss: 0.6402 | Speed: 58.0 batches/s | ETA: 00:24\n",
      "  Train: [56200/57525] ( 97.7%) | Loss: 0.6401 | Speed: 58.0 batches/s | ETA: 00:22\n",
      "  Train: [56300/57525] ( 97.9%) | Loss: 0.6401 | Speed: 58.0 batches/s | ETA: 00:21\n",
      "  Train: [56400/57525] ( 98.0%) | Loss: 0.6400 | Speed: 58.0 batches/s | ETA: 00:19\n",
      "  Train: [56500/57525] ( 98.2%) | Loss: 0.6400 | Speed: 58.0 batches/s | ETA: 00:17\n",
      "  Train: [56600/57525] ( 98.4%) | Loss: 0.6400 | Speed: 58.0 batches/s | ETA: 00:15\n",
      "  Train: [56700/57525] ( 98.6%) | Loss: 0.6400 | Speed: 58.0 batches/s | ETA: 00:14\n",
      "  Train: [56800/57525] ( 98.7%) | Loss: 0.6400 | Speed: 58.0 batches/s | ETA: 00:12\n",
      "  Train: [56900/57525] ( 98.9%) | Loss: 0.6400 | Speed: 58.0 batches/s | ETA: 00:10\n",
      "  Train: [57000/57525] ( 99.1%) | Loss: 0.6400 | Speed: 58.0 batches/s | ETA: 00:09\n",
      "  Train: [57100/57525] ( 99.3%) | Loss: 0.6400 | Speed: 58.0 batches/s | ETA: 00:07\n",
      "  Train: [57200/57525] ( 99.4%) | Loss: 0.6400 | Speed: 58.0 batches/s | ETA: 00:05\n",
      "  Train: [57300/57525] ( 99.6%) | Loss: 0.6400 | Speed: 58.0 batches/s | ETA: 00:03\n",
      "  Train: [57400/57525] ( 99.8%) | Loss: 0.6400 | Speed: 58.0 batches/s | ETA: 00:02\n",
      "  Train: [57500/57525] (100.0%) | Loss: 0.6399 | Speed: 58.0 batches/s | ETA: 00:00\n",
      "  Running validation...\n",
      "    Val: [  50/7191] (  0.7%)\n",
      "    Val: [ 100/7191] (  1.4%)\n",
      "    Val: [ 150/7191] (  2.1%)\n",
      "    Val: [ 200/7191] (  2.8%)\n",
      "    Val: [ 250/7191] (  3.5%)\n",
      "    Val: [ 300/7191] (  4.2%)\n",
      "    Val: [ 350/7191] (  4.9%)\n",
      "    Val: [ 400/7191] (  5.6%)\n",
      "    Val: [ 450/7191] (  6.3%)\n",
      "    Val: [ 500/7191] (  7.0%)\n",
      "    Val: [ 550/7191] (  7.6%)\n",
      "    Val: [ 600/7191] (  8.3%)\n",
      "    Val: [ 650/7191] (  9.0%)\n",
      "    Val: [ 700/7191] (  9.7%)\n",
      "    Val: [ 750/7191] ( 10.4%)\n",
      "    Val: [ 800/7191] ( 11.1%)\n",
      "    Val: [ 850/7191] ( 11.8%)\n",
      "    Val: [ 900/7191] ( 12.5%)\n",
      "    Val: [ 950/7191] ( 13.2%)\n",
      "    Val: [1000/7191] ( 13.9%)\n",
      "    Val: [1050/7191] ( 14.6%)\n",
      "    Val: [1100/7191] ( 15.3%)\n",
      "    Val: [1150/7191] ( 16.0%)\n",
      "    Val: [1200/7191] ( 16.7%)\n",
      "    Val: [1250/7191] ( 17.4%)\n",
      "    Val: [1300/7191] ( 18.1%)\n",
      "    Val: [1350/7191] ( 18.8%)\n",
      "    Val: [1400/7191] ( 19.5%)\n",
      "    Val: [1450/7191] ( 20.2%)\n",
      "    Val: [1500/7191] ( 20.9%)\n",
      "    Val: [1550/7191] ( 21.6%)\n",
      "    Val: [1600/7191] ( 22.3%)\n",
      "    Val: [1650/7191] ( 22.9%)\n",
      "    Val: [1700/7191] ( 23.6%)\n",
      "    Val: [1750/7191] ( 24.3%)\n",
      "    Val: [1800/7191] ( 25.0%)\n",
      "    Val: [1850/7191] ( 25.7%)\n",
      "    Val: [1900/7191] ( 26.4%)\n",
      "    Val: [1950/7191] ( 27.1%)\n",
      "    Val: [2000/7191] ( 27.8%)\n",
      "    Val: [2050/7191] ( 28.5%)\n",
      "    Val: [2100/7191] ( 29.2%)\n",
      "    Val: [2150/7191] ( 29.9%)\n",
      "    Val: [2200/7191] ( 30.6%)\n",
      "    Val: [2250/7191] ( 31.3%)\n",
      "    Val: [2300/7191] ( 32.0%)\n",
      "    Val: [2350/7191] ( 32.7%)\n",
      "    Val: [2400/7191] ( 33.4%)\n",
      "    Val: [2450/7191] ( 34.1%)\n",
      "    Val: [2500/7191] ( 34.8%)\n",
      "    Val: [2550/7191] ( 35.5%)\n",
      "    Val: [2600/7191] ( 36.2%)\n",
      "    Val: [2650/7191] ( 36.9%)\n",
      "    Val: [2700/7191] ( 37.5%)\n",
      "    Val: [2750/7191] ( 38.2%)\n",
      "    Val: [2800/7191] ( 38.9%)\n",
      "    Val: [2850/7191] ( 39.6%)\n",
      "    Val: [2900/7191] ( 40.3%)\n",
      "    Val: [2950/7191] ( 41.0%)\n",
      "    Val: [3000/7191] ( 41.7%)\n",
      "    Val: [3050/7191] ( 42.4%)\n",
      "    Val: [3100/7191] ( 43.1%)\n",
      "    Val: [3150/7191] ( 43.8%)\n",
      "    Val: [3200/7191] ( 44.5%)\n",
      "    Val: [3250/7191] ( 45.2%)\n",
      "    Val: [3300/7191] ( 45.9%)\n",
      "    Val: [3350/7191] ( 46.6%)\n",
      "    Val: [3400/7191] ( 47.3%)\n",
      "    Val: [3450/7191] ( 48.0%)\n",
      "    Val: [3500/7191] ( 48.7%)\n",
      "    Val: [3550/7191] ( 49.4%)\n",
      "    Val: [3600/7191] ( 50.1%)\n",
      "    Val: [3650/7191] ( 50.8%)\n",
      "    Val: [3700/7191] ( 51.5%)\n",
      "    Val: [3750/7191] ( 52.1%)\n",
      "    Val: [3800/7191] ( 52.8%)\n",
      "    Val: [3850/7191] ( 53.5%)\n",
      "    Val: [3900/7191] ( 54.2%)\n",
      "    Val: [3950/7191] ( 54.9%)\n",
      "    Val: [4000/7191] ( 55.6%)\n",
      "    Val: [4050/7191] ( 56.3%)\n",
      "    Val: [4100/7191] ( 57.0%)\n",
      "    Val: [4150/7191] ( 57.7%)\n",
      "    Val: [4200/7191] ( 58.4%)\n",
      "    Val: [4250/7191] ( 59.1%)\n",
      "    Val: [4300/7191] ( 59.8%)\n",
      "    Val: [4350/7191] ( 60.5%)\n",
      "    Val: [4400/7191] ( 61.2%)\n",
      "    Val: [4450/7191] ( 61.9%)\n",
      "    Val: [4500/7191] ( 62.6%)\n",
      "    Val: [4550/7191] ( 63.3%)\n",
      "    Val: [4600/7191] ( 64.0%)\n",
      "    Val: [4650/7191] ( 64.7%)\n",
      "    Val: [4700/7191] ( 65.4%)\n",
      "    Val: [4750/7191] ( 66.1%)\n",
      "    Val: [4800/7191] ( 66.8%)\n",
      "    Val: [4850/7191] ( 67.4%)\n",
      "    Val: [4900/7191] ( 68.1%)\n",
      "    Val: [4950/7191] ( 68.8%)\n",
      "    Val: [5000/7191] ( 69.5%)\n",
      "    Val: [5050/7191] ( 70.2%)\n",
      "    Val: [5100/7191] ( 70.9%)\n",
      "    Val: [5150/7191] ( 71.6%)\n",
      "    Val: [5200/7191] ( 72.3%)\n",
      "    Val: [5250/7191] ( 73.0%)\n",
      "    Val: [5300/7191] ( 73.7%)\n",
      "    Val: [5350/7191] ( 74.4%)\n",
      "    Val: [5400/7191] ( 75.1%)\n",
      "    Val: [5450/7191] ( 75.8%)\n",
      "    Val: [5500/7191] ( 76.5%)\n",
      "    Val: [5550/7191] ( 77.2%)\n",
      "    Val: [5600/7191] ( 77.9%)\n",
      "    Val: [5650/7191] ( 78.6%)\n",
      "    Val: [5700/7191] ( 79.3%)\n",
      "    Val: [5750/7191] ( 80.0%)\n",
      "    Val: [5800/7191] ( 80.7%)\n",
      "    Val: [5850/7191] ( 81.4%)\n",
      "    Val: [5900/7191] ( 82.0%)\n",
      "    Val: [5950/7191] ( 82.7%)\n",
      "    Val: [6000/7191] ( 83.4%)\n",
      "    Val: [6050/7191] ( 84.1%)\n",
      "    Val: [6100/7191] ( 84.8%)\n",
      "    Val: [6150/7191] ( 85.5%)\n",
      "    Val: [6200/7191] ( 86.2%)\n",
      "    Val: [6250/7191] ( 86.9%)\n",
      "    Val: [6300/7191] ( 87.6%)\n",
      "    Val: [6350/7191] ( 88.3%)\n",
      "    Val: [6400/7191] ( 89.0%)\n",
      "    Val: [6450/7191] ( 89.7%)\n",
      "    Val: [6500/7191] ( 90.4%)\n",
      "    Val: [6550/7191] ( 91.1%)\n",
      "    Val: [6600/7191] ( 91.8%)\n",
      "    Val: [6650/7191] ( 92.5%)\n",
      "    Val: [6700/7191] ( 93.2%)\n",
      "    Val: [6750/7191] ( 93.9%)\n",
      "    Val: [6800/7191] ( 94.6%)\n",
      "    Val: [6850/7191] ( 95.3%)\n",
      "    Val: [6900/7191] ( 96.0%)\n",
      "    Val: [6950/7191] ( 96.6%)\n",
      "    Val: [7000/7191] ( 97.3%)\n",
      "    Val: [7050/7191] ( 98.0%)\n",
      "    Val: [7100/7191] ( 98.7%)\n",
      "    Val: [7150/7191] ( 99.4%)\n",
      "\n",
      "  Epoch 02 Summary:\n",
      "    Train MSE (norm): 0.6399 | Val MSE (norm): 0.6054\n",
      "    Time: Train=16.5min, Val=0.6min, Total=17.1min\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Epoch 03/3\n",
      "============================================================\n",
      "  Train: [  100/57525] (  0.2%) | Loss: 0.6342 | Speed: 35.0 batches/s | ETA: 27:21\n",
      "  Train: [  200/57525] (  0.3%) | Loss: 0.6242 | Speed: 43.7 batches/s | ETA: 21:52\n",
      "  Train: [  300/57525] (  0.5%) | Loss: 0.6296 | Speed: 47.7 batches/s | ETA: 20:00\n",
      "  Train: [  400/57525] (  0.7%) | Loss: 0.6281 | Speed: 50.0 batches/s | ETA: 19:03\n",
      "  Train: [  500/57525] (  0.9%) | Loss: 0.6247 | Speed: 51.5 batches/s | ETA: 18:27\n",
      "  Train: [  600/57525] (  1.0%) | Loss: 0.6254 | Speed: 52.6 batches/s | ETA: 18:02\n",
      "  Train: [  700/57525] (  1.2%) | Loss: 0.6248 | Speed: 53.4 batches/s | ETA: 17:44\n",
      "  Train: [  800/57525] (  1.4%) | Loss: 0.6233 | Speed: 54.0 batches/s | ETA: 17:30\n",
      "  Train: [  900/57525] (  1.6%) | Loss: 0.6230 | Speed: 54.5 batches/s | ETA: 17:18\n",
      "  Train: [ 1000/57525] (  1.7%) | Loss: 0.6229 | Speed: 55.0 batches/s | ETA: 17:08\n",
      "  Train: [ 1100/57525] (  1.9%) | Loss: 0.6251 | Speed: 55.4 batches/s | ETA: 16:59\n",
      "  Train: [ 1200/57525] (  2.1%) | Loss: 0.6234 | Speed: 55.5 batches/s | ETA: 16:55\n",
      "  Train: [ 1300/57525] (  2.3%) | Loss: 0.6219 | Speed: 55.5 batches/s | ETA: 16:52\n",
      "  Train: [ 1400/57525] (  2.4%) | Loss: 0.6202 | Speed: 55.8 batches/s | ETA: 16:45\n",
      "  Train: [ 1500/57525] (  2.6%) | Loss: 0.6214 | Speed: 56.0 batches/s | ETA: 16:40\n",
      "  Train: [ 1600/57525] (  2.8%) | Loss: 0.6226 | Speed: 56.0 batches/s | ETA: 16:39\n",
      "  Train: [ 1700/57525] (  3.0%) | Loss: 0.6221 | Speed: 56.1 batches/s | ETA: 16:34\n",
      "  Train: [ 1800/57525] (  3.1%) | Loss: 0.6226 | Speed: 56.1 batches/s | ETA: 16:32\n",
      "  Train: [ 1900/57525] (  3.3%) | Loss: 0.6228 | Speed: 56.2 batches/s | ETA: 16:30\n",
      "  Train: [ 2000/57525] (  3.5%) | Loss: 0.6231 | Speed: 56.3 batches/s | ETA: 16:25\n",
      "  Train: [ 2100/57525] (  3.7%) | Loss: 0.6232 | Speed: 56.5 batches/s | ETA: 16:21\n",
      "  Train: [ 2200/57525] (  3.8%) | Loss: 0.6227 | Speed: 56.6 batches/s | ETA: 16:18\n",
      "  Train: [ 2300/57525] (  4.0%) | Loss: 0.6227 | Speed: 56.7 batches/s | ETA: 16:14\n",
      "  Train: [ 2400/57525] (  4.2%) | Loss: 0.6227 | Speed: 56.8 batches/s | ETA: 16:10\n",
      "  Train: [ 2500/57525] (  4.3%) | Loss: 0.6239 | Speed: 56.8 batches/s | ETA: 16:08\n",
      "  Train: [ 2600/57525] (  4.5%) | Loss: 0.6240 | Speed: 56.9 batches/s | ETA: 16:04\n",
      "  Train: [ 2700/57525] (  4.7%) | Loss: 0.6236 | Speed: 57.0 batches/s | ETA: 16:01\n",
      "  Train: [ 2800/57525] (  4.9%) | Loss: 0.6236 | Speed: 57.1 batches/s | ETA: 15:58\n",
      "  Train: [ 2900/57525] (  5.0%) | Loss: 0.6233 | Speed: 57.1 batches/s | ETA: 15:56\n",
      "  Train: [ 3000/57525] (  5.2%) | Loss: 0.6238 | Speed: 57.1 batches/s | ETA: 15:54\n",
      "  Train: [ 3100/57525] (  5.4%) | Loss: 0.6237 | Speed: 57.2 batches/s | ETA: 15:51\n",
      "  Train: [ 3200/57525] (  5.6%) | Loss: 0.6234 | Speed: 57.2 batches/s | ETA: 15:48\n",
      "  Train: [ 3300/57525] (  5.7%) | Loss: 0.6232 | Speed: 57.3 batches/s | ETA: 15:46\n",
      "  Train: [ 3400/57525] (  5.9%) | Loss: 0.6231 | Speed: 57.3 batches/s | ETA: 15:43\n",
      "  Train: [ 3500/57525] (  6.1%) | Loss: 0.6230 | Speed: 57.4 batches/s | ETA: 15:41\n",
      "  Train: [ 3600/57525] (  6.3%) | Loss: 0.6228 | Speed: 57.4 batches/s | ETA: 15:39\n",
      "  Train: [ 3700/57525] (  6.4%) | Loss: 0.6230 | Speed: 57.4 batches/s | ETA: 15:36\n",
      "  Train: [ 3800/57525] (  6.6%) | Loss: 0.6225 | Speed: 57.5 batches/s | ETA: 15:34\n",
      "  Train: [ 3900/57525] (  6.8%) | Loss: 0.6224 | Speed: 57.6 batches/s | ETA: 15:31\n",
      "  Train: [ 4000/57525] (  7.0%) | Loss: 0.6222 | Speed: 57.6 batches/s | ETA: 15:29\n",
      "  Train: [ 4100/57525] (  7.1%) | Loss: 0.6223 | Speed: 57.7 batches/s | ETA: 15:26\n",
      "  Train: [ 4200/57525] (  7.3%) | Loss: 0.6226 | Speed: 57.7 batches/s | ETA: 15:24\n",
      "  Train: [ 4300/57525] (  7.5%) | Loss: 0.6224 | Speed: 57.7 batches/s | ETA: 15:22\n",
      "  Train: [ 4400/57525] (  7.6%) | Loss: 0.6228 | Speed: 57.8 batches/s | ETA: 15:19\n",
      "  Train: [ 4500/57525] (  7.8%) | Loss: 0.6228 | Speed: 57.8 batches/s | ETA: 15:17\n",
      "  Train: [ 4600/57525] (  8.0%) | Loss: 0.6226 | Speed: 57.8 batches/s | ETA: 15:15\n",
      "  Train: [ 4700/57525] (  8.2%) | Loss: 0.6227 | Speed: 57.8 batches/s | ETA: 15:13\n",
      "  Train: [ 4800/57525] (  8.3%) | Loss: 0.6227 | Speed: 57.9 batches/s | ETA: 15:10\n",
      "  Train: [ 4900/57525] (  8.5%) | Loss: 0.6224 | Speed: 57.9 batches/s | ETA: 15:08\n",
      "  Train: [ 5000/57525] (  8.7%) | Loss: 0.6225 | Speed: 58.0 batches/s | ETA: 15:06\n",
      "  Train: [ 5100/57525] (  8.9%) | Loss: 0.6222 | Speed: 58.0 batches/s | ETA: 15:03\n",
      "  Train: [ 5200/57525] (  9.0%) | Loss: 0.6221 | Speed: 58.0 batches/s | ETA: 15:01\n",
      "  Train: [ 5300/57525] (  9.2%) | Loss: 0.6219 | Speed: 58.1 batches/s | ETA: 14:59\n",
      "  Train: [ 5400/57525] (  9.4%) | Loss: 0.6220 | Speed: 58.1 batches/s | ETA: 14:57\n",
      "  Train: [ 5500/57525] (  9.6%) | Loss: 0.6221 | Speed: 58.1 batches/s | ETA: 14:55\n",
      "  Train: [ 5600/57525] (  9.7%) | Loss: 0.6220 | Speed: 58.1 batches/s | ETA: 14:53\n",
      "  Train: [ 5700/57525] (  9.9%) | Loss: 0.6218 | Speed: 58.1 batches/s | ETA: 14:51\n",
      "  Train: [ 5800/57525] ( 10.1%) | Loss: 0.6222 | Speed: 58.1 batches/s | ETA: 14:49\n",
      "  Train: [ 5900/57525] ( 10.3%) | Loss: 0.6222 | Speed: 58.2 batches/s | ETA: 14:47\n",
      "  Train: [ 6000/57525] ( 10.4%) | Loss: 0.6224 | Speed: 58.2 batches/s | ETA: 14:45\n",
      "  Train: [ 6100/57525] ( 10.6%) | Loss: 0.6226 | Speed: 58.2 batches/s | ETA: 14:43\n",
      "  Train: [ 6200/57525] ( 10.8%) | Loss: 0.6223 | Speed: 58.2 batches/s | ETA: 14:41\n",
      "  Train: [ 6300/57525] ( 11.0%) | Loss: 0.6225 | Speed: 58.2 batches/s | ETA: 14:39\n",
      "  Train: [ 6400/57525] ( 11.1%) | Loss: 0.6223 | Speed: 58.3 batches/s | ETA: 14:37\n",
      "  Train: [ 6500/57525] ( 11.3%) | Loss: 0.6226 | Speed: 58.3 batches/s | ETA: 14:35\n",
      "  Train: [ 6600/57525] ( 11.5%) | Loss: 0.6229 | Speed: 58.3 batches/s | ETA: 14:33\n",
      "  Train: [ 6700/57525] ( 11.6%) | Loss: 0.6229 | Speed: 58.3 batches/s | ETA: 14:31\n",
      "  Train: [ 6800/57525] ( 11.8%) | Loss: 0.6229 | Speed: 58.3 batches/s | ETA: 14:30\n",
      "  Train: [ 6900/57525] ( 12.0%) | Loss: 0.6228 | Speed: 58.3 batches/s | ETA: 14:28\n",
      "  Train: [ 7000/57525] ( 12.2%) | Loss: 0.6227 | Speed: 58.3 batches/s | ETA: 14:26\n",
      "  Train: [ 7100/57525] ( 12.3%) | Loss: 0.6223 | Speed: 58.3 batches/s | ETA: 14:24\n",
      "  Train: [ 7200/57525] ( 12.5%) | Loss: 0.6224 | Speed: 58.3 batches/s | ETA: 14:23\n",
      "  Train: [ 7300/57525] ( 12.7%) | Loss: 0.6223 | Speed: 58.3 batches/s | ETA: 14:21\n",
      "  Train: [ 7400/57525] ( 12.9%) | Loss: 0.6227 | Speed: 58.3 batches/s | ETA: 14:19\n",
      "  Train: [ 7500/57525] ( 13.0%) | Loss: 0.6228 | Speed: 58.3 batches/s | ETA: 14:17\n",
      "  Train: [ 7600/57525] ( 13.2%) | Loss: 0.6225 | Speed: 58.3 batches/s | ETA: 14:16\n",
      "  Train: [ 7700/57525] ( 13.4%) | Loss: 0.6224 | Speed: 58.3 batches/s | ETA: 14:14\n",
      "  Train: [ 7800/57525] ( 13.6%) | Loss: 0.6222 | Speed: 58.3 batches/s | ETA: 14:12\n",
      "  Train: [ 7900/57525] ( 13.7%) | Loss: 0.6220 | Speed: 58.3 batches/s | ETA: 14:10\n",
      "  Train: [ 8000/57525] ( 13.9%) | Loss: 0.6216 | Speed: 58.4 batches/s | ETA: 14:08\n",
      "  Train: [ 8100/57525] ( 14.1%) | Loss: 0.6217 | Speed: 58.4 batches/s | ETA: 14:06\n",
      "  Train: [ 8200/57525] ( 14.3%) | Loss: 0.6217 | Speed: 58.4 batches/s | ETA: 14:04\n",
      "  Train: [ 8300/57525] ( 14.4%) | Loss: 0.6219 | Speed: 58.4 batches/s | ETA: 14:02\n",
      "  Train: [ 8400/57525] ( 14.6%) | Loss: 0.6218 | Speed: 58.4 batches/s | ETA: 14:01\n",
      "  Train: [ 8500/57525] ( 14.8%) | Loss: 0.6217 | Speed: 58.4 batches/s | ETA: 13:59\n",
      "  Train: [ 8600/57525] ( 15.0%) | Loss: 0.6219 | Speed: 58.4 batches/s | ETA: 13:57\n",
      "  Train: [ 8700/57525] ( 15.1%) | Loss: 0.6219 | Speed: 58.4 batches/s | ETA: 13:55\n",
      "  Train: [ 8800/57525] ( 15.3%) | Loss: 0.6221 | Speed: 58.4 batches/s | ETA: 13:53\n",
      "  Train: [ 8900/57525] ( 15.5%) | Loss: 0.6220 | Speed: 58.4 batches/s | ETA: 13:51\n",
      "  Train: [ 9000/57525] ( 15.6%) | Loss: 0.6221 | Speed: 58.4 batches/s | ETA: 13:50\n",
      "  Train: [ 9100/57525] ( 15.8%) | Loss: 0.6219 | Speed: 58.4 batches/s | ETA: 13:49\n",
      "  Train: [ 9200/57525] ( 16.0%) | Loss: 0.6219 | Speed: 58.4 batches/s | ETA: 13:48\n",
      "  Train: [ 9300/57525] ( 16.2%) | Loss: 0.6220 | Speed: 58.4 batches/s | ETA: 13:46\n",
      "  Train: [ 9400/57525] ( 16.3%) | Loss: 0.6218 | Speed: 58.4 batches/s | ETA: 13:44\n",
      "  Train: [ 9500/57525] ( 16.5%) | Loss: 0.6217 | Speed: 58.4 batches/s | ETA: 13:42\n",
      "  Train: [ 9600/57525] ( 16.7%) | Loss: 0.6216 | Speed: 58.4 batches/s | ETA: 13:41\n",
      "  Train: [ 9700/57525] ( 16.9%) | Loss: 0.6217 | Speed: 58.4 batches/s | ETA: 13:39\n",
      "  Train: [ 9800/57525] ( 17.0%) | Loss: 0.6220 | Speed: 58.4 batches/s | ETA: 13:37\n",
      "  Train: [ 9900/57525] ( 17.2%) | Loss: 0.6220 | Speed: 58.4 batches/s | ETA: 13:36\n",
      "  Train: [10000/57525] ( 17.4%) | Loss: 0.6221 | Speed: 58.4 batches/s | ETA: 13:34\n",
      "  Train: [10100/57525] ( 17.6%) | Loss: 0.6221 | Speed: 58.4 batches/s | ETA: 13:32\n",
      "  Train: [10200/57525] ( 17.7%) | Loss: 0.6220 | Speed: 58.4 batches/s | ETA: 13:30\n",
      "  Train: [10300/57525] ( 17.9%) | Loss: 0.6221 | Speed: 58.4 batches/s | ETA: 13:28\n",
      "  Train: [10400/57525] ( 18.1%) | Loss: 0.6220 | Speed: 58.4 batches/s | ETA: 13:27\n",
      "  Train: [10500/57525] ( 18.3%) | Loss: 0.6221 | Speed: 58.4 batches/s | ETA: 13:25\n",
      "  Train: [10600/57525] ( 18.4%) | Loss: 0.6221 | Speed: 58.4 batches/s | ETA: 13:23\n",
      "  Train: [10700/57525] ( 18.6%) | Loss: 0.6222 | Speed: 58.4 batches/s | ETA: 13:22\n",
      "  Train: [10800/57525] ( 18.8%) | Loss: 0.6221 | Speed: 58.4 batches/s | ETA: 13:20\n",
      "  Train: [10900/57525] ( 18.9%) | Loss: 0.6220 | Speed: 58.4 batches/s | ETA: 13:18\n",
      "  Train: [11000/57525] ( 19.1%) | Loss: 0.6219 | Speed: 58.4 batches/s | ETA: 13:17\n",
      "  Train: [11100/57525] ( 19.3%) | Loss: 0.6220 | Speed: 58.4 batches/s | ETA: 13:15\n",
      "  Train: [11200/57525] ( 19.5%) | Loss: 0.6220 | Speed: 58.4 batches/s | ETA: 13:13\n",
      "  Train: [11300/57525] ( 19.6%) | Loss: 0.6217 | Speed: 58.4 batches/s | ETA: 13:11\n",
      "  Train: [11400/57525] ( 19.8%) | Loss: 0.6218 | Speed: 58.4 batches/s | ETA: 13:10\n",
      "  Train: [11500/57525] ( 20.0%) | Loss: 0.6216 | Speed: 58.4 batches/s | ETA: 13:08\n",
      "  Train: [11600/57525] ( 20.2%) | Loss: 0.6218 | Speed: 58.4 batches/s | ETA: 13:06\n",
      "  Train: [11700/57525] ( 20.3%) | Loss: 0.6217 | Speed: 58.4 batches/s | ETA: 13:04\n",
      "  Train: [11800/57525] ( 20.5%) | Loss: 0.6216 | Speed: 58.4 batches/s | ETA: 13:02\n",
      "  Train: [11900/57525] ( 20.7%) | Loss: 0.6218 | Speed: 58.4 batches/s | ETA: 13:01\n",
      "  Train: [12000/57525] ( 20.9%) | Loss: 0.6220 | Speed: 58.4 batches/s | ETA: 12:59\n",
      "  Train: [12100/57525] ( 21.0%) | Loss: 0.6221 | Speed: 58.4 batches/s | ETA: 12:57\n",
      "  Train: [12200/57525] ( 21.2%) | Loss: 0.6222 | Speed: 58.4 batches/s | ETA: 12:55\n",
      "  Train: [12300/57525] ( 21.4%) | Loss: 0.6221 | Speed: 58.4 batches/s | ETA: 12:54\n",
      "  Train: [12400/57525] ( 21.6%) | Loss: 0.6222 | Speed: 58.4 batches/s | ETA: 12:52\n",
      "  Train: [12500/57525] ( 21.7%) | Loss: 0.6221 | Speed: 58.4 batches/s | ETA: 12:50\n",
      "  Train: [12600/57525] ( 21.9%) | Loss: 0.6223 | Speed: 58.4 batches/s | ETA: 12:48\n",
      "  Train: [12700/57525] ( 22.1%) | Loss: 0.6223 | Speed: 58.4 batches/s | ETA: 12:47\n",
      "  Train: [12800/57525] ( 22.3%) | Loss: 0.6223 | Speed: 58.4 batches/s | ETA: 12:45\n",
      "  Train: [12900/57525] ( 22.4%) | Loss: 0.6224 | Speed: 58.4 batches/s | ETA: 12:43\n",
      "  Train: [13000/57525] ( 22.6%) | Loss: 0.6224 | Speed: 58.4 batches/s | ETA: 12:41\n",
      "  Train: [13100/57525] ( 22.8%) | Loss: 0.6224 | Speed: 58.4 batches/s | ETA: 12:40\n",
      "  Train: [13200/57525] ( 22.9%) | Loss: 0.6224 | Speed: 58.4 batches/s | ETA: 12:38\n",
      "  Train: [13300/57525] ( 23.1%) | Loss: 0.6225 | Speed: 58.4 batches/s | ETA: 12:36\n",
      "  Train: [13400/57525] ( 23.3%) | Loss: 0.6223 | Speed: 58.4 batches/s | ETA: 12:34\n",
      "  Train: [13500/57525] ( 23.5%) | Loss: 0.6222 | Speed: 58.5 batches/s | ETA: 12:33\n",
      "  Train: [13600/57525] ( 23.6%) | Loss: 0.6223 | Speed: 58.5 batches/s | ETA: 12:31\n",
      "  Train: [13700/57525] ( 23.8%) | Loss: 0.6225 | Speed: 58.5 batches/s | ETA: 12:29\n",
      "  Train: [13800/57525] ( 24.0%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 12:27\n",
      "  Train: [13900/57525] ( 24.2%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 12:25\n",
      "  Train: [14000/57525] ( 24.3%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 12:24\n",
      "  Train: [14100/57525] ( 24.5%) | Loss: 0.6226 | Speed: 58.5 batches/s | ETA: 12:22\n",
      "  Train: [14200/57525] ( 24.7%) | Loss: 0.6226 | Speed: 58.5 batches/s | ETA: 12:20\n",
      "  Train: [14300/57525] ( 24.9%) | Loss: 0.6225 | Speed: 58.5 batches/s | ETA: 12:18\n",
      "  Train: [14400/57525] ( 25.0%) | Loss: 0.6226 | Speed: 58.5 batches/s | ETA: 12:17\n",
      "  Train: [14500/57525] ( 25.2%) | Loss: 0.6227 | Speed: 58.5 batches/s | ETA: 12:15\n",
      "  Train: [14600/57525] ( 25.4%) | Loss: 0.6226 | Speed: 58.5 batches/s | ETA: 12:13\n",
      "  Train: [14700/57525] ( 25.6%) | Loss: 0.6227 | Speed: 58.5 batches/s | ETA: 12:11\n",
      "  Train: [14800/57525] ( 25.7%) | Loss: 0.6227 | Speed: 58.5 batches/s | ETA: 12:10\n",
      "  Train: [14900/57525] ( 25.9%) | Loss: 0.6226 | Speed: 58.5 batches/s | ETA: 12:08\n",
      "  Train: [15000/57525] ( 26.1%) | Loss: 0.6226 | Speed: 58.5 batches/s | ETA: 12:06\n",
      "  Train: [15100/57525] ( 26.2%) | Loss: 0.6227 | Speed: 58.5 batches/s | ETA: 12:05\n",
      "  Train: [15200/57525] ( 26.4%) | Loss: 0.6225 | Speed: 58.5 batches/s | ETA: 12:03\n",
      "  Train: [15300/57525] ( 26.6%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 12:01\n",
      "  Train: [15400/57525] ( 26.8%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:59\n",
      "  Train: [15500/57525] ( 26.9%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:58\n",
      "  Train: [15600/57525] ( 27.1%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:56\n",
      "  Train: [15700/57525] ( 27.3%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:54\n",
      "  Train: [15800/57525] ( 27.5%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:52\n",
      "  Train: [15900/57525] ( 27.6%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:51\n",
      "  Train: [16000/57525] ( 27.8%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:49\n",
      "  Train: [16100/57525] ( 28.0%) | Loss: 0.6223 | Speed: 58.5 batches/s | ETA: 11:47\n",
      "  Train: [16200/57525] ( 28.2%) | Loss: 0.6222 | Speed: 58.5 batches/s | ETA: 11:45\n",
      "  Train: [16300/57525] ( 28.3%) | Loss: 0.6222 | Speed: 58.5 batches/s | ETA: 11:44\n",
      "  Train: [16400/57525] ( 28.5%) | Loss: 0.6223 | Speed: 58.5 batches/s | ETA: 11:43\n",
      "  Train: [16500/57525] ( 28.7%) | Loss: 0.6223 | Speed: 58.5 batches/s | ETA: 11:41\n",
      "  Train: [16600/57525] ( 28.9%) | Loss: 0.6223 | Speed: 58.5 batches/s | ETA: 11:39\n",
      "  Train: [16700/57525] ( 29.0%) | Loss: 0.6225 | Speed: 58.5 batches/s | ETA: 11:37\n",
      "  Train: [16800/57525] ( 29.2%) | Loss: 0.6225 | Speed: 58.5 batches/s | ETA: 11:36\n",
      "  Train: [16900/57525] ( 29.4%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:34\n",
      "  Train: [17000/57525] ( 29.6%) | Loss: 0.6225 | Speed: 58.5 batches/s | ETA: 11:32\n",
      "  Train: [17100/57525] ( 29.7%) | Loss: 0.6225 | Speed: 58.5 batches/s | ETA: 11:30\n",
      "  Train: [17200/57525] ( 29.9%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:28\n",
      "  Train: [17300/57525] ( 30.1%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:27\n",
      "  Train: [17400/57525] ( 30.2%) | Loss: 0.6225 | Speed: 58.5 batches/s | ETA: 11:25\n",
      "  Train: [17500/57525] ( 30.4%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:23\n",
      "  Train: [17600/57525] ( 30.6%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:21\n",
      "  Train: [17700/57525] ( 30.8%) | Loss: 0.6223 | Speed: 58.6 batches/s | ETA: 11:20\n",
      "  Train: [17800/57525] ( 30.9%) | Loss: 0.6222 | Speed: 58.6 batches/s | ETA: 11:18\n",
      "  Train: [17900/57525] ( 31.1%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 11:16\n",
      "  Train: [18000/57525] ( 31.3%) | Loss: 0.6223 | Speed: 58.5 batches/s | ETA: 11:15\n",
      "  Train: [18100/57525] ( 31.5%) | Loss: 0.6223 | Speed: 58.5 batches/s | ETA: 11:13\n",
      "  Train: [18200/57525] ( 31.6%) | Loss: 0.6225 | Speed: 58.5 batches/s | ETA: 11:11\n",
      "  Train: [18300/57525] ( 31.8%) | Loss: 0.6225 | Speed: 58.5 batches/s | ETA: 11:09\n",
      "  Train: [18400/57525] ( 32.0%) | Loss: 0.6225 | Speed: 58.5 batches/s | ETA: 11:08\n",
      "  Train: [18500/57525] ( 32.2%) | Loss: 0.6225 | Speed: 58.5 batches/s | ETA: 11:06\n",
      "  Train: [18600/57525] ( 32.3%) | Loss: 0.6226 | Speed: 58.5 batches/s | ETA: 11:05\n",
      "  Train: [18700/57525] ( 32.5%) | Loss: 0.6224 | Speed: 58.5 batches/s | ETA: 11:03\n",
      "  Train: [18800/57525] ( 32.7%) | Loss: 0.6222 | Speed: 58.5 batches/s | ETA: 11:01\n",
      "  Train: [18900/57525] ( 32.9%) | Loss: 0.6222 | Speed: 58.5 batches/s | ETA: 11:00\n",
      "  Train: [19000/57525] ( 33.0%) | Loss: 0.6221 | Speed: 58.5 batches/s | ETA: 10:58\n",
      "  Train: [19100/57525] ( 33.2%) | Loss: 0.6221 | Speed: 58.5 batches/s | ETA: 10:56\n",
      "  Train: [19200/57525] ( 33.4%) | Loss: 0.6221 | Speed: 58.5 batches/s | ETA: 10:55\n",
      "  Train: [19300/57525] ( 33.6%) | Loss: 0.6219 | Speed: 58.5 batches/s | ETA: 10:53\n",
      "  Train: [19400/57525] ( 33.7%) | Loss: 0.6221 | Speed: 58.5 batches/s | ETA: 10:51\n",
      "  Train: [19500/57525] ( 33.9%) | Loss: 0.6221 | Speed: 58.5 batches/s | ETA: 10:49\n",
      "  Train: [19600/57525] ( 34.1%) | Loss: 0.6221 | Speed: 58.5 batches/s | ETA: 10:48\n",
      "  Train: [19700/57525] ( 34.2%) | Loss: 0.6220 | Speed: 58.5 batches/s | ETA: 10:46\n",
      "  Train: [19800/57525] ( 34.4%) | Loss: 0.6221 | Speed: 58.5 batches/s | ETA: 10:44\n",
      "  Train: [19900/57525] ( 34.6%) | Loss: 0.6220 | Speed: 58.5 batches/s | ETA: 10:42\n",
      "  Train: [20000/57525] ( 34.8%) | Loss: 0.6220 | Speed: 58.5 batches/s | ETA: 10:41\n",
      "  Train: [20100/57525] ( 34.9%) | Loss: 0.6221 | Speed: 58.5 batches/s | ETA: 10:39\n",
      "  Train: [20200/57525] ( 35.1%) | Loss: 0.6221 | Speed: 58.5 batches/s | ETA: 10:37\n",
      "  Train: [20300/57525] ( 35.3%) | Loss: 0.6220 | Speed: 58.5 batches/s | ETA: 10:35\n",
      "  Train: [20400/57525] ( 35.5%) | Loss: 0.6220 | Speed: 58.5 batches/s | ETA: 10:34\n",
      "  Train: [20500/57525] ( 35.6%) | Loss: 0.6219 | Speed: 58.5 batches/s | ETA: 10:32\n",
      "  Train: [20600/57525] ( 35.8%) | Loss: 0.6218 | Speed: 58.5 batches/s | ETA: 10:30\n",
      "  Train: [20700/57525] ( 36.0%) | Loss: 0.6218 | Speed: 58.5 batches/s | ETA: 10:28\n",
      "  Train: [20800/57525] ( 36.2%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 10:27\n",
      "  Train: [20900/57525] ( 36.3%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 10:25\n",
      "  Train: [21000/57525] ( 36.5%) | Loss: 0.6218 | Speed: 58.6 batches/s | ETA: 10:23\n",
      "  Train: [21100/57525] ( 36.7%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 10:22\n",
      "  Train: [21200/57525] ( 36.9%) | Loss: 0.6218 | Speed: 58.6 batches/s | ETA: 10:20\n",
      "  Train: [21300/57525] ( 37.0%) | Loss: 0.6218 | Speed: 58.6 batches/s | ETA: 10:18\n",
      "  Train: [21400/57525] ( 37.2%) | Loss: 0.6217 | Speed: 58.6 batches/s | ETA: 10:16\n",
      "  Train: [21500/57525] ( 37.4%) | Loss: 0.6218 | Speed: 58.6 batches/s | ETA: 10:15\n",
      "  Train: [21600/57525] ( 37.5%) | Loss: 0.6217 | Speed: 58.6 batches/s | ETA: 10:13\n",
      "  Train: [21700/57525] ( 37.7%) | Loss: 0.6217 | Speed: 58.6 batches/s | ETA: 10:11\n",
      "  Train: [21800/57525] ( 37.9%) | Loss: 0.6218 | Speed: 58.6 batches/s | ETA: 10:09\n",
      "  Train: [21900/57525] ( 38.1%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 10:08\n",
      "  Train: [22000/57525] ( 38.2%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 10:06\n",
      "  Train: [22100/57525] ( 38.4%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 10:04\n",
      "  Train: [22200/57525] ( 38.6%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 10:02\n",
      "  Train: [22300/57525] ( 38.8%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 10:01\n",
      "  Train: [22400/57525] ( 38.9%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:59\n",
      "  Train: [22500/57525] ( 39.1%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:57\n",
      "  Train: [22600/57525] ( 39.3%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:56\n",
      "  Train: [22700/57525] ( 39.5%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 09:54\n",
      "  Train: [22800/57525] ( 39.6%) | Loss: 0.6222 | Speed: 58.6 batches/s | ETA: 09:52\n",
      "  Train: [22900/57525] ( 39.8%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 09:50\n",
      "  Train: [23000/57525] ( 40.0%) | Loss: 0.6222 | Speed: 58.6 batches/s | ETA: 09:49\n",
      "  Train: [23100/57525] ( 40.2%) | Loss: 0.6222 | Speed: 58.6 batches/s | ETA: 09:47\n",
      "  Train: [23200/57525] ( 40.3%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 09:45\n",
      "  Train: [23300/57525] ( 40.5%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:43\n",
      "  Train: [23400/57525] ( 40.7%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:42\n",
      "  Train: [23500/57525] ( 40.9%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 09:40\n",
      "  Train: [23600/57525] ( 41.0%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 09:38\n",
      "  Train: [23700/57525] ( 41.2%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 09:37\n",
      "  Train: [23800/57525] ( 41.4%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:35\n",
      "  Train: [23900/57525] ( 41.5%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:33\n",
      "  Train: [24000/57525] ( 41.7%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 09:31\n",
      "  Train: [24100/57525] ( 41.9%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 09:30\n",
      "  Train: [24200/57525] ( 42.1%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:28\n",
      "  Train: [24300/57525] ( 42.2%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:26\n",
      "  Train: [24400/57525] ( 42.4%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:25\n",
      "  Train: [24500/57525] ( 42.6%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:23\n",
      "  Train: [24600/57525] ( 42.8%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 09:21\n",
      "  Train: [24700/57525] ( 42.9%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:19\n",
      "  Train: [24800/57525] ( 43.1%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:18\n",
      "  Train: [24900/57525] ( 43.3%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 09:16\n",
      "  Train: [25000/57525] ( 43.5%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 09:14\n",
      "  Train: [25100/57525] ( 43.6%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:13\n",
      "  Train: [25200/57525] ( 43.8%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 09:11\n",
      "  Train: [25300/57525] ( 44.0%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:09\n",
      "  Train: [25400/57525] ( 44.2%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 09:07\n",
      "  Train: [25500/57525] ( 44.3%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 09:06\n",
      "  Train: [25600/57525] ( 44.5%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 09:04\n",
      "  Train: [25700/57525] ( 44.7%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 09:02\n",
      "  Train: [25800/57525] ( 44.9%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 09:01\n",
      "  Train: [25900/57525] ( 45.0%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 08:59\n",
      "  Train: [26000/57525] ( 45.2%) | Loss: 0.6221 | Speed: 58.6 batches/s | ETA: 08:57\n",
      "  Train: [26100/57525] ( 45.4%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 08:55\n",
      "  Train: [26200/57525] ( 45.5%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 08:54\n",
      "  Train: [26300/57525] ( 45.7%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 08:52\n",
      "  Train: [26400/57525] ( 45.9%) | Loss: 0.6220 | Speed: 58.6 batches/s | ETA: 08:50\n",
      "  Train: [26500/57525] ( 46.1%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 08:49\n",
      "  Train: [26600/57525] ( 46.2%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 08:47\n",
      "  Train: [26700/57525] ( 46.4%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 08:45\n",
      "  Train: [26800/57525] ( 46.6%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 08:44\n",
      "  Train: [26900/57525] ( 46.8%) | Loss: 0.6218 | Speed: 58.6 batches/s | ETA: 08:42\n",
      "  Train: [27000/57525] ( 46.9%) | Loss: 0.6218 | Speed: 58.6 batches/s | ETA: 08:40\n",
      "  Train: [27100/57525] ( 47.1%) | Loss: 0.6218 | Speed: 58.6 batches/s | ETA: 08:38\n",
      "  Train: [27200/57525] ( 47.3%) | Loss: 0.6218 | Speed: 58.6 batches/s | ETA: 08:37\n",
      "  Train: [27300/57525] ( 47.5%) | Loss: 0.6218 | Speed: 58.6 batches/s | ETA: 08:35\n",
      "  Train: [27400/57525] ( 47.6%) | Loss: 0.6219 | Speed: 58.6 batches/s | ETA: 08:33\n",
      "  Train: [27500/57525] ( 47.8%) | Loss: 0.6219 | Speed: 58.7 batches/s | ETA: 08:31\n",
      "  Train: [27600/57525] ( 48.0%) | Loss: 0.6218 | Speed: 58.7 batches/s | ETA: 08:30\n",
      "  Train: [27700/57525] ( 48.2%) | Loss: 0.6218 | Speed: 58.7 batches/s | ETA: 08:28\n",
      "  Train: [27800/57525] ( 48.3%) | Loss: 0.6218 | Speed: 58.7 batches/s | ETA: 08:26\n",
      "  Train: [27900/57525] ( 48.5%) | Loss: 0.6218 | Speed: 58.7 batches/s | ETA: 08:25\n",
      "  Train: [28000/57525] ( 48.7%) | Loss: 0.6217 | Speed: 58.7 batches/s | ETA: 08:23\n",
      "  Train: [28100/57525] ( 48.8%) | Loss: 0.6217 | Speed: 58.7 batches/s | ETA: 08:21\n",
      "  Train: [28200/57525] ( 49.0%) | Loss: 0.6216 | Speed: 58.7 batches/s | ETA: 08:19\n",
      "  Train: [28300/57525] ( 49.2%) | Loss: 0.6216 | Speed: 58.7 batches/s | ETA: 08:18\n",
      "  Train: [28400/57525] ( 49.4%) | Loss: 0.6215 | Speed: 58.7 batches/s | ETA: 08:16\n",
      "  Train: [28500/57525] ( 49.5%) | Loss: 0.6214 | Speed: 58.7 batches/s | ETA: 08:14\n",
      "  Train: [28600/57525] ( 49.7%) | Loss: 0.6214 | Speed: 58.7 batches/s | ETA: 08:13\n",
      "  Train: [28700/57525] ( 49.9%) | Loss: 0.6215 | Speed: 58.7 batches/s | ETA: 08:11\n",
      "  Train: [28800/57525] ( 50.1%) | Loss: 0.6216 | Speed: 58.7 batches/s | ETA: 08:09\n",
      "  Train: [28900/57525] ( 50.2%) | Loss: 0.6215 | Speed: 58.6 batches/s | ETA: 08:08\n",
      "  Train: [29000/57525] ( 50.4%) | Loss: 0.6215 | Speed: 58.6 batches/s | ETA: 08:06\n",
      "  Train: [29100/57525] ( 50.6%) | Loss: 0.6215 | Speed: 58.6 batches/s | ETA: 08:04\n",
      "  Train: [29200/57525] ( 50.8%) | Loss: 0.6215 | Speed: 58.6 batches/s | ETA: 08:02\n",
      "  Train: [29300/57525] ( 50.9%) | Loss: 0.6214 | Speed: 58.6 batches/s | ETA: 08:01\n",
      "  Train: [29400/57525] ( 51.1%) | Loss: 0.6214 | Speed: 58.6 batches/s | ETA: 07:59\n",
      "  Train: [29500/57525] ( 51.3%) | Loss: 0.6213 | Speed: 58.6 batches/s | ETA: 07:57\n",
      "  Train: [29600/57525] ( 51.5%) | Loss: 0.6213 | Speed: 58.6 batches/s | ETA: 07:56\n",
      "  Train: [29700/57525] ( 51.6%) | Loss: 0.6213 | Speed: 58.6 batches/s | ETA: 07:54\n",
      "  Train: [29800/57525] ( 51.8%) | Loss: 0.6213 | Speed: 58.7 batches/s | ETA: 07:52\n",
      "  Train: [29900/57525] ( 52.0%) | Loss: 0.6213 | Speed: 58.7 batches/s | ETA: 07:50\n",
      "  Train: [30000/57525] ( 52.2%) | Loss: 0.6213 | Speed: 58.7 batches/s | ETA: 07:49\n",
      "  Train: [30100/57525] ( 52.3%) | Loss: 0.6212 | Speed: 58.7 batches/s | ETA: 07:47\n",
      "  Train: [30200/57525] ( 52.5%) | Loss: 0.6212 | Speed: 58.7 batches/s | ETA: 07:45\n",
      "  Train: [30300/57525] ( 52.7%) | Loss: 0.6212 | Speed: 58.7 batches/s | ETA: 07:44\n",
      "  Train: [30400/57525] ( 52.8%) | Loss: 0.6212 | Speed: 58.7 batches/s | ETA: 07:42\n",
      "  Train: [30500/57525] ( 53.0%) | Loss: 0.6212 | Speed: 58.7 batches/s | ETA: 07:40\n",
      "  Train: [30600/57525] ( 53.2%) | Loss: 0.6212 | Speed: 58.7 batches/s | ETA: 07:39\n",
      "  Train: [30700/57525] ( 53.4%) | Loss: 0.6212 | Speed: 58.7 batches/s | ETA: 07:37\n",
      "  Train: [30800/57525] ( 53.5%) | Loss: 0.6212 | Speed: 58.7 batches/s | ETA: 07:35\n",
      "  Train: [30900/57525] ( 53.7%) | Loss: 0.6212 | Speed: 58.7 batches/s | ETA: 07:33\n",
      "  Train: [31000/57525] ( 53.9%) | Loss: 0.6213 | Speed: 58.7 batches/s | ETA: 07:32\n",
      "  Train: [31100/57525] ( 54.1%) | Loss: 0.6213 | Speed: 58.7 batches/s | ETA: 07:30\n",
      "  Train: [31200/57525] ( 54.2%) | Loss: 0.6213 | Speed: 58.7 batches/s | ETA: 07:28\n",
      "  Train: [31300/57525] ( 54.4%) | Loss: 0.6213 | Speed: 58.7 batches/s | ETA: 07:26\n",
      "  Train: [31400/57525] ( 54.6%) | Loss: 0.6213 | Speed: 58.7 batches/s | ETA: 07:25\n",
      "  Train: [31500/57525] ( 54.8%) | Loss: 0.6213 | Speed: 58.7 batches/s | ETA: 07:23\n",
      "  Train: [31600/57525] ( 54.9%) | Loss: 0.6213 | Speed: 58.7 batches/s | ETA: 07:21\n",
      "  Train: [31700/57525] ( 55.1%) | Loss: 0.6212 | Speed: 58.7 batches/s | ETA: 07:20\n",
      "  Train: [31800/57525] ( 55.3%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 07:18\n",
      "  Train: [31900/57525] ( 55.5%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 07:16\n",
      "  Train: [32000/57525] ( 55.6%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 07:14\n",
      "  Train: [32100/57525] ( 55.8%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 07:13\n",
      "  Train: [32200/57525] ( 56.0%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 07:11\n",
      "  Train: [32300/57525] ( 56.1%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 07:09\n",
      "  Train: [32400/57525] ( 56.3%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 07:08\n",
      "  Train: [32500/57525] ( 56.5%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 07:06\n",
      "  Train: [32600/57525] ( 56.7%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 07:04\n",
      "  Train: [32700/57525] ( 56.8%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 07:03\n",
      "  Train: [32800/57525] ( 57.0%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 07:01\n",
      "  Train: [32900/57525] ( 57.2%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 06:59\n",
      "  Train: [33000/57525] ( 57.4%) | Loss: 0.6210 | Speed: 58.7 batches/s | ETA: 06:57\n",
      "  Train: [33100/57525] ( 57.5%) | Loss: 0.6210 | Speed: 58.7 batches/s | ETA: 06:56\n",
      "  Train: [33200/57525] ( 57.7%) | Loss: 0.6210 | Speed: 58.7 batches/s | ETA: 06:54\n",
      "  Train: [33300/57525] ( 57.9%) | Loss: 0.6210 | Speed: 58.7 batches/s | ETA: 06:52\n",
      "  Train: [33400/57525] ( 58.1%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 06:51\n",
      "  Train: [33500/57525] ( 58.2%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 06:49\n",
      "  Train: [33600/57525] ( 58.4%) | Loss: 0.6210 | Speed: 58.7 batches/s | ETA: 06:47\n",
      "  Train: [33700/57525] ( 58.6%) | Loss: 0.6211 | Speed: 58.7 batches/s | ETA: 06:46\n",
      "  Train: [33800/57525] ( 58.8%) | Loss: 0.6210 | Speed: 58.7 batches/s | ETA: 06:44\n",
      "  Train: [33900/57525] ( 58.9%) | Loss: 0.6210 | Speed: 58.7 batches/s | ETA: 06:42\n",
      "  Train: [34000/57525] ( 59.1%) | Loss: 0.6209 | Speed: 58.7 batches/s | ETA: 06:41\n",
      "  Train: [34100/57525] ( 59.3%) | Loss: 0.6208 | Speed: 58.7 batches/s | ETA: 06:39\n",
      "  Train: [34200/57525] ( 59.5%) | Loss: 0.6208 | Speed: 58.7 batches/s | ETA: 06:37\n",
      "  Train: [34300/57525] ( 59.6%) | Loss: 0.6207 | Speed: 58.7 batches/s | ETA: 06:35\n",
      "  Train: [34400/57525] ( 59.8%) | Loss: 0.6207 | Speed: 58.7 batches/s | ETA: 06:34\n",
      "  Train: [34500/57525] ( 60.0%) | Loss: 0.6206 | Speed: 58.7 batches/s | ETA: 06:32\n",
      "  Train: [34600/57525] ( 60.1%) | Loss: 0.6206 | Speed: 58.7 batches/s | ETA: 06:30\n",
      "  Train: [34700/57525] ( 60.3%) | Loss: 0.6206 | Speed: 58.7 batches/s | ETA: 06:29\n",
      "  Train: [34800/57525] ( 60.5%) | Loss: 0.6206 | Speed: 58.7 batches/s | ETA: 06:27\n",
      "  Train: [34900/57525] ( 60.7%) | Loss: 0.6206 | Speed: 58.7 batches/s | ETA: 06:25\n",
      "  Train: [35000/57525] ( 60.8%) | Loss: 0.6206 | Speed: 58.7 batches/s | ETA: 06:23\n",
      "  Train: [35100/57525] ( 61.0%) | Loss: 0.6205 | Speed: 58.7 batches/s | ETA: 06:22\n",
      "  Train: [35200/57525] ( 61.2%) | Loss: 0.6205 | Speed: 58.7 batches/s | ETA: 06:20\n",
      "  Train: [35300/57525] ( 61.4%) | Loss: 0.6204 | Speed: 58.7 batches/s | ETA: 06:18\n",
      "  Train: [35400/57525] ( 61.5%) | Loss: 0.6205 | Speed: 58.7 batches/s | ETA: 06:17\n",
      "  Train: [35500/57525] ( 61.7%) | Loss: 0.6205 | Speed: 58.7 batches/s | ETA: 06:15\n",
      "  Train: [35600/57525] ( 61.9%) | Loss: 0.6205 | Speed: 58.7 batches/s | ETA: 06:13\n",
      "  Train: [35700/57525] ( 62.1%) | Loss: 0.6205 | Speed: 58.7 batches/s | ETA: 06:12\n",
      "  Train: [35800/57525] ( 62.2%) | Loss: 0.6204 | Speed: 58.7 batches/s | ETA: 06:10\n",
      "  Train: [35900/57525] ( 62.4%) | Loss: 0.6204 | Speed: 58.7 batches/s | ETA: 06:08\n",
      "  Train: [36000/57525] ( 62.6%) | Loss: 0.6204 | Speed: 58.7 batches/s | ETA: 06:06\n",
      "  Train: [36100/57525] ( 62.8%) | Loss: 0.6203 | Speed: 58.7 batches/s | ETA: 06:05\n",
      "  Train: [36200/57525] ( 62.9%) | Loss: 0.6203 | Speed: 58.6 batches/s | ETA: 06:03\n",
      "  Train: [36300/57525] ( 63.1%) | Loss: 0.6202 | Speed: 58.6 batches/s | ETA: 06:01\n",
      "  Train: [36400/57525] ( 63.3%) | Loss: 0.6202 | Speed: 58.6 batches/s | ETA: 06:00\n",
      "  Train: [36500/57525] ( 63.5%) | Loss: 0.6201 | Speed: 58.6 batches/s | ETA: 05:58\n",
      "  Train: [36600/57525] ( 63.6%) | Loss: 0.6201 | Speed: 58.6 batches/s | ETA: 05:56\n",
      "  Train: [36700/57525] ( 63.8%) | Loss: 0.6201 | Speed: 58.6 batches/s | ETA: 05:55\n",
      "  Train: [36800/57525] ( 64.0%) | Loss: 0.6201 | Speed: 58.6 batches/s | ETA: 05:53\n",
      "  Train: [36900/57525] ( 64.1%) | Loss: 0.6200 | Speed: 58.7 batches/s | ETA: 05:51\n",
      "  Train: [37000/57525] ( 64.3%) | Loss: 0.6200 | Speed: 58.7 batches/s | ETA: 05:49\n",
      "  Train: [37100/57525] ( 64.5%) | Loss: 0.6200 | Speed: 58.7 batches/s | ETA: 05:48\n",
      "  Train: [37200/57525] ( 64.7%) | Loss: 0.6199 | Speed: 58.7 batches/s | ETA: 05:46\n",
      "  Train: [37300/57525] ( 64.8%) | Loss: 0.6199 | Speed: 58.7 batches/s | ETA: 05:44\n",
      "  Train: [37400/57525] ( 65.0%) | Loss: 0.6199 | Speed: 58.7 batches/s | ETA: 05:43\n",
      "  Train: [37500/57525] ( 65.2%) | Loss: 0.6198 | Speed: 58.7 batches/s | ETA: 05:41\n",
      "  Train: [37600/57525] ( 65.4%) | Loss: 0.6198 | Speed: 58.7 batches/s | ETA: 05:39\n",
      "  Train: [37700/57525] ( 65.5%) | Loss: 0.6198 | Speed: 58.7 batches/s | ETA: 05:37\n",
      "  Train: [37800/57525] ( 65.7%) | Loss: 0.6197 | Speed: 58.7 batches/s | ETA: 05:36\n",
      "  Train: [37900/57525] ( 65.9%) | Loss: 0.6197 | Speed: 58.7 batches/s | ETA: 05:34\n",
      "  Train: [38000/57525] ( 66.1%) | Loss: 0.6197 | Speed: 58.7 batches/s | ETA: 05:32\n",
      "  Train: [38100/57525] ( 66.2%) | Loss: 0.6197 | Speed: 58.7 batches/s | ETA: 05:31\n",
      "  Train: [38200/57525] ( 66.4%) | Loss: 0.6197 | Speed: 58.7 batches/s | ETA: 05:29\n",
      "  Train: [38300/57525] ( 66.6%) | Loss: 0.6197 | Speed: 58.7 batches/s | ETA: 05:27\n",
      "  Train: [38400/57525] ( 66.8%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 05:26\n",
      "  Train: [38500/57525] ( 66.9%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 05:24\n",
      "  Train: [38600/57525] ( 67.1%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 05:22\n",
      "  Train: [38700/57525] ( 67.3%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 05:20\n",
      "  Train: [38800/57525] ( 67.4%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 05:19\n",
      "  Train: [38900/57525] ( 67.6%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 05:17\n",
      "  Train: [39000/57525] ( 67.8%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 05:15\n",
      "  Train: [39100/57525] ( 68.0%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 05:14\n",
      "  Train: [39200/57525] ( 68.1%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 05:12\n",
      "  Train: [39300/57525] ( 68.3%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 05:10\n",
      "  Train: [39400/57525] ( 68.5%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 05:08\n",
      "  Train: [39500/57525] ( 68.7%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 05:07\n",
      "  Train: [39600/57525] ( 68.8%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 05:05\n",
      "  Train: [39700/57525] ( 69.0%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 05:03\n",
      "  Train: [39800/57525] ( 69.2%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 05:01\n",
      "  Train: [39900/57525] ( 69.4%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 05:00\n",
      "  Train: [40000/57525] ( 69.5%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:58\n",
      "  Train: [40100/57525] ( 69.7%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:56\n",
      "  Train: [40200/57525] ( 69.9%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:55\n",
      "  Train: [40300/57525] ( 70.1%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:53\n",
      "  Train: [40400/57525] ( 70.2%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:51\n",
      "  Train: [40500/57525] ( 70.4%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:49\n",
      "  Train: [40600/57525] ( 70.6%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:48\n",
      "  Train: [40700/57525] ( 70.8%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:46\n",
      "  Train: [40800/57525] ( 70.9%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 04:44\n",
      "  Train: [40900/57525] ( 71.1%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:43\n",
      "  Train: [41000/57525] ( 71.3%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 04:41\n",
      "  Train: [41100/57525] ( 71.4%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:39\n",
      "  Train: [41200/57525] ( 71.6%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:37\n",
      "  Train: [41300/57525] ( 71.8%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:36\n",
      "  Train: [41400/57525] ( 72.0%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:34\n",
      "  Train: [41500/57525] ( 72.1%) | Loss: 0.6196 | Speed: 58.7 batches/s | ETA: 04:32\n",
      "  Train: [41600/57525] ( 72.3%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:31\n",
      "  Train: [41700/57525] ( 72.5%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:29\n",
      "  Train: [41800/57525] ( 72.7%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:27\n",
      "  Train: [41900/57525] ( 72.8%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:26\n",
      "  Train: [42000/57525] ( 73.0%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:24\n",
      "  Train: [42100/57525] ( 73.2%) | Loss: 0.6195 | Speed: 58.7 batches/s | ETA: 04:22\n",
      "  Train: [42200/57525] ( 73.4%) | Loss: 0.6194 | Speed: 58.7 batches/s | ETA: 04:20\n",
      "  Train: [42300/57525] ( 73.5%) | Loss: 0.6194 | Speed: 58.7 batches/s | ETA: 04:19\n",
      "  Train: [42400/57525] ( 73.7%) | Loss: 0.6194 | Speed: 58.7 batches/s | ETA: 04:17\n",
      "  Train: [42500/57525] ( 73.9%) | Loss: 0.6194 | Speed: 58.7 batches/s | ETA: 04:15\n",
      "  Train: [42600/57525] ( 74.1%) | Loss: 0.6194 | Speed: 58.7 batches/s | ETA: 04:14\n",
      "  Train: [42700/57525] ( 74.2%) | Loss: 0.6194 | Speed: 58.7 batches/s | ETA: 04:12\n",
      "  Train: [42800/57525] ( 74.4%) | Loss: 0.6194 | Speed: 58.7 batches/s | ETA: 04:10\n",
      "  Train: [42900/57525] ( 74.6%) | Loss: 0.6194 | Speed: 58.7 batches/s | ETA: 04:09\n",
      "  Train: [43000/57525] ( 74.8%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 04:07\n",
      "  Train: [43100/57525] ( 74.9%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 04:05\n",
      "  Train: [43200/57525] ( 75.1%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 04:03\n",
      "  Train: [43300/57525] ( 75.3%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 04:02\n",
      "  Train: [43400/57525] ( 75.4%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 04:00\n",
      "  Train: [43500/57525] ( 75.6%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 03:58\n",
      "  Train: [43600/57525] ( 75.8%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 03:57\n",
      "  Train: [43700/57525] ( 76.0%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 03:55\n",
      "  Train: [43800/57525] ( 76.1%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 03:53\n",
      "  Train: [43900/57525] ( 76.3%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 03:52\n",
      "  Train: [44000/57525] ( 76.5%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 03:50\n",
      "  Train: [44100/57525] ( 76.7%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 03:48\n",
      "  Train: [44200/57525] ( 76.8%) | Loss: 0.6193 | Speed: 58.7 batches/s | ETA: 03:46\n",
      "  Train: [44300/57525] ( 77.0%) | Loss: 0.6192 | Speed: 58.7 batches/s | ETA: 03:45\n",
      "  Train: [44400/57525] ( 77.2%) | Loss: 0.6192 | Speed: 58.7 batches/s | ETA: 03:43\n",
      "  Train: [44500/57525] ( 77.4%) | Loss: 0.6191 | Speed: 58.7 batches/s | ETA: 03:41\n",
      "  Train: [44600/57525] ( 77.5%) | Loss: 0.6192 | Speed: 58.7 batches/s | ETA: 03:40\n",
      "  Train: [44700/57525] ( 77.7%) | Loss: 0.6192 | Speed: 58.7 batches/s | ETA: 03:38\n",
      "  Train: [44800/57525] ( 77.9%) | Loss: 0.6192 | Speed: 58.7 batches/s | ETA: 03:36\n",
      "  Train: [44900/57525] ( 78.1%) | Loss: 0.6191 | Speed: 58.7 batches/s | ETA: 03:35\n",
      "  Train: [45000/57525] ( 78.2%) | Loss: 0.6192 | Speed: 58.7 batches/s | ETA: 03:33\n",
      "  Train: [45100/57525] ( 78.4%) | Loss: 0.6191 | Speed: 58.7 batches/s | ETA: 03:31\n",
      "  Train: [45200/57525] ( 78.6%) | Loss: 0.6191 | Speed: 58.7 batches/s | ETA: 03:29\n",
      "  Train: [45300/57525] ( 78.7%) | Loss: 0.6190 | Speed: 58.7 batches/s | ETA: 03:28\n",
      "  Train: [45400/57525] ( 78.9%) | Loss: 0.6190 | Speed: 58.7 batches/s | ETA: 03:26\n",
      "  Train: [45500/57525] ( 79.1%) | Loss: 0.6190 | Speed: 58.7 batches/s | ETA: 03:24\n",
      "  Train: [45600/57525] ( 79.3%) | Loss: 0.6190 | Speed: 58.7 batches/s | ETA: 03:23\n",
      "  Train: [45700/57525] ( 79.4%) | Loss: 0.6190 | Speed: 58.7 batches/s | ETA: 03:21\n",
      "  Train: [45800/57525] ( 79.6%) | Loss: 0.6190 | Speed: 58.7 batches/s | ETA: 03:19\n",
      "  Train: [45900/57525] ( 79.8%) | Loss: 0.6190 | Speed: 58.7 batches/s | ETA: 03:17\n",
      "  Train: [46000/57525] ( 80.0%) | Loss: 0.6190 | Speed: 58.7 batches/s | ETA: 03:16\n",
      "  Train: [46100/57525] ( 80.1%) | Loss: 0.6190 | Speed: 58.7 batches/s | ETA: 03:14\n",
      "  Train: [46200/57525] ( 80.3%) | Loss: 0.6190 | Speed: 58.7 batches/s | ETA: 03:12\n",
      "  Train: [46300/57525] ( 80.5%) | Loss: 0.6189 | Speed: 58.7 batches/s | ETA: 03:11\n",
      "  Train: [46400/57525] ( 80.7%) | Loss: 0.6189 | Speed: 58.7 batches/s | ETA: 03:09\n",
      "  Train: [46500/57525] ( 80.8%) | Loss: 0.6188 | Speed: 58.7 batches/s | ETA: 03:07\n",
      "  Train: [46600/57525] ( 81.0%) | Loss: 0.6188 | Speed: 58.7 batches/s | ETA: 03:06\n",
      "  Train: [46700/57525] ( 81.2%) | Loss: 0.6188 | Speed: 58.7 batches/s | ETA: 03:04\n",
      "  Train: [46800/57525] ( 81.4%) | Loss: 0.6188 | Speed: 58.7 batches/s | ETA: 03:02\n",
      "  Train: [46900/57525] ( 81.5%) | Loss: 0.6189 | Speed: 58.7 batches/s | ETA: 03:00\n",
      "  Train: [47000/57525] ( 81.7%) | Loss: 0.6188 | Speed: 58.7 batches/s | ETA: 02:59\n",
      "  Train: [47100/57525] ( 81.9%) | Loss: 0.6188 | Speed: 58.7 batches/s | ETA: 02:57\n",
      "  Train: [47200/57525] ( 82.1%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:55\n",
      "  Train: [47300/57525] ( 82.2%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:54\n",
      "  Train: [47400/57525] ( 82.4%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:52\n",
      "  Train: [47500/57525] ( 82.6%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:50\n",
      "  Train: [47600/57525] ( 82.7%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:48\n",
      "  Train: [47700/57525] ( 82.9%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:47\n",
      "  Train: [47800/57525] ( 83.1%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:45\n",
      "  Train: [47900/57525] ( 83.3%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:43\n",
      "  Train: [48000/57525] ( 83.4%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:42\n",
      "  Train: [48100/57525] ( 83.6%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:40\n",
      "  Train: [48200/57525] ( 83.8%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:38\n",
      "  Train: [48300/57525] ( 84.0%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:37\n",
      "  Train: [48400/57525] ( 84.1%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:35\n",
      "  Train: [48500/57525] ( 84.3%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:33\n",
      "  Train: [48600/57525] ( 84.5%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:31\n",
      "  Train: [48700/57525] ( 84.7%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:30\n",
      "  Train: [48800/57525] ( 84.8%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:28\n",
      "  Train: [48900/57525] ( 85.0%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:26\n",
      "  Train: [49000/57525] ( 85.2%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:25\n",
      "  Train: [49100/57525] ( 85.4%) | Loss: 0.6187 | Speed: 58.7 batches/s | ETA: 02:23\n",
      "  Train: [49200/57525] ( 85.5%) | Loss: 0.6186 | Speed: 58.7 batches/s | ETA: 02:21\n",
      "  Train: [49300/57525] ( 85.7%) | Loss: 0.6186 | Speed: 58.7 batches/s | ETA: 02:20\n",
      "  Train: [49400/57525] ( 85.9%) | Loss: 0.6186 | Speed: 58.7 batches/s | ETA: 02:18\n",
      "  Train: [49500/57525] ( 86.0%) | Loss: 0.6185 | Speed: 58.7 batches/s | ETA: 02:16\n",
      "  Train: [49600/57525] ( 86.2%) | Loss: 0.6186 | Speed: 58.7 batches/s | ETA: 02:14\n",
      "  Train: [49700/57525] ( 86.4%) | Loss: 0.6186 | Speed: 58.7 batches/s | ETA: 02:13\n",
      "  Train: [49800/57525] ( 86.6%) | Loss: 0.6186 | Speed: 58.7 batches/s | ETA: 02:11\n",
      "  Train: [49900/57525] ( 86.7%) | Loss: 0.6186 | Speed: 58.7 batches/s | ETA: 02:09\n",
      "  Train: [50000/57525] ( 86.9%) | Loss: 0.6186 | Speed: 58.7 batches/s | ETA: 02:08\n",
      "  Train: [50100/57525] ( 87.1%) | Loss: 0.6185 | Speed: 58.7 batches/s | ETA: 02:06\n",
      "  Train: [50200/57525] ( 87.3%) | Loss: 0.6185 | Speed: 58.7 batches/s | ETA: 02:04\n",
      "  Train: [50300/57525] ( 87.4%) | Loss: 0.6185 | Speed: 58.7 batches/s | ETA: 02:03\n",
      "  Train: [50400/57525] ( 87.6%) | Loss: 0.6185 | Speed: 58.7 batches/s | ETA: 02:01\n",
      "  Train: [50500/57525] ( 87.8%) | Loss: 0.6185 | Speed: 58.7 batches/s | ETA: 01:59\n",
      "  Train: [50600/57525] ( 88.0%) | Loss: 0.6185 | Speed: 58.7 batches/s | ETA: 01:57\n",
      "  Train: [50700/57525] ( 88.1%) | Loss: 0.6185 | Speed: 58.7 batches/s | ETA: 01:56\n",
      "  Train: [50800/57525] ( 88.3%) | Loss: 0.6184 | Speed: 58.7 batches/s | ETA: 01:54\n",
      "  Train: [50900/57525] ( 88.5%) | Loss: 0.6184 | Speed: 58.7 batches/s | ETA: 01:52\n",
      "  Train: [51000/57525] ( 88.7%) | Loss: 0.6184 | Speed: 58.7 batches/s | ETA: 01:51\n",
      "  Train: [51100/57525] ( 88.8%) | Loss: 0.6184 | Speed: 58.7 batches/s | ETA: 01:49\n",
      "  Train: [51200/57525] ( 89.0%) | Loss: 0.6183 | Speed: 58.7 batches/s | ETA: 01:47\n",
      "  Train: [51300/57525] ( 89.2%) | Loss: 0.6184 | Speed: 58.7 batches/s | ETA: 01:46\n",
      "  Train: [51400/57525] ( 89.4%) | Loss: 0.6184 | Speed: 58.7 batches/s | ETA: 01:44\n",
      "  Train: [51500/57525] ( 89.5%) | Loss: 0.6184 | Speed: 58.7 batches/s | ETA: 01:42\n",
      "  Train: [51600/57525] ( 89.7%) | Loss: 0.6183 | Speed: 58.7 batches/s | ETA: 01:40\n",
      "  Train: [51700/57525] ( 89.9%) | Loss: 0.6183 | Speed: 58.7 batches/s | ETA: 01:39\n",
      "  Train: [51800/57525] ( 90.0%) | Loss: 0.6183 | Speed: 58.7 batches/s | ETA: 01:37\n",
      "  Train: [51900/57525] ( 90.2%) | Loss: 0.6183 | Speed: 58.7 batches/s | ETA: 01:35\n",
      "  Train: [52000/57525] ( 90.4%) | Loss: 0.6183 | Speed: 58.7 batches/s | ETA: 01:34\n",
      "  Train: [52100/57525] ( 90.6%) | Loss: 0.6183 | Speed: 58.7 batches/s | ETA: 01:32\n",
      "  Train: [52200/57525] ( 90.7%) | Loss: 0.6183 | Speed: 58.7 batches/s | ETA: 01:30\n",
      "  Train: [52300/57525] ( 90.9%) | Loss: 0.6183 | Speed: 58.7 batches/s | ETA: 01:29\n",
      "  Train: [52400/57525] ( 91.1%) | Loss: 0.6183 | Speed: 58.7 batches/s | ETA: 01:27\n",
      "  Train: [52500/57525] ( 91.3%) | Loss: 0.6183 | Speed: 58.7 batches/s | ETA: 01:25\n",
      "  Train: [52600/57525] ( 91.4%) | Loss: 0.6182 | Speed: 58.7 batches/s | ETA: 01:23\n",
      "  Train: [52700/57525] ( 91.6%) | Loss: 0.6182 | Speed: 58.7 batches/s | ETA: 01:22\n",
      "  Train: [52800/57525] ( 91.8%) | Loss: 0.6182 | Speed: 58.7 batches/s | ETA: 01:20\n",
      "  Train: [52900/57525] ( 92.0%) | Loss: 0.6182 | Speed: 58.7 batches/s | ETA: 01:18\n",
      "  Train: [53000/57525] ( 92.1%) | Loss: 0.6182 | Speed: 58.7 batches/s | ETA: 01:17\n",
      "  Train: [53100/57525] ( 92.3%) | Loss: 0.6181 | Speed: 58.7 batches/s | ETA: 01:15\n",
      "  Train: [53200/57525] ( 92.5%) | Loss: 0.6181 | Speed: 58.7 batches/s | ETA: 01:13\n",
      "  Train: [53300/57525] ( 92.7%) | Loss: 0.6181 | Speed: 58.7 batches/s | ETA: 01:12\n",
      "  Train: [53400/57525] ( 92.8%) | Loss: 0.6181 | Speed: 58.7 batches/s | ETA: 01:10\n",
      "  Train: [53500/57525] ( 93.0%) | Loss: 0.6181 | Speed: 58.7 batches/s | ETA: 01:08\n",
      "  Train: [53600/57525] ( 93.2%) | Loss: 0.6181 | Speed: 58.7 batches/s | ETA: 01:06\n",
      "  Train: [53700/57525] ( 93.4%) | Loss: 0.6180 | Speed: 58.7 batches/s | ETA: 01:05\n",
      "  Train: [53800/57525] ( 93.5%) | Loss: 0.6180 | Speed: 58.7 batches/s | ETA: 01:03\n",
      "  Train: [53900/57525] ( 93.7%) | Loss: 0.6180 | Speed: 58.7 batches/s | ETA: 01:01\n",
      "  Train: [54000/57525] ( 93.9%) | Loss: 0.6180 | Speed: 58.7 batches/s | ETA: 01:00\n",
      "  Train: [54100/57525] ( 94.0%) | Loss: 0.6180 | Speed: 58.7 batches/s | ETA: 00:58\n",
      "  Train: [54200/57525] ( 94.2%) | Loss: 0.6180 | Speed: 58.7 batches/s | ETA: 00:56\n",
      "  Train: [54300/57525] ( 94.4%) | Loss: 0.6180 | Speed: 58.7 batches/s | ETA: 00:54\n",
      "  Train: [54400/57525] ( 94.6%) | Loss: 0.6180 | Speed: 58.7 batches/s | ETA: 00:53\n",
      "  Train: [54500/57525] ( 94.7%) | Loss: 0.6179 | Speed: 58.7 batches/s | ETA: 00:51\n",
      "  Train: [54600/57525] ( 94.9%) | Loss: 0.6179 | Speed: 58.7 batches/s | ETA: 00:49\n",
      "  Train: [54700/57525] ( 95.1%) | Loss: 0.6179 | Speed: 58.7 batches/s | ETA: 00:48\n",
      "  Train: [54800/57525] ( 95.3%) | Loss: 0.6179 | Speed: 58.7 batches/s | ETA: 00:46\n",
      "  Train: [54900/57525] ( 95.4%) | Loss: 0.6179 | Speed: 58.7 batches/s | ETA: 00:44\n",
      "  Train: [55000/57525] ( 95.6%) | Loss: 0.6179 | Speed: 58.7 batches/s | ETA: 00:43\n",
      "  Train: [55100/57525] ( 95.8%) | Loss: 0.6179 | Speed: 58.7 batches/s | ETA: 00:41\n",
      "  Train: [55200/57525] ( 96.0%) | Loss: 0.6179 | Speed: 58.7 batches/s | ETA: 00:39\n",
      "  Train: [55300/57525] ( 96.1%) | Loss: 0.6179 | Speed: 58.7 batches/s | ETA: 00:37\n",
      "  Train: [55400/57525] ( 96.3%) | Loss: 0.6180 | Speed: 58.7 batches/s | ETA: 00:36\n",
      "  Train: [55500/57525] ( 96.5%) | Loss: 0.6180 | Speed: 58.7 batches/s | ETA: 00:34\n",
      "  Train: [55600/57525] ( 96.7%) | Loss: 0.6179 | Speed: 58.7 batches/s | ETA: 00:32\n",
      "  Train: [55700/57525] ( 96.8%) | Loss: 0.6179 | Speed: 58.7 batches/s | ETA: 00:31\n",
      "  Train: [55800/57525] ( 97.0%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:29\n",
      "  Train: [55900/57525] ( 97.2%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:27\n",
      "  Train: [56000/57525] ( 97.3%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:26\n",
      "  Train: [56100/57525] ( 97.5%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:24\n",
      "  Train: [56200/57525] ( 97.7%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:22\n",
      "  Train: [56300/57525] ( 97.9%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:20\n",
      "  Train: [56400/57525] ( 98.0%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:19\n",
      "  Train: [56500/57525] ( 98.2%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:17\n",
      "  Train: [56600/57525] ( 98.4%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:15\n",
      "  Train: [56700/57525] ( 98.6%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:14\n",
      "  Train: [56800/57525] ( 98.7%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:12\n",
      "  Train: [56900/57525] ( 98.9%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:10\n",
      "  Train: [57000/57525] ( 99.1%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:08\n",
      "  Train: [57100/57525] ( 99.3%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:07\n",
      "  Train: [57200/57525] ( 99.4%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:05\n",
      "  Train: [57300/57525] ( 99.6%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:03\n",
      "  Train: [57400/57525] ( 99.8%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:02\n",
      "  Train: [57500/57525] (100.0%) | Loss: 0.6179 | Speed: 58.6 batches/s | ETA: 00:00\n",
      "  Running validation...\n",
      "    Val: [  50/7191] (  0.7%)\n",
      "    Val: [ 100/7191] (  1.4%)\n",
      "    Val: [ 150/7191] (  2.1%)\n",
      "    Val: [ 200/7191] (  2.8%)\n",
      "    Val: [ 250/7191] (  3.5%)\n",
      "    Val: [ 300/7191] (  4.2%)\n",
      "    Val: [ 350/7191] (  4.9%)\n",
      "    Val: [ 400/7191] (  5.6%)\n",
      "    Val: [ 450/7191] (  6.3%)\n",
      "    Val: [ 500/7191] (  7.0%)\n",
      "    Val: [ 550/7191] (  7.6%)\n",
      "    Val: [ 600/7191] (  8.3%)\n",
      "    Val: [ 650/7191] (  9.0%)\n",
      "    Val: [ 700/7191] (  9.7%)\n",
      "    Val: [ 750/7191] ( 10.4%)\n",
      "    Val: [ 800/7191] ( 11.1%)\n",
      "    Val: [ 850/7191] ( 11.8%)\n",
      "    Val: [ 900/7191] ( 12.5%)\n",
      "    Val: [ 950/7191] ( 13.2%)\n",
      "    Val: [1000/7191] ( 13.9%)\n",
      "    Val: [1050/7191] ( 14.6%)\n",
      "    Val: [1100/7191] ( 15.3%)\n",
      "    Val: [1150/7191] ( 16.0%)\n",
      "    Val: [1200/7191] ( 16.7%)\n",
      "    Val: [1250/7191] ( 17.4%)\n",
      "    Val: [1300/7191] ( 18.1%)\n",
      "    Val: [1350/7191] ( 18.8%)\n",
      "    Val: [1400/7191] ( 19.5%)\n",
      "    Val: [1450/7191] ( 20.2%)\n",
      "    Val: [1500/7191] ( 20.9%)\n",
      "    Val: [1550/7191] ( 21.6%)\n",
      "    Val: [1600/7191] ( 22.3%)\n",
      "    Val: [1650/7191] ( 22.9%)\n",
      "    Val: [1700/7191] ( 23.6%)\n",
      "    Val: [1750/7191] ( 24.3%)\n",
      "    Val: [1800/7191] ( 25.0%)\n",
      "    Val: [1850/7191] ( 25.7%)\n",
      "    Val: [1900/7191] ( 26.4%)\n",
      "    Val: [1950/7191] ( 27.1%)\n",
      "    Val: [2000/7191] ( 27.8%)\n",
      "    Val: [2050/7191] ( 28.5%)\n",
      "    Val: [2100/7191] ( 29.2%)\n",
      "    Val: [2150/7191] ( 29.9%)\n",
      "    Val: [2200/7191] ( 30.6%)\n",
      "    Val: [2250/7191] ( 31.3%)\n",
      "    Val: [2300/7191] ( 32.0%)\n",
      "    Val: [2350/7191] ( 32.7%)\n",
      "    Val: [2400/7191] ( 33.4%)\n",
      "    Val: [2450/7191] ( 34.1%)\n",
      "    Val: [2500/7191] ( 34.8%)\n",
      "    Val: [2550/7191] ( 35.5%)\n",
      "    Val: [2600/7191] ( 36.2%)\n",
      "    Val: [2650/7191] ( 36.9%)\n",
      "    Val: [2700/7191] ( 37.5%)\n",
      "    Val: [2750/7191] ( 38.2%)\n",
      "    Val: [2800/7191] ( 38.9%)\n",
      "    Val: [2850/7191] ( 39.6%)\n",
      "    Val: [2900/7191] ( 40.3%)\n",
      "    Val: [2950/7191] ( 41.0%)\n",
      "    Val: [3000/7191] ( 41.7%)\n",
      "    Val: [3050/7191] ( 42.4%)\n",
      "    Val: [3100/7191] ( 43.1%)\n",
      "    Val: [3150/7191] ( 43.8%)\n",
      "    Val: [3200/7191] ( 44.5%)\n",
      "    Val: [3250/7191] ( 45.2%)\n",
      "    Val: [3300/7191] ( 45.9%)\n",
      "    Val: [3350/7191] ( 46.6%)\n",
      "    Val: [3400/7191] ( 47.3%)\n",
      "    Val: [3450/7191] ( 48.0%)\n",
      "    Val: [3500/7191] ( 48.7%)\n",
      "    Val: [3550/7191] ( 49.4%)\n",
      "    Val: [3600/7191] ( 50.1%)\n",
      "    Val: [3650/7191] ( 50.8%)\n",
      "    Val: [3700/7191] ( 51.5%)\n",
      "    Val: [3750/7191] ( 52.1%)\n",
      "    Val: [3800/7191] ( 52.8%)\n",
      "    Val: [3850/7191] ( 53.5%)\n",
      "    Val: [3900/7191] ( 54.2%)\n",
      "    Val: [3950/7191] ( 54.9%)\n",
      "    Val: [4000/7191] ( 55.6%)\n",
      "    Val: [4050/7191] ( 56.3%)\n",
      "    Val: [4100/7191] ( 57.0%)\n",
      "    Val: [4150/7191] ( 57.7%)\n",
      "    Val: [4200/7191] ( 58.4%)\n",
      "    Val: [4250/7191] ( 59.1%)\n",
      "    Val: [4300/7191] ( 59.8%)\n",
      "    Val: [4350/7191] ( 60.5%)\n",
      "    Val: [4400/7191] ( 61.2%)\n",
      "    Val: [4450/7191] ( 61.9%)\n",
      "    Val: [4500/7191] ( 62.6%)\n",
      "    Val: [4550/7191] ( 63.3%)\n",
      "    Val: [4600/7191] ( 64.0%)\n",
      "    Val: [4650/7191] ( 64.7%)\n",
      "    Val: [4700/7191] ( 65.4%)\n",
      "    Val: [4750/7191] ( 66.1%)\n",
      "    Val: [4800/7191] ( 66.8%)\n",
      "    Val: [4850/7191] ( 67.4%)\n",
      "    Val: [4900/7191] ( 68.1%)\n",
      "    Val: [4950/7191] ( 68.8%)\n",
      "    Val: [5000/7191] ( 69.5%)\n",
      "    Val: [5050/7191] ( 70.2%)\n",
      "    Val: [5100/7191] ( 70.9%)\n",
      "    Val: [5150/7191] ( 71.6%)\n",
      "    Val: [5200/7191] ( 72.3%)\n",
      "    Val: [5250/7191] ( 73.0%)\n",
      "    Val: [5300/7191] ( 73.7%)\n",
      "    Val: [5350/7191] ( 74.4%)\n",
      "    Val: [5400/7191] ( 75.1%)\n",
      "    Val: [5450/7191] ( 75.8%)\n",
      "    Val: [5500/7191] ( 76.5%)\n",
      "    Val: [5550/7191] ( 77.2%)\n",
      "    Val: [5600/7191] ( 77.9%)\n",
      "    Val: [5650/7191] ( 78.6%)\n",
      "    Val: [5700/7191] ( 79.3%)\n",
      "    Val: [5750/7191] ( 80.0%)\n",
      "    Val: [5800/7191] ( 80.7%)\n",
      "    Val: [5850/7191] ( 81.4%)\n",
      "    Val: [5900/7191] ( 82.0%)\n",
      "    Val: [5950/7191] ( 82.7%)\n",
      "    Val: [6000/7191] ( 83.4%)\n",
      "    Val: [6050/7191] ( 84.1%)\n",
      "    Val: [6100/7191] ( 84.8%)\n",
      "    Val: [6150/7191] ( 85.5%)\n",
      "    Val: [6200/7191] ( 86.2%)\n",
      "    Val: [6250/7191] ( 86.9%)\n",
      "    Val: [6300/7191] ( 87.6%)\n",
      "    Val: [6350/7191] ( 88.3%)\n",
      "    Val: [6400/7191] ( 89.0%)\n",
      "    Val: [6450/7191] ( 89.7%)\n",
      "    Val: [6500/7191] ( 90.4%)\n",
      "    Val: [6550/7191] ( 91.1%)\n",
      "    Val: [6600/7191] ( 91.8%)\n",
      "    Val: [6650/7191] ( 92.5%)\n",
      "    Val: [6700/7191] ( 93.2%)\n",
      "    Val: [6750/7191] ( 93.9%)\n",
      "    Val: [6800/7191] ( 94.6%)\n",
      "    Val: [6850/7191] ( 95.3%)\n",
      "    Val: [6900/7191] ( 96.0%)\n",
      "    Val: [6950/7191] ( 96.6%)\n",
      "    Val: [7000/7191] ( 97.3%)\n",
      "    Val: [7050/7191] ( 98.0%)\n",
      "    Val: [7100/7191] ( 98.7%)\n",
      "    Val: [7150/7191] ( 99.4%)\n",
      "\n",
      "  Epoch 03 Summary:\n",
      "    Train MSE (norm): 0.6178 | Val MSE (norm): 0.5879\n",
      "    Time: Train=16.4min, Val=0.6min, Total=16.9min\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 5e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler: reduces LR when validation loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "EPOCHS = 3\n",
    "BATCH  = 512\n",
    "\n",
    "def iter_minibatches(indexes, batch_size=256, shuffle=True, epoch=None, use_normalized=True):\n",
    "    \"\"\"\n",
    "    Generate minibatches. If use_normalized=True, returns normalized targets.\n",
    "    \"\"\"\n",
    "    idx = np.asarray(indexes)\n",
    "    if shuffle:\n",
    "        if epoch is not None:\n",
    "            rng = np.random.default_rng(42 + epoch)\n",
    "        else:\n",
    "            rng = np.random.default_rng()\n",
    "        rng.shuffle(idx)\n",
    "    \n",
    "    target_array = y_norm if use_normalized else y\n",
    "    \n",
    "    for start in range(0, len(idx), batch_size):\n",
    "        mb = idx[start:start+batch_size]\n",
    "        yield (\n",
    "            torch.from_numpy(X_seq[mb]).to(device),           # (B, 4, 23) float32\n",
    "            torch.from_numpy(X_cell[mb]).long().to(device),   # (B,) int64\n",
    "            torch.from_numpy(X_ph[mb]).long().to(device),     # (B,) int64\n",
    "            torch.from_numpy(X_chr[mb]).long().to(device),    # (B,) int64\n",
    "            torch.from_numpy(X_strand[mb]).long().to(device), # (B,) int64\n",
    "            torch.from_numpy(target_array[mb]).to(device),    # (B,) float32\n",
    "        )\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model.train()\n",
    "    train_sum, n_train = 0.0, 0\n",
    "    batch_count = 0\n",
    "    total_train_batches = len(idx_train) // BATCH + (1 if len(idx_train) % BATCH != 0 else 0)\n",
    "    last_update_time = time.time()\n",
    "    \n",
    "    for seq, cl, ph, ch, st, tgt_norm in iter_minibatches(idx_train, batch_size=BATCH, shuffle=True, epoch=epoch, use_normalized=True):\n",
    "        pred_norm = model(seq, cl, ph, ch, st)\n",
    "        loss = criterion(pred_norm, tgt_norm)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_sum += loss.item() * tgt_norm.size(0)\n",
    "        n_train   += tgt_norm.size(0)\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Progress update every 100 batches or every 30 seconds\n",
    "        current_time = time.time()\n",
    "        if batch_count % 100 == 0 or (current_time - last_update_time) >= 30:\n",
    "            progress = (batch_count / total_train_batches) * 100\n",
    "            elapsed = current_time - epoch_start\n",
    "            batches_per_sec = batch_count / elapsed if elapsed > 0 else 0\n",
    "            eta_seconds = (total_train_batches - batch_count) / batches_per_sec if batches_per_sec > 0 else 0\n",
    "            eta_min = int(eta_seconds // 60)\n",
    "            eta_sec = int(eta_seconds % 60)\n",
    "            current_loss = train_sum / n_train if n_train > 0 else 0.0\n",
    "            \n",
    "            print(f\"  Train: [{batch_count:5d}/{total_train_batches}] ({progress:5.1f}%) | \"\n",
    "                  f\"Loss: {current_loss:.4f} | \"\n",
    "                  f\"Speed: {batches_per_sec:.1f} batches/s | \"\n",
    "                  f\"ETA: {eta_min:02d}:{eta_sec:02d}\", flush=True)\n",
    "            last_update_time = current_time\n",
    "\n",
    "    train_loss = train_sum / n_train if n_train > 0 else 0.0\n",
    "    train_time = time.time() - epoch_start\n",
    "    \n",
    "    # ---- val ----\n",
    "    print(\"  Running validation...\", flush=True)\n",
    "    val_start = time.time()\n",
    "    model.eval()\n",
    "    val_sum, n_val = 0.0, 0\n",
    "    val_batch_count = 0\n",
    "    total_val_batches = len(idx_val) // BATCH + (1 if len(idx_val) % BATCH != 0 else 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq, cl, ph, ch, st, tgt_norm in iter_minibatches(idx_val, batch_size=BATCH, shuffle=False, use_normalized=True):\n",
    "            pred_norm = model(seq, cl, ph, ch, st)\n",
    "            loss = criterion(pred_norm, tgt_norm)\n",
    "            val_sum += loss.item() * tgt_norm.size(0)\n",
    "            n_val   += tgt_norm.size(0)\n",
    "            val_batch_count += 1\n",
    "            \n",
    "            if val_batch_count % 50 == 0:\n",
    "                val_progress = (val_batch_count / total_val_batches) * 100\n",
    "                print(f\"    Val: [{val_batch_count:4d}/{total_val_batches}] ({val_progress:5.1f}%)\", flush=True)\n",
    "\n",
    "    val_loss = val_sum / n_val if n_val > 0 else 0.0\n",
    "    val_time = time.time() - val_start\n",
    "    total_time = time.time() - epoch_start\n",
    "    \n",
    "    # Update learning rate scheduler based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n  Epoch {epoch:02d} Summary:\")\n",
    "    print(f\"    Train MSE (norm): {train_loss:.4f} | Val MSE (norm): {val_loss:.4f}\")\n",
    "    print(f\"    Time: Train={train_time/60:.1f}min, Val={val_time/60:.1f}min, Total={total_time/60:.1f}min\")\n",
    "    print(f\"{'='*60}\\n\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8709bf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'MSE': 0.39029095253485785, 'Pearson': 0.6419672048660741, 'Spearman': 0.5032218744854215, 'Accuracy': 0.6817393368687873}\n",
      "Test: {'MSE': 0.3922537024687953, 'Pearson': 0.6413218177448481, 'Spearman': 0.5034756434551025, 'Accuracy': 0.6814704294153245}\n"
     ]
    }
   ],
   "source": [
    "def mse_doc(yhat, y):\n",
    "    yhat = np.asarray(yhat, dtype=np.float64)\n",
    "    y    = np.asarray(y,    dtype=np.float64)\n",
    "    n = y.size\n",
    "    return float(np.sum((y - yhat)**2) / n)\n",
    "\n",
    "def pearson_doc(x, y):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "    n      = x.size\n",
    "    sum_x  = np.sum(x)\n",
    "    sum_y  = np.sum(y)\n",
    "    sum_xy = np.sum(x * y)\n",
    "    sum_x2 = np.sum(x * x)\n",
    "    sum_y2 = np.sum(y * y)\n",
    "    denom = np.sqrt((n * sum_x2 - sum_x * sum_x) * (n * sum_y2 - sum_y * sum_y))\n",
    "    return float((n * sum_xy - sum_x * sum_y) / denom) if denom != 0.0 else 0.0\n",
    "\n",
    "def _ranks_avg(a):\n",
    "    a = np.asarray(a, dtype=np.float64)\n",
    "    order = np.argsort(a, kind=\"mergesort\")\n",
    "    ranks = np.empty_like(order, dtype=np.float64)\n",
    "    sa = a[order]\n",
    "    diff = np.concatenate(([True], sa[1:] != sa[:-1], [True]))\n",
    "    idx = np.flatnonzero(diff)\n",
    "    for s, e in zip(idx[:-1], idx[1:]):\n",
    "        ranks[order[s:e]] = 0.5 * (s + e - 1) + 1.0\n",
    "    return ranks\n",
    "\n",
    "def spearman_doc(x, y):\n",
    "    rx = _ranks_avg(x)\n",
    "    ry = _ranks_avg(y)\n",
    "    d  = rx - ry\n",
    "    n  = rx.size\n",
    "    denom = n * (n * n - 1.0)\n",
    "    return float(1.0 - (6.0 * np.sum(d * d)) / denom) if denom != 0.0 else 0.0\n",
    "\n",
    "def accuracy_direction(yhat, y):\n",
    "    yhat = np.asarray(yhat, dtype=np.float64)\n",
    "    y    = np.asarray(y,    dtype=np.float64)\n",
    "    return float(np.mean((yhat >= 0) == (y >= 0)))\n",
    "\n",
    "@torch.no_grad()\n",
    "def preds_and_trues(indexes, batch_size=256):\n",
    "    \"\"\"\n",
    "    Get predictions and true values. Predictions are de-normalized from normalized space.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ps_norm, ys_norm = [], []\n",
    "    for seq, cl, ph, ch, st, tgt_norm in iter_minibatches(indexes, batch_size=batch_size, shuffle=False, use_normalized=True):\n",
    "        out_norm = model(seq, cl, ph, ch, st)\n",
    "        ps_norm.append(out_norm.detach().cpu().numpy())\n",
    "        ys_norm.append(tgt_norm.detach().cpu().numpy())\n",
    "    \n",
    "    yhat_norm = np.concatenate(ps_norm)\n",
    "    y_norm = np.concatenate(ys_norm)\n",
    "\n",
    "    yhat_real = yhat_norm * sigma + mu\n",
    "    y_real = y_norm * sigma + mu\n",
    "    \n",
    "    return yhat_real, y_real\n",
    "\n",
    "def eval_split(indexes):\n",
    "    yhat, y = preds_and_trues(indexes, batch_size=256)\n",
    "    return {\n",
    "        \"MSE\": mse_doc(yhat, y),\n",
    "        \"Pearson\": pearson_doc(yhat, y),\n",
    "        \"Spearman\": spearman_doc(yhat, y),\n",
    "        \"Accuracy\": accuracy_direction(yhat, y),\n",
    "    }\n",
    "\n",
    "# --- print results ---\n",
    "print(\"Validation:\", eval_split(idx_val))\n",
    "print(\"Test:\",       eval_split(idx_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
